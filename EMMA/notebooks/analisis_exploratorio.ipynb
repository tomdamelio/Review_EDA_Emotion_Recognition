{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis exploratorio de los datos\n",
    "- Primero importamos librerias y datos a ser usados\n",
    "- Para juntar datos de varias columnas en una sola se ha usado la funcion get_values, obtenida del link: https://stackoverflow.com/questions/38334296/reversing-one-hot-encoding-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerias a usar\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Creación de data frames a usar\n",
    "df_metadata = pd.read_csv(\"c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - Metadata.csv\")\n",
    "df_data_type = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - Data type.csv')\n",
    "df_participants = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - Participants.csv')\n",
    "df_self_report = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - Self report.csv')\n",
    "df_emotion_elicitation_techniques = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - Emotion elicitation techniques.csv')\n",
    "df_eda = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - EDA.csv')\n",
    "df_statistical_learning_models = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - Statistical Learning model.csv')\n",
    "df_performances = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - Performances.csv')\n",
    "df_alg_perf = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Tabla Normalizada - Alg_Perf.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set tamaño de gráficos para todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gráfico de barra por año (2010-2020) por paper según modelos de emociones empleado (categoriales o dimensionales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rellenar datos faltantes y NO SE DROPEAN los duplicados (hay papers que usan multiples modelos)\n",
    "df_statistical_learning_models=df_statistical_learning_models.fillna('-')\n",
    "\n",
    "#ploteo\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "plt.rcParams[\"legend.fontsize\"] = 15\n",
    "plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "\n",
    "category_order = [2010, 2011, 2012, 2013, 2014, 2015, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "g= sns.countplot(x='year', \n",
    "    data= df_statistical_learning_models, \n",
    "    hue='affective_model', \n",
    "    order=category_order)\n",
    "g.set(title = 'Cantidad de papers por año (2010 - 2020)', xlabel = 'Año', ylabel = 'Cantidad de papers')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis estadístico para determinar si modelos de detección de arousal performan mejor que los basados en valence (Algoritmos de regresión)\n",
    "- Procedimiento: subsetear para quedarnos con modelos dimensionales(columna affective model), subsetear con regressor, donde nos quedamos solo con las dimensiones que sean arousal/valence, quedarse con la medida de perforrmace que mas aparezca (count), hacer el test estadístico correspondiente (t, wettney, etc)\n",
    "- Resultado: no existe diferencia estadisticamente significativa entre grupos, por lo que los algoritmos de regresión basados en modelos dimensionales (basados en arousal o valence) no performan mejor uno sobre otros (Segun t de student y u de mann-whitney).\n",
    "- Tener en cuenta el tamaño de la muestra que cumple con todos los criterios mencionados en el Procedimiento: 16 modelos en total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creacion del data frames, y subseteo por: modelos dimensionales, tipo regressor, dimensiones arousal/valence\n",
    "df = df_alg_perf\n",
    "df = df[df['affective_model'] == 'dimensional']\n",
    "df = df[df['is_regressor'].isin(['x', 'X'])]\n",
    "df = df[df['regre_model_output_dimensions'].isin(['arousal', 'valence'])]\n",
    "\n",
    "#que medida de performance mas aparece?\n",
    "#dataframe con performances\n",
    "df_performance_medidas = df.iloc[:,67:]\n",
    "\n",
    "#busqueda de la medida de performance mas recurrente, paso de los valores a 0 y 1 para facilitar su conteo\n",
    "df_performance_medidas = df_performance_medidas.fillna('No')\n",
    "df_performance_medidas = df_performance_medidas.replace('-', 'No')\n",
    "df_performance_medidas=df_performance_medidas.mask(df_performance_medidas != 'No','Yes')\n",
    "\n",
    "#obtener performance mas frecuente\n",
    "def get_value(row):\n",
    "    for c in df_performance_medidas.columns:\n",
    "        if row[c] == 'Yes':\n",
    "            return c\n",
    "\n",
    "df_performance_medidas = df_performance_medidas.apply(get_value, axis=1)\n",
    "df_performance_medidas = pd.DataFrame(df_performance_medidas)\n",
    "df_performance_medidas.columns = ['Performances']\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context = 'notebook')\n",
    "sns.countplot(x='Performances', data=df_performance_medidas)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#subseteo por la medida de performance que mas aparece (RMSE)\n",
    "df = df.fillna('-')\n",
    "df = df[df['Root-Mean-Square-Error-(RMSE)'] != '-']\n",
    "\n",
    "print(df)\n",
    "#la muestra es de 16 modelos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos los estadísticos\n",
    "\n",
    "obtenido de: https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_arousal = df[df['regre_model_output_dimensions'] == 'arousal']\n",
    "df_arousal = df_arousal['Root-Mean-Square-Error-(RMSE)']\n",
    "arousal = df_arousal.values.tolist()\n",
    "arousal = list(map(float, arousal))\n",
    "\n",
    "df_valence = df[df['regre_model_output_dimensions'] == 'valence']\n",
    "df_valence = df_valence['Root-Mean-Square-Error-(RMSE)']\n",
    "valence = df_valence.values.tolist()\n",
    "valence = list(map(float, valence))\n",
    "\n",
    "#Test parametrico - t de student\n",
    "print('Students t-test')\n",
    "from scipy.stats import ttest_ind\n",
    "stat, p = ttest_ind(arousal, valence)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')\n",
    "\n",
    "\n",
    "#Test no parametrico - U de Mann-Whitney\n",
    "print('Mann-Whitney U Test')\n",
    "from scipy.stats import mannwhitneyu\n",
    "stat, p = mannwhitneyu(arousal, valence)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis estadístico para determinar si modelos de detección de arousal performan mejor que los basados en valence (Algoritmos de clasificación)\n",
    "- Procedimiento: subsetear para quedarnos con modelos dimensionales(columna affective model), quedarse solo con clasificación binarias (LA,HA/LV,HV), quedarse con la medida de performance que mas aparezca (hacer count). hacer el test estadístico correspondiente (t, wettney, etc), que depende del supuesto (si hay normalidad se aplica paramétrico, sino no-parametrico).\n",
    "- Resultado: no existe diferencia estadisticamente significativa entre grupos, por lo que los algoritmos clasificadores basados en modelos dimensionales de clasificacion binaria (HA/LA, HV/LV) no performan mejor uno sobre otros (Segun t de student y u de mann-whitney).\n",
    "- Tener en cuenta el tamaño de la muestra que cumple con todos los criterios mencionados en el Procedimiento: 10 modelos en total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion data frame y subseteo por: modelos dimensionales, tipo clasiffier, categorias HA/LA y HV/LV\n",
    "df_2 = df_alg_perf\n",
    "df_2 = df_2[df_2['affective_model'] == 'dimensional']\n",
    "df_2 = df_2[df_2['is_classifier'].isin(['x', 'X'])]\n",
    "df_2 = df_2[df_2['class_model_output_categories'].isin(['HA, LA', 'HV, LV'])]\n",
    "\n",
    "#que medida de performance mas aparece en este grupo?\n",
    "#dataframe con performances\n",
    "df2_performance_medidas = df_2.iloc[:,67:]\n",
    "\n",
    "#búsqueda de la medida de performance que mas aparece, paso de datos a 0 y 1 para facilitar su conteo\n",
    "df2_performance_medidas = df2_performance_medidas.fillna('No')\n",
    "df2_performance_medidas = df2_performance_medidas.replace('-', 'No')\n",
    "df2_performance_medidas=df2_performance_medidas.mask(df2_performance_medidas != 'No','Yes')\n",
    "\n",
    "def get_value(row):\n",
    "    for c in df2_performance_medidas.columns:\n",
    "        if row[c] == 'Yes':\n",
    "            return c\n",
    "\n",
    "df2_performance_medidas = df2_performance_medidas.apply(get_value, axis=1)\n",
    "df2_performance_medidas = pd.DataFrame(df2_performance_medidas)\n",
    "df2_performance_medidas.columns = ['Performances']\n",
    "\n",
    "#ploteo para determinar la medida de performance mas recurrente\n",
    "sns.set_theme(style=\"whitegrid\", context = 'notebook')\n",
    "sns.countplot(x='Performances', data=df2_performance_medidas)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#subseteo por la medida de performance que mas aparece (UAR)\n",
    "df_2 = df_2.fillna('-')\n",
    "df_2 = df_2[df_2['unweighted average-recall-(UAR)'] != '-']\n",
    "\n",
    "print(df_2)\n",
    "#la muestra es de 10 modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos los estadísticos\n",
    "\n",
    "obtenido de: https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arousal = df_2[df_2['class_model_output_categories'] == 'HA, LA']\n",
    "df_arousal = df_arousal['unweighted average-recall-(UAR)']\n",
    "arousal = df_arousal.values.tolist()\n",
    "arousal = list(map(float, arousal))\n",
    "\n",
    "df_valence = df_2[df_2['class_model_output_categories'] == 'HV, LV']\n",
    "df_valence = df_valence['unweighted average-recall-(UAR)']\n",
    "valence = df_valence.values.tolist()\n",
    "valence = list(map(float, valence))\n",
    "\n",
    "#Test parametrico - t de student\n",
    "print('Students t-test')\n",
    "from scipy.stats import ttest_ind\n",
    "stat, p = ttest_ind(arousal, valence)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')\n",
    "\n",
    "\n",
    "#Test no parametrico - U de Mann-Whitney\n",
    "print('Mann-Whitney U Test')\n",
    "from scipy.stats import mannwhitneyu\n",
    "stat, p = mannwhitneyu(arousal, valence)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gráficos frencuencia de los modelos algoritimicos, según modelos de regresión y clasificación\n",
    "\n",
    "- Interpretaciones: Los algoritmos clasificadores son por mucho los mas usados, ademas de ser los que mas variedad representan. Que implica esto? Es lo mismo aplicar algoritmos clasificadores o regresores?\n",
    "- Nota: ordenar los valores de los gráficos y unirlos en uno, buscar graficar los mas usados (primeros 5 o 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para regressor\n",
    "df_algoritmos_regre = df_statistical_learning_models.iloc[:,43:58]\n",
    "\n",
    "def get_value(row):\n",
    "     for c in df_algoritmos_regre.columns:\n",
    "         if row[c]== 'x':\n",
    "             return c\n",
    "\n",
    "df_algoritmos_regre = df_algoritmos_regre.apply(get_value, axis=1)\n",
    "df_algoritmos_regre = pd.DataFrame(df_algoritmos_regre)\n",
    "df_algoritmos_regre.columns = ['Algorithms']\n",
    "\n",
    "#Lista de algoritmos unicos de regresion\n",
    "algoritmos_de_regresion = df_algoritmos_regre[\"Algorithms\"].unique()\n",
    "#print(algoritmos_de_regresion)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "g = sns.countplot(x='Algorithms', data=df_algoritmos_regre, order=df_algoritmos_regre.Algorithms.value_counts().index)\n",
    "g.set(title = 'Cantidad de modelos de tipo regressor', xlabel = 'Algoritmo', ylabel = 'Cantidad de modelos')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#para classifier\n",
    "df_algoritmos_class = df_statistical_learning_models.iloc[:,8:40]\n",
    "def get_value(row):\n",
    "     for c in df_algoritmos_class.columns:\n",
    "         if row[c]== 'x':\n",
    "             return c\n",
    "\n",
    "df_algoritmos_class = df_algoritmos_class.apply(get_value, axis=1)\n",
    "df_algoritmos_class = pd.DataFrame(df_algoritmos_class)\n",
    "df_algoritmos_class.columns = ['Algorithms']\n",
    "\n",
    "#Lista de algoritmos unicos de clasificacion\n",
    "algoritmos_de_clasificacion = df_algoritmos_class[\"Algorithms\"].unique()\n",
    "print(algoritmos_de_clasificacion)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "g = sns.countplot(x='Algorithms', data=df_algoritmos_class, order=df_algoritmos_class.Algorithms.value_counts().index)\n",
    "g.set(title = 'Cantidad de modelos de tipo classifier', xlabel = 'Algoritmo', ylabel = 'Cantidad de modelos')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#cantidad de modelos por tipo (regressor o clasiffier)\n",
    "df_class_or_regre = df_statistical_learning_models.iloc[:,[5,40]]\n",
    "def get_value(row):\n",
    "     for c in df_class_or_regre.columns:\n",
    "         if row[c]== 'x':\n",
    "             return c\n",
    "\n",
    "df_class_or_regre = df_class_or_regre.apply(get_value, axis=1)\n",
    "df_class_or_regre = pd.DataFrame(df_class_or_regre)\n",
    "df_class_or_regre.columns = ['Algorithms']\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "sns.countplot(x='Algorithms', data=df_class_or_regre)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#pie plot\n",
    "quantity = df_class_or_regre['Algorithms'].value_counts()\n",
    "df_class_or_regre_quantity = pd.DataFrame(quantity)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "names = 'Classifier', 'Regressor'\n",
    "plt.pie(df_class_or_regre_quantity['Algorithms'], labels = names, labeldistance = 1.15, wedgeprops = { 'linewidth' : 3, 'edgecolor' : 'white' })\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gráfico frencuencia de los tipos de elicitation (por modalidad o tecnica especifica)\n",
    "\n",
    "Nota: Falta organizar bien la data en la tabla, para obtener las siguientes barras: multimodal, modalidad visual, mod auditory, mod somatosensory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frecuencia tipos elicitation por modalidad\n",
    "\n",
    "df_emotion_elicitation_techniques=df_emotion_elicitation_techniques.fillna('-')\n",
    "df_emotion_elicitation_techniques_sin_duplicates = df_emotion_elicitation_techniques.drop_duplicates(subset='paper_id')\n",
    "\n",
    "df_eli_modalidad = df_emotion_elicitation_techniques_sin_duplicates.iloc[:,3:7]\n",
    "def get_value(row):\n",
    "     for c in df_eli_modalidad.columns:\n",
    "         if row[c]== 'x':\n",
    "             return c\n",
    "\n",
    "df_eli_modalidad = df_eli_modalidad.apply(get_value, axis=1)\n",
    "df_eli_modalidad = pd.DataFrame(df_eli_modalidad)\n",
    "df_eli_modalidad.columns = ['Elicitation modality']\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context = 'notebook')\n",
    "sns.countplot(x='Elicitation modality', data=df_eli_modalidad)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gráfico de barra por año (2010-2020) según tipos de base de datos (privada, pública)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rellenar datos faltantes y dropeo de duplicados\n",
    "df_data_type=df_data_type.fillna('-')\n",
    "df_data_type_sin_duplicates = df_data_type.drop_duplicates(subset='paper_id')\n",
    "\n",
    "#ploteo\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "plt.rcParams[\"legend.fontsize\"] = 15\n",
    "plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "\n",
    "category_order = [2010, 2011, 2012, 2013, 2014, 2015, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "g = sns.countplot(x='year', \n",
    "    data= df_data_type_sin_duplicates, \n",
    "    hue='db_type', \n",
    "    order=category_order)\n",
    "g.set(title = 'Frecuencia de uso de bases de datos públicas y privadas por año (2010 - 2020)', xlabel = 'Año', ylabel = 'Cantidad')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gráfico frencuencia de uso de cada base de datos pública encontrada\n",
    "- Interpretacion: Un predominio de pocas bases de datos. Estamos todo el tiempo sacando conclusiones sobre los mismos sujetos? Ver predominio de bases de datos publicas por sobre las privadas\n",
    "- No se ha tenido en cuenta el dato aportado por Lorenzo (mas de una db por paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bases de datos\n",
    "df_data_type=df_data_type.fillna('-')\n",
    "df_data_type_sin_duplicates = df_data_type.drop_duplicates(subset='paper_id')\n",
    "\n",
    "df_db = df_data_type_sin_duplicates.iloc[:,11:]\n",
    "def get_value(row):\n",
    "     for c in df_db.columns:\n",
    "         if row[c]== 'x':\n",
    "             return c\n",
    "\n",
    "df_db = df_db.apply(get_value, axis=1)\n",
    "df_db = pd.DataFrame(df_db)\n",
    "\n",
    "df_db = df_db.replace('Multimodal Dyadic Behavior (MMDB)', 'MMDB')\n",
    "df_db.columns = ['Database']\n",
    "\n",
    "#plot\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "plt.rcParams[\"legend.fontsize\"] = 15\n",
    "plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "g = sns.countplot(x='Database', data=df_db, order = df_db.Database.value_counts().index)\n",
    "g.set(title = 'Frecuencia de uso de bases de datos públicas', xlabel = 'Base de datos', ylabel = 'Cantidad')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gráfico frecuencia de papers según revista científica de origen, distinguiendo entre las que poseen orientación en ingeniería de datos y las que no\n",
    "\n",
    "Nota: falta filtrar bien cuales son journal, y cuales de estas son de ingenieria o no (diferenciarlas con color o hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por revista\n",
    "df_metadata=df_metadata.fillna('-')\n",
    "df_metadata_sin_duplicates = df_metadata.drop_duplicates(subset='paper_id')\n",
    "\n",
    "df_source_title = df_metadata_sin_duplicates[['paper_id','source_title','source_type_journal']]\n",
    "df_source_title = df_source_title[df_source_title['source_type_journal'].isin(['x', 'X'])]\n",
    "\n",
    "plt.rcParams[\"legend.fontsize\"] = 15\n",
    "plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "g = sns.countplot(x='source_title', data=df_source_title, order = df_source_title.source_title.value_counts().index)\n",
    "g.set(title = 'Cantidad de papers por journal', xlabel = 'Journal', ylabel = 'Cantidad')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "lista_journals = df_source_title[\"source_title\"].value_counts()\n",
    "print(lista_journals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e31aef8222fb7c235d2ed8e74ce17e973738f89b37261e7466b7a63a6dfb1214"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
