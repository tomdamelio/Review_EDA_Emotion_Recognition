Paper ID,Model ID,Paper (APA citation),Meta-data,,,,,,,,,Data type ,,,,,,,,,,,,,,,,,,,,,,,,,,,,Participants,,,,,,Self-report,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Emotion elicitation techniques,,,,,,,,,,,,,,,,,,,,,,,EDA,,,,,,,,,,,,,,,,,"Affective model ('dimensional', 'categorical', '-')  ",Statistical Learning model,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Results - Highest Performance,,,,,,,,,,,,,,,,,,,,Reviewer,,,Comments:                                     ,,Discrepancia
,,,Authors,Title,Year,Source Title,Source Type,,,,First author's country affiliation,Original?                                                                                             (put an x)   ,,,,Database,,,,,,,,,,,,,,,,,,,,,,,,N                         (Sample Size),N Female:                                         e.g. 13,Age ,,,Country ,"Affective self-report questionnaire:                                    a) Yes; b) No; c) Relies on other's questionnaire                                                                      IF ""No"" GO TO ""Emotion elicitaion technique""",Affective questionaire,,,,,,Affective measure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Modality,,,,,Task type,,"Technique name                 IF 'ad hoc task' (i.e. it is not based on a validated technique), complete '-'",Technique clasification,,,,,,,,,,,,,,,,,,Techinque description,Elicitation duration (per sample):                                  ,,,,EDA Device:,,EDA sensor location,,,,,,,,,,,,,,,,Model type,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Model's level:,,N Model Input (Samples or X):                                                    e.g. 255 ,Interpretation of the model/features                                                              ,,Public code,,Accuracy,Precision,F-Measure,Recall / Sensitivity / True Positive Rate,Specificity / True Negative Rate,False Positive Rate,False Negative Rate,ROC AUC,R2,R,MAE (Mean Absolute Error),Concordance Correlation Coefficient (CCC),Spearman’s ranking correlation,Mean square error (MSE),Root Mean Square Error (RMSE),"Pearson Correlation
Coefficient (r)","unweighted
average recall (UAR)",Matthews Correlation Coefficient,G-mean (Geometric mean),"Other                          (change name of the column, put an x, add a new ""Other "" column in ""Results"") ",Emma                                     (put an x),Lolo                                       (put an x),Agus                                         (put an x),,,
,,,,,,,,,,,,Private,Public,Both,Uppon request,"Database?                                       (put an x)                                                          IF ""x"", GO TO ""Statistical Learning Model"" after completing Data type ",Which Database?     ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dimensional ,,,,,,,,,Categorical,,,,,,,,,,,,,,,,,,,,,,,,,,Other,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Hemibody (e.g. right, left, dominant, non-dominant)",Hands,,,,,,,,,,Wrist,Chest (rectus abdominis),Left Lobe Temporalis,"Other?                                                 (change name of the column, put an x, add a new ""Other "" column in ""EDA sensor location"") ",,Classifier,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Regressor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Device created by authors? (""homemade"" device)",Device specification  e.g. a) Neulog; b) BIOPAC,,Hands?,Fingers,,,,,,Phalanges,,,,,,,,Model Output               (Target or Y): Categories,,Types,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Model Output (Target or Y): Dimension,,Types,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dimensional?                     (put an x) ,Dimensions,,,,,,,,Categorical?                   (put an x) ,Categories,,,,,,,,,,,,,,,,,,,,,,,,,"Other measure                       (change name of the column, put an x, add a new ""Other "" column in ""Affective measure"") ",Other,"Multimodality?                     (put an x) 
",Modalities,,,,Active task,Pasive task,," IF ""Modalities"" == Visual                     Which visual elicitation?",,,," IF ""Modalities"" == Auditory         Which auditory elicitation?",,Multiple techniques,Driving,Imagination techniques /memory recall,Social interactions,Virtual Reality,Meditation,Reading,UX,TEM clips (Tactile Enhanced Multimedia),Videogame,Puzzle,Other,,Elicitation duration (xx m xx s),Elicitation duration Range    (xx m xx s - xx m xxs),Elicitation duration Mean    (xx m xx s),Elicitation duration Median    (xx m xx s),,,,,,,,,,,,,,,,,,,Number,Which categories?,Logistic Regression,Support Vector Machine (SVM),k-Nearest Neighbor (k-NN),Quadratic discrimant clasifier (QDA),Linear Discriminant Analysis (LDA),Tree based models,Naive Bayes,AVG,LASSO-T,Ridge-T,LASSO-VAR,Ridge-VAR,TPA,HDC-MER,HMM,Gradient boosting,Artificial Neural Network  (ANN),,,,,,,,,,,,,,,"radial basis function
(RBF) ",1R rule,JRip,Number,Which dimensions?,Linear Regression,Support Vector Regression (SVR),Polynomial Regression,Ridge regression,Logistic regression,Random Regression,knn,decision tree,Multilayer regression,Boosted regression trees,"Other                                                           (change name of the column, put an x, add a new ""Other "" column in ""Categories"") ",Artificial Neural Network  (ANN),,,,,,"Other                          (change name of the column, put an x, add a new ""Other "" column in ""Types"") ",,,,Did they interpret the results physiologically?                        (put an x),Interpretations of the model/features           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,Journal     (put an x) ,Conference  (put an x) ,"Pre-print 
(put an x)","Other                          (change name of the column, put an x, add a new ""Other column""column in ""Source Type"")",,,,,,,DEAP    (put an x) ,AMIGOS     (put an x) ,MAHNOB    (put an x) ,CASE    (put an x) ,Ascertain,Cog.load,Multimodal Dyadic Behavior (MMDB),RECOLA,DECAF,Driving Workload,(AV+EC) 2015,Liris,SenseEmotion,PMEmo,AFEW,Hazumi1911,"Bio Vid Emo
DB",RCDAT,DREAMER,"Non-EEG Biosignals Data Set for
Assessment and Visualization of Neurological Status","Stress Recognition in Automobile
Drivers Data Set",PsPM-HRA1,"Other                          (change name of the column, put an x, add a new ""Other "" column in ""Which Database?"") ",,,Mean Age                                         e.g. 30,Median Age                                e.g. 30,Range Age:                                         e.g. 25-35,,,SAM    (put an x) ,Perceived Stress Scale (PSS),PANAS,DES,Affective Grid,"Other                          (change name of the column, put an x, add a new ""Other "" column in ""Affective questionaire"") ",,Valence,Arousal,Dominance,Like / Dislike,Familiarity,Stress,Engagement,"Other                          (change name of the column, put an x, add a new ""Other "" column in ""Dimension"") ",,Anger,Stress,Disgust,Fear,Sadness,Surprise,Happiness,Pleasant,Unpleasant,Anxiety,Neutral,Funny,Horror,Weepy,Boredom,Relaxation,Amusement,Confusion,Curiosity,Delight,flow/engagement,Frustration,Tenderness,Joy,"Other                          (change name of the column, put an x, add a new ""Other "" column in ""Categories"") ",,Other,,Visual,Auditory,Somatosensory,"Other measure                       (change name of the column, put an x, add a new ""Other "" column in ""Modalities"") ",,,,Pictures,Videos,Words,Other,Music,Other,,,,,,,,,,,,,,,,,,,,,,Thumb,Index,MIddle,Ring,Small,Little,Proximal,Medial,Distal,,,,,,,,,,,,,,,,,,,,,,,,Fully connected Layer?,Fully connected NN (or Multi layer perceptron),Convolutional NN,Recurrent NN,GRU (Gated Recurrent Units),LSTM,Cellular Neural Networks,AdaBoost DT,Quantum Neural Network (QNN),Probabilistic Neural Network (PNN),Backpropagation (BP),Extreme Learning Machine (ELM),ANN,Spiking Deep Belief Network (SDBN),"Other                          (change name of the column, put an x, add a new ""Other "" column in ""ANN"") ",,,,,,,,,,,,,,,,,Fully connected NN (or Multi layer perceptron),Convolutional NN,Recurrent NN,LSTM,PNN (probabilistic neural network),"Other                          (change name of the column, put an x, add a new ""Other "" column in ""ANN"") ",,Inter subject level,Intra subject level,,,,Public code?                                                                                (put an x),Where?,,,,,,,,,,,,,,,,,,,,,,,,,,
1,1,"Zangróniz, R., Martínez-Rodrigo, A., Pastor, J. M., López, M. T., & Fernández-Caballero, A. (2017). Electrodermal Activity Sensor for Classification of Calm/Distress Condition. Sensors (Basel, Switzerland), 17(10), E2324. https://doi.org/10.3390/s17102324
","Roberto Zangróniz, Arturo Martínez-Rodrigo, José Manuel Pastor, María T. López and Antonio Fernández-Caballero",Electrodermal Activity Sensor for Classification of Calm/Distress Condition,2017,Sensors,x,-,,-,Spain,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,45,20,-,-,-,Spain,Relies on other's questionnaire,x,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,-,"The participant sits in front of the experimentation monitor and the wearable is put in the wrist of the non-dominant hand (see Figure 1). In this regard, the experimentation monitor consists of a high resolution, 28 inch screen. When the technician verifies the proper functioning of the wearable and its communication with the software, the experiment starts. Firstly, the participant has to carefully read the general instructions of the experiment. Then, ten pictures that are labelled with high arousal and low valence are shown consecutively during 6 s each to the participant. Silences consisting of blank images with a fixed duration of 1 s are inserted before each picture used from the database. The pictures are selected randomly from the set of images that fulfil the condition. Therefore, the segment used for subsequent analysis is 70 s long (10 pictures 6 s duration, plus one blank image before each picture). In this sense, a single presentation of many stimuli presented for a short period of time might favour the continuity of emotional state [29]. Afterwards, a distracting task is presented to the participant so that his/her emotional state comes to neutral. Next, the experiment continues by showing randomly another set of ten images from IAPS that fulfil the condition to be part of those previously labelled as low arousal and high valence. Therefore, two segments of data from each individual are finally obtained, one for calm condition and another for distress condition. Again, silences are used before each picture. Lastly, the distracting task is offered again.",6s,-,-,-,Yes,-,not dominant,-,-,-,-,-,,-,-,-,-,x,,,-,dimensional,2,"calm, distress",-,-,-,,-,x,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,,-,-,x,-,?,-,-,-,-,89.18,-,-,93.9,85.36,-,,-,-,,-,,,,,,,,,-,,,x,,,
2,2,"Liu, M., Fan, D., Zhang, X., & Gong, X. (2017). Human Emotion Recognition Based on Galvanic Skin Response Signal Feature Selection and SVM. 157–160. Scopus. https://doi.org/10.1109/ICSCSE.2016.0051
","Mingyang Liu, Di Fan, Xiaohan Zhang, Xiaopeng Gong",Human Emotion Recognition Based on Galvanic Skin Response Signal Feature Selection and SVM,2016,2016 International Conference on Smart City and Systems Engineering,-,x,,-,China,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,NF,NF,NF,NF,NF,China,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,X,X,-,-,-,x,-,?,,,?,?,?,-,,-,-,-,,,,,,,-,"All subjects are Junior in Jilin University of
Changchun, China. Materials for eliciting emotions are 4
video fragments cut elaborately from large amounts of
movies representing 4 different emotions, such as happiness,
grief, anger, and fear. Each fragment last 4-5 minutes and
was connected with a short video which also last 4-5
minutes for calm recovery. And we add the emotion when
people stay in calm. So there are 5 emotions that make the
pattern recognition.",-,4-5 m,-,-,x,"e-Health Sensor
Platform V2.0",-,x,-,x,x,-,,-,-,-,-,-,,,-,categorical,5,"Happiness, Grief, Fear, Anger, Calm",-,x,-,-,-,,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,,-,,-,-,66.67,-,-,,,,,-,-,,-,,,,,,,,,-,-,-,x,,,
3,3,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
","Ayata, D., Yaslan, Y., & Kamasak, M. E.
",Emotion Based Music Recommendation System Using Wearable Physiological Sensors,2018,IEEE Transactions on Consumer Electronics,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,71.53,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
3,4,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
","Ayata, D., Yaslan, Y., & Kamasak, M. E.
",Emotion Based Music Recommendation System Using Wearable Physiological Sensors,2018,IEEE Transactions on Consumer Electronics,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,,,-,dimensional,2,"LV, HV",-,-,-,-,-,,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,,,-,,x,-,71.04,-,-,-,-,,,-,-,,-,,,,,,,,,-,-,-,x,,,
4,5,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061905","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,-,Austria,-,-,-,-,x,-,-,x,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,64,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
4,6,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061906","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,-,Austria,-,-,-,-,x,-,-,x,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,53,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
4,7,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061907","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,-,Austria,-,-,-,-,x,-,-,x,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,60,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
4,8,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061908","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,-,Austria,-,-,-,-,x,-,-,x,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,68,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
4,9,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061909","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,-,Austria,-,-,-,-,x,-,-,x,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,x,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,75,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
5,10,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384","Wei, J., Chen, T., Liu, G., & Yang, J.",Higher-order Multivariable Polynomial Regression to Estimate Human Affective States.,2016,Scientific Reports,x,-,,-,China,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,27,27,19.44,-,18-22,-,Relies on other's questionnaire,x,-,,,,-,x,x,x,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,IAPS,X,,,-,-,-,-,,-,-,-,,,,,,,-,-,6s,-,-,-,-,Biopac MP 150 system,-,-,-,-,-,-,-,-,-,-,-,-,,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,arousal,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,x,"By using the gradient fields (see Fig. 6(c,d)), the affective HMPM indirectly supports that the affective valence
and arousal have their origins in human brain’s motivational circuits. It seems to be reasonable that evaluative
affective components (valence and arousal) are associated with the broad functions of brain’s motivational circuits—appetitive subcircuits activation (pleasant) and defensive subcircuits activation (unpleasant) and an intensity of these two subcircuits activation (arousal)9
. Affective cues can induce skin conductance activations through
the limbic-hypothalamic EDA1 pathway, and the pleasant affect may additionally induce skin conductance activations through the premotor-basal ganglia EDA2 pathway37. The gradient field of valence (Fig. 6(c)) indicates
that the valence factor is mainly determined by the two dimensions: gain, that has neural activation intensity
meanings, and decay time constant, that has the meanings of skin conductance different pathways37,45. The gradient field of arousal (Fig. 6(d)) indicates that arousal factor is mainly determined by the gain dimension. These
findings may indirectly support prior researches9,48.
The HMPR is an important supplement to emotional estimation methodology. The HMPR, in fact, is not only
theoretically supported by the Taylor theorem, but also able to obtain an intuitive HMPM to efficiently estimate
the affective valence and arousal from pure skin conductance responses. Moreover, the result of comparing the
HMPR with the ANN models (see Supplementary Table S7) showed that both the HMPR and ANN can obtain
relative accurate computing results. Such accurate estimation results surely increases the impact in the wearable
computing fields such as smart watches, Mi Band, and Google Glass, etc. It is a trend now to detect human affect
by multimodal signals (e.g., neural activations, facial videos, voice recordings, body gestures, and physiological
signals, etc.",-,-,-,-,-,-,-,-,-,-,-,96,,,,,,,,,,,,,,,,
5,11,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384","Wei, J., Chen, T., Liu, G., & Yang, J.",Higher-order Multivariable Polynomial Regression to Estimate Human Affective States.,2016,Scientific Reports,x,-,,-,China,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,27,27,19.44,-,18-22,-,Relies on other's questionnaire,x,-,,,,-,x,x,x,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,IAPS,X,,,-,-,-,-,,-,-,-,,,,,,,-,-,6s,-,-,-,-,Biopac MP 150 system,-,-,-,-,-,-,-,-,-,-,-,-,,,-,categorical,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,,valence,-,-,x,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,98,,,,,,,,,,,,,,,,
6,12,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,-,USA,-,-,-,-,x,-,-,-,-,,,x,,,,,,,,,,,,,,,,-,100,-,,,0-30 months,USA,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,x,-,x,-,Multimodal Dyadic Behavior (MMDB),-,,,-,-,-,-,,-,x,-,,,,,,,-,"A set of semi-structured play interactions with adults, known as
Multimodal Dyadic Behavior (MMDB), was designed for the experimental sessions to stimulate different
emotions; event 1: ―greeting‖, event 2: ―playing with a ball‖, event 3: ―looking at a book and turning its
pages‖, event 4: ―using the book as a hat‖, and event 5: ―tickling‖. These experiments are aimed at
analyzing and decoding the children’s social communicative behavior at early ages and are consistent
with the Rapid-ABC play protocol",-,3m-5m,-,-,,,"Right, left",-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,2,"acceptance, boredom",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,75,81,,71,,,,75,,,,,,,,,,,,,,,,,,
6,13,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,-,USA,-,-,-,-,x,-,-,-,-,,,x,,,,,,,,,,,,,,,,-,100,-,-,-,0-30 months,USA,No,-,-,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,x,-,x,-,Rapid-ABC,-,,,-,-,-,-,,-,x,-,,,,,,,-,"A set of semi-structured play interactions with adults, known as
Multimodal Dyadic Behavior (MMDB), was designed for the experimental sessions to stimulate different
emotions; event 1: ―greeting‖, event 2: ―playing with a ball‖, event 3: ―looking at a book and turning its
pages‖, event 4: ―using the book as a hat‖, and event 5: ―tickling‖. These experiments are aimed at
analyzing and decoding the children’s social communicative behavior at early ages and are consistent
with the Rapid-ABC play protocol",-,3m-5m,-,-,,,"Right, left",-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,2,"acceptance, joy",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,84,85.00,,84,-,-,-,89,-,-,-,,,,,,,,,-,-,-,x,,,
6,14,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,-,USA,-,-,-,-,x,-,-,-,-,,,x,,,,,,,,,,,,,,,,-,100,-,-,-,0-30 months,USA,No,-,-,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,x,-,x,-,Rapid-ABC,-,,,-,-,-,-,,-,x,-,,,,,,,-,"A set of semi-structured play interactions with adults, known as
Multimodal Dyadic Behavior (MMDB), was designed for the experimental sessions to stimulate different
emotions; event 1: ―greeting‖, event 2: ―playing with a ball‖, event 3: ―looking at a book and turning its
pages‖, event 4: ―using the book as a hat‖, and event 5: ―tickling‖. These experiments are aimed at
analyzing and decoding the children’s social communicative behavior at early ages and are consistent
with the Rapid-ABC play protocol",-,3m-5m,-,-,,,"Right, left",-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,2,"boredom, joy",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,90,82,,88,-,-,-,87,-,-,-,,,,,,,,,-,-,-,x,,,
6,15,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,-,USA,-,-,-,-,x,-,-,-,-,,,x,,,,,,,,,,,,,,,,-,100,-,-,-,0-30 months,USA,No,-,-,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,x,-,x,-,Rapid-ABC,-,,,-,-,-,-,,-,x,-,,,,,,,-,"A set of semi-structured play interactions with adults, known as
Multimodal Dyadic Behavior (MMDB), was designed for the experimental sessions to stimulate different
emotions; event 1: ―greeting‖, event 2: ―playing with a ball‖, event 3: ―looking at a book and turning its
pages‖, event 4: ―using the book as a hat‖, and event 5: ―tickling‖. These experiments are aimed at
analyzing and decoding the children’s social communicative behavior at early ages and are consistent
with the Rapid-ABC play protocol",-,3m-5m,-,-,,,"Right, left",-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,3,"acceptance, joy, boredom",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,69,,,,-,-,-,,-,-,-,,,,,,,,,-,-,-,x,,,
6,16,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,-,USA,-,-,-,-,x,-,-,-,-,,,x,,,,,,,,,,,,,,,,-,100,-,-,-,0-30 months,USA,No,-,-,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,x,-,x,-,Rapid-ABC,-,,,-,-,-,-,,-,x,-,,,,,,,-,"A set of semi-structured play interactions with adults, known as
Multimodal Dyadic Behavior (MMDB), was designed for the experimental sessions to stimulate different
emotions; event 1: ―greeting‖, event 2: ―playing with a ball‖, event 3: ―looking at a book and turning its
pages‖, event 4: ―using the book as a hat‖, and event 5: ―tickling‖. These experiments are aimed at
analyzing and decoding the children’s social communicative behavior at early ages and are consistent
with the Rapid-ABC play protocol",-,3m-5m,-,-,,,"Right, left",-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,2,"acceptance, boredom",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,70,71,,70,-,-,-,63,-,-,-,,,,,,,,,-,-,-,x,,,
6,17,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,-,USA,-,-,-,-,x,-,-,-,-,,,x,,,,,,,,,,,,,,,,-,100,-,-,-,0-30 months,USA,No,-,-,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,x,-,x,-,Rapid-ABC,-,,,-,-,-,-,,-,x,-,,,,,,,-,"A set of semi-structured play interactions with adults, known as
Multimodal Dyadic Behavior (MMDB), was designed for the experimental sessions to stimulate different
emotions; event 1: ―greeting‖, event 2: ―playing with a ball‖, event 3: ―looking at a book and turning its
pages‖, event 4: ―using the book as a hat‖, and event 5: ―tickling‖. These experiments are aimed at
analyzing and decoding the children’s social communicative behavior at early ages and are consistent
with the Rapid-ABC play protocol",-,3m-5m,-,-,,,"Right, left",-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,2,"acceptance, joy",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,82,80,-,82,-,-,-,81,-,-,-,,,,,,,,,-,-,-,x,,,
6,18,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,-,USA,-,-,-,-,x,-,-,-,-,,,x,,,,,,,,,,,,,,,,-,100,-,-,-,0-30 months,USA,No,-,-,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,x,-,x,-,Rapid-ABC,-,,,-,-,-,-,,-,x,-,,,,,,,-,"A set of semi-structured play interactions with adults, known as
Multimodal Dyadic Behavior (MMDB), was designed for the experimental sessions to stimulate different
emotions; event 1: ―greeting‖, event 2: ―playing with a ball‖, event 3: ―looking at a book and turning its
pages‖, event 4: ―using the book as a hat‖, and event 5: ―tickling‖. These experiments are aimed at
analyzing and decoding the children’s social communicative behavior at early ages and are consistent
with the Rapid-ABC play protocol",-,3m-5m,-,-,,,"Right, left",-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,2,"boredom, joy",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,85,77,,85,-,-,-,86,-,-,-,,,,,,,,,-,-,-,x,,,
6,19,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,-,USA,-,-,-,-,x,-,-,-,-,,,x,,,,,,,,,,,,,,,,-,100,-,-,-,0-30 months,USA,No,-,-,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,x,-,x,-,Rapid-ABC,-,,,-,-,-,-,,-,x,-,,,,,,,-,"A set of semi-structured play interactions with adults, known as
Multimodal Dyadic Behavior (MMDB), was designed for the experimental sessions to stimulate different
emotions; event 1: ―greeting‖, event 2: ―playing with a ball‖, event 3: ―looking at a book and turning its
pages‖, event 4: ―using the book as a hat‖, and event 5: ―tickling‖. These experiments are aimed at
analyzing and decoding the children’s social communicative behavior at early ages and are consistent
with the Rapid-ABC play protocol",-,3m-5m,-,-,,,"Right, left",-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,3,"baseline, stress, amusement",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,66,-,,,-,-,-,,-,-,-,,,,,,,,,-,-,-,x,,,
7,20,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E4,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,3,"baseline, stress, amusement",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,54.36,-,45.48,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,21,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E5,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,3,"baseline, stress, amusement",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,56.67,-,45.74,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,22,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E6,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,3,"baseline, stress, amusement",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,59.85,-,49.06,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,23,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E7,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,3,"baseline, stress, amusement",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,62.32,-,42.72,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,24,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E8,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,categorical,3,"baseline, stress, amusement",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,54.98,-,45.2,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,25,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,categorical,3,"baseline, stress, amusement",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,48.49,-,43.88,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,26,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,categorical,3,"baseline, stress, amusement",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,45,-,42.4,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,27,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,categorical,3,"baseline, stress, amusement",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,54.06,-,48.33,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,28,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,categorical,3,"baseline, stress, amusement",-,-,-,-,x,-,-,,,,,,,,,,,-,,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,67.07,-,46.83,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,29,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,dimensional,2,"stress,  not stress",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,40.03,-,37.26,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,30,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E6,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,dimensional,2,"stress,  not stress",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,76.21,-,70.95,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,31,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E6,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,dimensional,2,"stress,  not stress",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,76.29,-,70.88,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,32,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E6,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,dimensional,2,"stress,  not stress",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,x,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,79.71,-,75.34,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,33,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E6,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,dimensional,2,"stress,  not stress",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,78.08,-,69.86,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,34,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,Empatica E6,not dominant,-,-,-,-,-,-,-,-,-,-,x,,,-,dimensional,2,"stress,  not stress",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,73.13,-,68.3,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,35,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,dimensional,2,"stress,  not stress",-,-,-,-,-,x,-,,,,,,,,,,,-,,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,73.55,-,69.88,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,36,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,dimensional,2,"stress,  not stress",-,-,-,-,-,x,-,,,,,,,,,,,-,,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,77.51,-,73.63,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,37,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,dimensional,2,"stress,  not stress",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,75.5,-,71.97,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,38,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,dimensional,2,"stress,  not stress",-,-,-,-,x,-,-,,,,,,,,,,,-,,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,81.7,-,74.51,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
7,39,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,-,Germany,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,15,3,27.5,-,-,Germany,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,x,TSST,-,,,-,-,-,x,,-,x,,x,,,,,,-,,-,-,-,-,No,RespiBAN Professional,-,-,-,-,-,-,-,-,-,-,-,-,x,,-,dimensional,2,"stress,  not stress",-,-,x,-,-,x,-,,,,,,,,,,,-,,-,,-,,-,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,69.73,-,66.64,-,-,-,,-,-,,-,,,,,,,,,-,-,-,x,,,
8,40,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180","Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G.",A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices.,2018,"2018 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2018",-,x,,-,UK,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,6,-,-,-,-,-,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,"The intention is to collect data from users in their natural environment, over a prolonged period.",-,-,-,-,-,Microsoft Band 2,-,-,-,-,-,-,-,-,-,-,-,x,-,,-,dimensional,2,"relaxed, stress",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,54,-,65,-,55,35,-,-,,-,,,,,,,,,-,-,-,x,,,
8,41,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180","Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G.",A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices.,2018,"2018 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2018",-,x,,-,UK,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,6,-,-,-,-,-,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,x,-,x,-,-,,,,,,,,,,,,,,,,,,,"The intention is to collect data from users in their natural environment, over a prolonged period.",-,-,-,-,-,Microsoft Band 2,-,-,-,-,-,-,-,-,-,-,-,x,-,,-,categorical,2,"relaxed, stress",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,52,-,80,-,73,20,-,-,,-,,,,,,,,,-,-,-,x,,,
8,42,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180","Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G.",A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices.,2018,"2018 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2018",-,x,,-,UK,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,6,-,-,-,-,-,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,,,,,,x,,,,,,,,,,,,,,,,,,,,,"The intention is to collect data from users in their natural environment, over a prolonged period.",-,-,-,-,-,Microsoft Band 2,-,-,-,-,-,-,-,-,-,-,-,x,-,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,56,-,56,-,43,44,-,-,,-,,,,,,,,,-,-,-,x,,,
9,43,"Amalan, S., Shyam, A., Anusha, A. S., Preejith, S. P., Tony, A., Jayaraj, J., & Mohanasankar, S. (2018). Electrodermal Activity based Classification of Induced Stress in a Controlled Setting. MeMeA 2018 - 2018 IEEE International Symposium on Medical Measurements and Applications, Proceedings. Scopus. https://doi.org/10.1109/MeMeA.2018.8438703","Amalan, S., Shyam, A., Anusha, A. S., Preejith, S. P., Tony, A., Jayaraj, J., & Mohanasankar, S.",Electrodermal Activity based Classification of Induced Stress in a Controlled Setting.,2018,"MeMeA 2018 - 2018 IEEE International Symposium on Medical Measurements and Applications, Proceedings",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,9,,,21-30,India,No,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,-,-,,,x,-,Trier Social Stress Test (TSST),-,,,-,-,-,x,,-,-,-,-,,,,,,-,"A participant is given 3-5 minutes to prepare for a 5-minute presentation, followed by a 5-minute mental arithmetic task of counting numbers backwards continuously in large steps. The unanticipated element is that a large audience enters the room after the preparatory phase trying to induce stress in the form of stage fright. In the end, the participant is briefed and made to relax.",15m,-,-,-,x,"Gen II integrated wearable device from Analog Devices, Inc",-,-,-,-,-,-,-,-,-,-,-,x,-,,-,dimensional,2,"stress, not stress",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,,,,94,93,,,,,,,,,,,,,,,,,,,,,
10,44,"Machot, F. A., Ali, M., Ranasinghe, S., Mosa, A. H., & Kyandoghere, K. (2018). Improving subject-independent human emotion recognition using electrodermal activity sensors for active and assisted living. 222–228. Scopus. https://doi.org/10.1145/3197768.3201523","Machot, F. A., Ali, M., Ranasinghe, S., Mosa, A. H., & Kyandoghere, K",Improving subject-independent human emotion recognition using electrodermal activity sensors for active and assisted living.,2018,ACM International Conference Proceeding Series,-,x,,-,austria,-,-,-,-,x,-,-,x,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,,-,-,-,-,-,68,75,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
11,45,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589","Girardi, D., Lanubile, F., & Novielli, N.",Emotion detection using noninvasive low cost sensors.,2018,"2017 7th International Conference on Affective Computing and Intelligent Interaction, ACII 2017",-,x,,-,Italy,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,19,3,-,-,20-40,?,Relies on other's questionnaire,x,-,,,,-,x,x,x,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,,,,,,,,,,,,,,,,,,,,,,,,-,Shimmer GSR+Unit2,,,,,,,,,,,,,,,,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,67,63,64,,,,,,,,,,,,,,,,,,,,,,
11,46,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589","Girardi, D., Lanubile, F., & Novielli, N.",Emotion detection using noninvasive low cost sensors.,2018,"2017 7th International Conference on Affective Computing and Intelligent Interaction, ACII 2017",-,x,,-,Italy,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,19,3,-,-,20-40,?,Relies on other's questionnaire,x,-,,,,-,x,x,x,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shimmer GSR+Unit2,,,,,,,,,,,,,,,,dimensional,2,"LV, HV",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,58,35,50,,,,,,,,,,,,,,,,,,,,,,
12,47,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076","Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z.","Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions.",2018,International Journal of Medical Engineering and Informatics,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,20,-,22.83,-,19-30,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,-,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-,-,"Grove
(a standalone LM324 quadruple operational amplifier based on EDA sensor kit)",-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"neutral, stress",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"Figure 6 shows similar EDA signal behaviour during resting and recovery phases.
Signal constantly rises to a limit at this point before declining and rising again. Signals at
these phases had low fluctuation and sudden rise of amplitude because the subject sits in
a relaxed phase. This situation is similar to a neutral simulated driving condition. The
most frequent fluctuation of EDA signal was observed during anger-simulated driving,
followed by stress and neutral driving. This finding shows that the subject is endowed
with the most intense sympathetic response during anger, followed by stress and neutral
emotion. Further processing was required to determine significant differences of EDA
properties during different simulated driving tasks.
Figure 7 indicates that band-pass-filtered EDA signals at neutral simulated driving
tasks had similar response to control and recovery sessions, wherein the subject
demonstrates the least physiological response because of the simplicity of driving
scenario. The subject possesses a high level of physiological response during states of
anger than in stress-simulated driving tasks. The overall amplitude of signals is high and
the fluctuations of spikes occurred frequently.",-,-,100,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
12,48,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076","Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z.","Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions.",2018,International Journal of Medical Engineering and Informatics,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,20,-,22.83,-,19-30,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,-,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-,-,"Grove
(a standalone LM324 quadruple operational amplifier based on EDA sensor kit)",-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"neutral, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,100,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
12,49,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076","Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z.","Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions.",2018,International Journal of Medical Engineering and Informatics,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,20,-,22.83,-,19-30,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,-,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-,-,"Grove
(a standalone LM324 quadruple operational amplifier based on EDA sensor kit)",-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"stress, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,65,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
12,50,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076","Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z.","Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions.",2018,International Journal of Medical Engineering and Informatics,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,20,-,22.83,-,19-30,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,-,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-,-,"Grove
(a standalone LM324 quadruple operational amplifier based on EDA sensor kit)",-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"neutral, stress, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,77,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
13,51,"Setyohadi, D. B., Kusrohmaniah, S., Gunawan, S. B., Pranowo, & Prabuwono, A. S. (2018). Galvanic skin response data classification for emotion detection. International Journal of Electrical and Computer Engineering, 8(5), 4004–4014. Scopus. https://doi.org/10.11591/ijece.v8i5.pp4004-4014","Setyohadi, D. B., Kusrohmaniah, S., Gunawan, S. B., Pranowo, & Prabuwono, A. S.",Galvanic skin response data classification for emotion detection.,2018,International Journal of Electrical and Computer Engineering,x,-,,-,Indonesia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,39,-,-,-,-,Indonesia,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,x,-,-,-,,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"neutral, negative, positive",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,75.65,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
14,52,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,interest,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.28,-,0.14,,,,,,,-,-,-,x,,,
14,53,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,curiosity,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.27,-,0.2,,,,,,,-,-,-,x,,,
14,54,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,coping potential,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.19,-,0.24,,,,,,,-,-,-,x,,,
14,55,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,novelty,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.28,-,0.23,,,,,,,-,-,-,x,,,
14,56,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,complexity,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.23,-,0.29,,,,,,,-,-,-,x,,,
14,57,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,interest,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.29,-,0.05,,,,,,,-,-,-,x,,,
14,58,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,curiosity,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.32,-,0.03,,,,,,,-,-,-,x,,,
14,59,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,coping potential,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.24,-,0.08,,,,,,,-,-,-,x,,,
14,60,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,novelty,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.33,-,0.14,,,,,,,-,-,-,x,,,
14,61,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,complexity,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,-,x,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.28,-,0.13,,,,,,,-,-,-,x,,,
14,62,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,interest,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,x,-,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.3,-,0.02,,,,,,,-,-,-,x,,,
14,63,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,curiosity,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,x,-,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.34,-,-0.04,,,,,,,-,-,-,x,,,
14,64,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,coping potential,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,x,-,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.24,-,0.01,,,,,,,-,-,-,x,,,
14,65,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,novelty,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,x,-,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.34,-,0.02,,,,,,,-,-,-,x,,,
14,66,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,-,Switzerland,-,x,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,52,33,25.7,-,-,-,-,-,-,,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,complexity,-,-,-,,,x,,,,,-,-,-,-,-,-,-,-,x,-,,-,-,,,-,-,-,-,-,-,-,-,-,-,0.29,-,0.01,,,,,,,-,-,-,x,,,
15,67,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,x,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,56.67,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,68,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,66.67,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,69,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,70,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,70,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,80,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,71,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"anger, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,66.67,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,72,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"anger, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,83.33,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,73,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,70.83,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,74,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,79.17,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,75,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,66.67,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,76,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"anger, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,75,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,77,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"anger, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,60,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,78,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,75,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,79,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,79.17,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,80,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"happy, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,79.17,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,81,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"anger, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,80,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,82,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"anger, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,83.33,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,83,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,x,-,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"anger, stress",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,65,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,84,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,x,-,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"anger, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,100,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,85,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,x,-,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"stress, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,100,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,86,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"happy, anger, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,40,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,87,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"happy, anger, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,46.67,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,88,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"happy, anger, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,30.56,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,89,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"happy, anger, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,44.44,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,90,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"happy, anger, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,40,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,91,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"happy, anger, recovery",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,53.33,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
15,92,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,25,-,23.92,-,21-39,Malasya,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,x,-,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,-,-,-,-,-,-,-,Grove,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"anges, stress, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,76.67,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
16,93,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660","Wiem, M. B. H., & Lachiri, Z.",Emotion sensing from physiological signals using three defined areas in arousal-valence model.,2017,"2017 International Conference on Control, Automation and Diagnosis, ICCAD 2017",-,x,,-,Tunisia,-,-,-,-,x,-,-,x,-,,,-,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"calm arousal, medium arousal, excited arousal",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,,-,-,-,-,-,50.52,,,,,,,,,,,,,,,,,,,,,,,,,
16,94,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660","Wiem, M. B. H., & Lachiri, Z.",Emotion sensing from physiological signals using three defined areas in arousal-valence model.,2017,"2017 International Conference on Control, Automation and Diagnosis, ICCAD 2017",-,x,,-,Tunisia,-,-,-,-,x,-,-,x,-,,,-,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LV, neutral valence, HV",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,,-,-,-,-,-,48.93,,,,,,,,,,,,,,,,,,,,,,,,,
17,95,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"happiness, rest",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,67.58,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,96,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"happiness, rest",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,74.7,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,97,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"happiness, rest",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.84,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,98,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"happiness, rest",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.82,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,99,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"happiness, others emotions",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,69.29,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,100,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"happiness, others emotions",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,55.88,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,101,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"happiness, others emotions",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.92,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,102,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"happiness, others emotions",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.91,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,103,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"peacefulness, rest",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,74.87,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,104,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"peacefulness, rest",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,74.86,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,105,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"peacefulness, rest",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.91,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,106,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"peacefulness, rest",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.93,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,107,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"peacefulness, others emotions",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,64.47,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,108,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"peacefulness, others emotions",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,57.55,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,109,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"peacefulness, others emotions",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,75.34,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,110,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"peacefulness, others emotions",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,70.37,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,111,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"sadness, rest",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,68.69,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,112,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"sadness, rest",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,74.75,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,113,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"sadness, rest",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.98,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,114,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"sadness, rest",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,95.49,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,115,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"sadness, others emotions",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,67.35,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,116,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"sadness, others emotions",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,57.97,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,117,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"sadness, others emotions",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,74.93,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,118,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"sadness, others emotions",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,67.99,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,119,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"fear, rest",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,76.09,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,120,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"fear, rest",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,74.87,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,121,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"fear, rest",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.89,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,122,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"fear, rest",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.84,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,123,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"fear, others emotions",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,70.21,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,124,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"fear, others emotions",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,69.24,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,125,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"fear, others emotions",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.95,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
17,126,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,,,,,,,,,,,,,,,,-,35,14,-,-,21-25,Iran,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,-,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-,-,"PowerLab (manufactured
by ADInstruments)",-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,2,"fear, others emotions",-,-,-,x,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"After preprocessing and windowing the signals, several nonlinear features were extracted (Fig. 1). Lower SD1/SD2 ratios
emphasized on a more regular pattern of signals during happiness and sadness compared to fear and peacefulness. SD1
and SD2 measures mainly reflect parasympathetic, and both
sympathetic and parasympathetic activities, respectively. The
results revealed that based on the dimensional model, opposite emotions may have the same patterns of Poincare indices.
In addition, positive LEs revealed that GSR signals have
a chaotic trajectory which are derived from a chaotic system. The higher entropies indicated the higher irregularity of
GSR signals during fear. The lower RQA measures during all
emotional stimuli, a higher dimensional GSR signal can be
concluded. VMAX and LAM mark a time period in which the
state changes very slowly or does not change. Consequently,
higher values of the indices during rest condition reveal the
system stability. Next, three feature selection algorithms were
applied to the data",-,-,99.96,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
18,127,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533","Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B.",End-to-end learning for dimensional emotion recognition from physiological signals.,2017,Proceedings - IEEE International Conference on Multimedia and Expo,-,x,,-,Germany,-,-,-,-,x,-,-,-,-,,,-,x,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,,,,-,1,arousal,-,-,-,,,,,,,,,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,0.284,,,,,,,,-,,,,,,
18,128,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533","Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B.",End-to-end learning for dimensional emotion recognition from physiological signals.,2017,Proceedings - IEEE International Conference on Multimedia and Expo,-,x,,-,Germany,-,-,-,-,x,-,-,-,-,,,-,x,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,valence,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,0.336,,,,,,,,-,,,,,,
19,129,"Hernández-García, A., Fernández-Martínez, F., & Díaz-De-maría, F. (2017). Emotion and attention: Predicting electrodermal activity through video visual descriptors. 914–923. Scopus. https://doi.org/10.1145/3106426.3109418","Hernández-García, A., Fernández-Martínez, F., & Díaz-De-maría, F.",Emotion and attention: Predicting electrodermal activity through video visual descriptors.,2017,"Proceedings - 2017 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,22,12,-,-,21-59,-,No,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,"In order to collect ground truth data for the present study, the
EDA was measured on 22 subjects, 10 men and 12 women, with ages
between 21 and 59 years old, while they watched a concantenation
of videos. The data set consists of 44 videos with an average duration
of 44 seconds (σ = 21) which belong to the awarded spots at the 2002
Cannes International Advertising Festival. The whole sequence of
spots was projected in a movie theater while the EDA was recorded
on each subject by means of the Sociograph device",-,-,44s,-,-,Sociograph,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,1,arousal,x,-,-,,,,,,,,-,-,-,-,-,-,-,-,?,?,-,x,"The relationship between visual descriptors and other kind of
subjective information like aesthetics or appeal reported deliberately by participants via a score, for instance, had been already
demonstrated in previous works [16]. However, finding some correlation with EDA has a great interest because it is a psychophysiological reaction controlled by the autonomous nervous system,
thus it is automatic and is directly related to actual emotional and
attentional activation, avoiding the implicit bias of opinions and
judgments.
Correlation between the set of visual descriptors and SCL and
SCR was not so clear as in the case of SUM. One explanation for
this is that these measures alone reflect subtleties that are more
difficult to capture by simple methods and a relatively small set of
features",-,-,-,-,-,-,-,-,-,-,0.06,-,0.93,,,,,,,,,-,,,,,,
20,130,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",,"Wiem, M. B. H., & Lachiri, Z.",2017,"4th International Conference on Control Engineering and Information Technology, CEIT 2016",x,-,,-,Tunisia,-,-,-,-,x,-,-,x,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,62.23,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
20,131,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",,"Wiem, M. B. H., & Lachiri, Z.",2017,"4th International Conference on Control Engineering and Information Technology, CEIT 2016",x,-,,-,Tunisia,-,-,-,-,x,-,-,x,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,,-,-,-,-,x,-,-,-,-,-,-,55.78,-,-,-,-,-,-,,-,-,-,-,,,,,,,,-,-,-,x,,,
21,132,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,133,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,134,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,135,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,136,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,137,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,138,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,139,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,140,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,141,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,142,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,143,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,-,Germany,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,8,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-,-,Biosignalplux,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,dimensional,5,"HAHV, HALV, LAHV, LALV, neutral",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Physiological measures in the real world are more complex
than those under laboratory conditions. In this paper, we have
shown that ER (Emotion Recognition) by using physiological
signals is influenced by human movements. We investigate
the recognition of a group of emotions by using various
physiological signals and commonly used algorithms in both,
the lab and a real-world scenario. Our results show that
the ST (Skin Temperature), EDA (Electrodermal Activity),
and EMG (Electromyography) signals achieve the highest
accuracies when the data is collected when the participants
are at rest. Our results show that the Decision Tree is the
best classification algorithm. From our work we can conclude,
that human movement influences the physiological signals
measured from a user, and therefore, influence the results of
the ER. For this reason, one must take this effect into account
when developing models for emotion recognition. ER models
based on controlled experiments cannot be accurately used to
recognize emotion in real-world scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,144,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072","Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A.",Human emotion classifications for automotive driver using skin conductance response signal.,2017,"2016 International Conference on Advances in Electrical, Electronic and Systems Engineering, ICAEES 2016",-,x,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,19,-,-,-,23-36,Malasya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,-,-,-,x,,-,,,,,,,,,,,,,,,,,,,,,,,-,BioRadio 150 wireless sensor,-,-,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,5,"happy, sad, anger, disgust, fear",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,,,,-,-,,,67.1-72.1,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
22,145,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072","Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A.",Human emotion classifications for automotive driver using skin conductance response signal.,2017,"2016 International Conference on Advances in Electrical, Electronic and Systems Engineering, ICAEES 2016",-,x,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,19,-,-,-,23-36,Malasya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,5s,-,-,-,-,BioRadio 150 wireless sensor,-,-,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,5,"happy, sad, anger, disgust, fear",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,,,,-,-,,,43.1-62.5,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
22,146,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072","Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A.",Human emotion classifications for automotive driver using skin conductance response signal.,2017,"2016 International Conference on Advances in Electrical, Electronic and Systems Engineering, ICAEES 2016",-,x,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,19,-,-,-,23-36,Malasya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,1m-2-m,-,-,-,BioRadio 150 wireless sensor,-,-,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,5,"happy, sad, anger, disgust, fear",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,,,,-,-,,,40.2-53.2,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
23,147,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130","Ayata, D., Yaslan, Y., & Kamasak, M.","Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches.",2017,"2016 Medical Technologies National Conference, TIPTEKNO 2016",-,x,,-,Turkey,-,-,-,-,x,x,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,1280,-,-,-,-,71.53,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
23,148,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130","Ayata, D., Yaslan, Y., & Kamasak, M.","Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches.",2017,"2016 Medical Technologies National Conference, TIPTEKNO 2016",-,x,,-,Turkey,-,-,-,-,x,x,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,71.04,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
24,149,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677","Greco, A., Valenza, G., Citi, L., & Scilingo, E. P.",Arousal and valence recognition of affective sounds based on electrodermal activity.,2017,IEEE Sensors Journal,x,-,,-,Italy,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,25,-,-,-,25-35,-,Relies on  other's questionnaire,-,-,-,,,-,x,x,x,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,IADS,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"During the
experiment, participants were seated in a comfortable chair
in a controlled environment while listening to the IADS
sounds. Each subject was left alone in the room where the
experiment took place for the whole duration (29 minutes).
The acoustic stimulation was performed by using headphones
while the subject’s eyes were closed, to avoid any kind of
visual interference.
The experimental protocol consisted of 8 sessions: after
an initial resting session of 5 minutes, three arousal sessions
alternated with neutral sessions (see Figure 2). Each arousal
and neutral session was different from the others in regard
to the arousal level (labeled as N (neutral), L (low), M
(medium) and H (high)). We selected four arousal ranges that
were not overlapped. Such levels were set according to the
IADS scores reported in table I. Within each arousing session,
the acoustic stimuli were selected to have both negative and
positive valence. Each neutral session lasted 1 minute and 28
seconds, while the three arousal sessions had a duration of
3 minutes and 40 seconds, 4 minutes, and 5 minutes and 20
seconds, respectively. The different duration of each arousal
session is due to the different length of acoustic stimuli having
the same range of positive and negative valence.",-, 1 m 28 s - 5 m 20 s,-,-,-,BIOPAC MP150 ,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,dimensional,3,"LA, medium arousal, HA",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,75,-,-,-,-,77.33,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
24,150,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677","Greco, A., Valenza, G., Citi, L., & Scilingo, E. P.",Arousal and valence recognition of affective sounds based on electrodermal activity.,2017,IEEE Sensors Journal,x,-,,-,Italy,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,25,-,-,-,25-35,-,Relies on  other's questionnaire,-,-,-,,,-,x,x,x,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,IADS,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,"During the
experiment, participants were seated in a comfortable chair
in a controlled environment while listening to the IADS
sounds. Each subject was left alone in the room where the
experiment took place for the whole duration (29 minutes).
The acoustic stimulation was performed by using headphones
while the subject’s eyes were closed, to avoid any kind of
visual interference.
The experimental protocol consisted of 8 sessions: after
an initial resting session of 5 minutes, three arousal sessions
alternated with neutral sessions (see Figure 2). Each arousal
and neutral session was different from the others in regard
to the arousal level (labeled as N (neutral), L (low), M
(medium) and H (high)). We selected four arousal ranges that
were not overlapped. Such levels were set according to the
IADS scores reported in table I. Within each arousing session,
the acoustic stimuli were selected to have both negative and
positive valence. Each neutral session lasted 1 minute and 28
seconds, while the three arousal sessions had a duration of
3 minutes and 40 seconds, 4 minutes, and 5 minutes and 20
seconds, respectively. The different duration of each arousal
session is due to the different length of acoustic stimuli having
the same range of positive and negative valence.",-, 1 m 28 s - 5 m 20 s,-,-,-,BIOPAC MP150 ,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,dimensional,2,"LV, HV",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,150,-,-,-,-,84,-,-,-,-,-,-,-,-,-,-,,,,,,,,,-,-,-,x,,,
25,151,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.","Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I.",A design framework for human emotion recognition using electrocardiogram and skin conductance response signals.,2017,Journal of Engineering Science and Technology,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,23,15,-,-,23-36,Malasya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,each picture display contained five pictures and was displayed for 4 seconds each,4s,-,-,-,-,BioRadio 150,-,-,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,5,"happy, sad, anger, disgust, fear",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,,,,-,-,-,-,55.28,-,-,66.2,68.56,-,-,-,-,-,-,,,,,,,,,-,,-,x,,,
25,152,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.","Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I.",A design framework for human emotion recognition using electrocardiogram and skin conductance response signals.,2017,Journal of Engineering Science and Technology,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,23,15,-,-,23-36,Malasya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,x,-,-,-,x,,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,,4s,-,-,-,-,BioRadio 150,-,-,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,5,"happy, sad, anger, disgust, fear",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,,,,-,-,-,-,48.46,-,-,65,62.18,-,-,-,-,-,-,,,,,,,,,-,,-,x,,,
25,153,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.","Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I.",A design framework for human emotion recognition using electrocardiogram and skin conductance response signals.,2017,Journal of Engineering Science and Technology,x,-,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,23,15,-,-,23-36,Malasya,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,BioRadio 150,-,-,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,5,"happy, sad, anger, disgust, fear",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,,,,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,,,-,-,-,-,-,69.86,-,-,58.96,70.94,-,-,-,-,-,-,,,,,,,,,-,,-,x,,,
26,154,"Zhang, Q., Lai, X., & Liu, G. (2016). Emotion recognition of GSR based on an improved quantum neural network. 1, 488–492. Scopus. https://doi.org/10.1109/IHMSC.2016.66","Zhang, Q., Lai, X., & Liu, G.",Emotion recognition of GSR based on an improved quantum neural network.,2016,"Proceedings - 2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2016",-,x,,-,China,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,35,-,-,-,18-22,China,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,,,,,x,-,-,,,,,,,,,,,,,,,,,,,"In the experiment, the process of collecting
the signal of the emotions was playing neutral materials
which were to keep mood in peace for 2 minutes,
introducing the materials background and the target
emotion for 30 seconds, and playing the materials of the
target emotion for 5 minutes. If the subject felt that a
corresponding emotion was generated, the button next to
the seat should be pressed to make a mark. Finally, after
collecting the signal of target emotion, the subjects
needed to evaluate the effectiveness of emotion had been
induced, which the type of the emotion had been induced
and intensity of emotion (very strong, strong, weak, very
weak).",,,,,-,MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"happy, grief, fear, angry, calm",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,x,,,,,,,,,-,-,-,,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,175,-,-,-,-,84.9,-,-,-,-,-,,-,-,-,-,-,,,,,,,,-,-,-,x,,,
27,155,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",A novel signal-based fusion approach for accurate music emotion recognition.,2016,"Biomedical Engineering - Applications, Basis and Communications",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,Iran,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"56 short musical excerpts with 14 stimuli
per each emotional set were selected, which was
alidated by Vieillard et al.36 They consisted of a melody
with accompaniment composed in piano timbre, and
followed the rules of the Western tonal system",15m,-,-,-,-,PowerLab,-,-,-,x,-,x,-,-,-,-,-,-,-,,-,categorical,5,"happy, sad, scary, peaceful, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,69.93,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
27,156,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",A novel signal-based fusion approach for accurate music emotion recognition.,2016,"Biomedical Engineering - Applications, Basis and Communications",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,Iran,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"56 short musical excerpts with 14 stimuli
per each emotional set were selected, which was
alidated by Vieillard et al.36 They consisted of a melody
with accompaniment composed in piano timbre, and
followed the rules of the Western tonal system",15m,-,-,-,-,PowerLab,-,-,-,x,-,x,-,-,-,-,-,-,-,,-,dimensional,3,"LA, HA, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,79.02,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
27,157,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",A novel signal-based fusion approach for accurate music emotion recognition.,2016,"Biomedical Engineering - Applications, Basis and Communications",x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,Iran,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"56 short musical excerpts with 14 stimuli
per each emotional set were selected, which was
alidated by Vieillard et al.36 They consisted of a melody
with accompaniment composed in piano timbre, and
followed the rules of the Western tonal system",15m,-,-,-,-,PowerLab,-,-,-,x,-,x,-,-,-,-,-,-,-,,-,dimensional,3,"LV, HV, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,81.82,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,158,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"happy, sad",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,100,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,159,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"happy, sad",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,100,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,160,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"happy, sad",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,100,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,161,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"happy, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,83.13,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,162,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"happy, neutral",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,90.58,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,163,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"happy, neutral",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,84.58,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,164,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"sad, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,100,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,165,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"sad, neutral",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,95.76,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
28,166,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,-,India,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,-,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,2m34s,-,-,-,-,LabVIEW,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"sad, neutral",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,97.75,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
29,167,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,-,Canada,-,x,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,33,12,29.7,-,-,Canada,Yes,-,-,-,,,-,x,x,x,-,x,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,,-,-,-,-,-,x,-,,x,"Furthermore, it is worth noting that quality adaptive arousal and
valence classifiers performed significantly above chance on all the
modalities except on valence recognition using GSR that is in corroboration
with the finding in [7]. It is worthy to mention that low
unimodal performances on GSR could be due to the fact that GSR
responses are slow. Therefore, GSR is an unsuitable modality for
an experiment with short recordings like ours.",-,-,-,-,52,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
29,168,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,-,Canada,-,x,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,33,12,29.7,-,-,Canada,Yes,-,-,-,,,-,x,x,x,-,x,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,x,,,,,,,,,,,-,-,,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,,-,-,-,-,-,x,-,,x,"Furthermore, it is worth noting that quality adaptive arousal and
valence classifiers performed significantly above chance on all the
modalities except on valence recognition using GSR that is in corroboration
with the finding in [7]. It is worthy to mention that low
unimodal performances on GSR could be due to the fact that GSR
responses are slow. Therefore, GSR is an unsuitable modality for
an experiment with short recordings like ours.",-,-,-,-,55,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
29,169,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,-,Canada,-,x,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,33,12,29.7,-,-,Canada,Yes,-,-,-,,,-,x,x,x,-,x,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,x,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,,-,-,-,-,-,x,-,,x,"Furthermore, it is worth noting that quality adaptive arousal and
valence classifiers performed significantly above chance on all the
modalities except on valence recognition using GSR that is in corroboration
with the finding in [7]. It is worthy to mention that low
unimodal performances on GSR could be due to the fact that GSR
responses are slow. Therefore, GSR is an unsuitable modality for
an experiment with short recordings like ours.",-,-,-,-,52,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
29,170,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,-,Canada,-,x,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,33,12,29.7,-,-,Canada,Yes,-,-,-,,,-,x,x,x,-,x,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,-,-,-,-,-,x,,,,,,,,,,,-,-,,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,,-,-,-,-,-,x,-,,x,"Furthermore, it is worth noting that quality adaptive arousal and
valence classifiers performed significantly above chance on all the
modalities except on valence recognition using GSR that is in corroboration
with the finding in [7]. It is worthy to mention that low
unimodal performances on GSR could be due to the fact that GSR
responses are slow. Therefore, GSR is an unsuitable modality for
an experiment with short recordings like ours.",-,-,-,-,52,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
29,171,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,-,Canada,-,x,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,33,12,29.7,-,-,Canada,Yes,-,-,-,,,-,x,x,x,-,x,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LL, HL",-,x,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,,-,-,-,-,-,x,-,,x,"Furthermore, it is worth noting that quality adaptive arousal and
valence classifiers performed significantly above chance on all the
modalities except on valence recognition using GSR that is in corroboration
with the finding in [7]. It is worthy to mention that low
unimodal performances on GSR could be due to the fact that GSR
responses are slow. Therefore, GSR is an unsuitable modality for
an experiment with short recordings like ours.",-,-,-,-,52,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
29,172,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,-,Canada,-,x,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,33,12,29.7,-,-,Canada,Yes,-,-,-,,,-,x,x,x,-,x,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LL, HL",-,-,-,-,-,-,x,,,,,,,,,,,-,-,,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,,-,-,-,-,-,x,-,-,x,"Furthermore, it is worth noting that quality adaptive arousal and
valence classifiers performed significantly above chance on all the
modalities except on valence recognition using GSR that is in corroboration
with the finding in [7]. It is worthy to mention that low
unimodal performances on GSR could be due to the fact that GSR
responses are slow. Therefore, GSR is an unsuitable modality for
an experiment with short recordings like ours.",-,-,-,-,53,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
30,173,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H.",2016,IECBES 2016 - IEEE-EMBS Conference on Biomedical Engineering and Sciences,-,x,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,20,4,22.83,-,-,Malasya,-,-,-,-,,,-,-,-,-,,-,-,,,-,-,-,,-,-,-,,-,,,,--,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,-,"-G25
Logitech steering wheel kit",-,-,,-,-,-,-,x,-,-,-,-,,,,,,,"A driving simulator consisting of a driving unit (G25
Logitech steering wheel kit), a large display, and simulation
software (Speed Dreams) was utilized for emotion stimulation.
Driving scenarios were configured to stimulate three
investigated emotions: neutral, stress, and anger. The neutral
scenario consists of casual driving at a speed of 80 km/h on a
circular roadway with a wide road and good vision [14]. The
stress scenario consists of a snowy mountain track with a
narrow road and poor vision, presumed to trigger sudden
turning, leading to vehicle drifting and impact [8][14][15][16].
Finally, the anger scenario requires the driver to complete a
designated driving task within a time limit. This task was
performed along with four artificial intelligence contestants,
with the aim of triggering aggressive road events, such as
tailgating, sudden overtaking, and reckless driving [17][18].
To further trigger driving anger, the subjects were also
informed that deductions would be made to their reward
money if they failed to accomplish the driving task within the
time frame; the full amount was still given at the end of
experiment",50m,-,-,-,-,BioRadio 150,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"neutral, stress",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"Poor classification accuracy may arise because anger, fear,
or stress emotions fall at the same section according to the
valence–arousal and pleasant–unpleasant models [20][25].
Consequently, humans possess similar physiological
characteristics during these two emotions, leading to poor
classification accuracy. A number of research have also
demonstrated that the emotions of drivers differ considerably
in vulnerability to disturbance. Therefore, EDA measurements
for this experiment are still insufficient in detecting slight
physiological changes between anger and stress.",-,-,85,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
30,174,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H.",2016,IECBES 2016 - IEEE-EMBS Conference on Biomedical Engineering and Sciences,-,x,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,20,4,22.83,-,-,Malasya,-,-,-,-,,,-,-,-,-,,-,-,,,-,-,-,,-,-,-,,-,,,,--,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,-,"-G25
Logitech steering wheel kit",-,-,,-,-,-,-,x,-,-,-,-,,,,,,,"A driving simulator consisting of a driving unit (G25
Logitech steering wheel kit), a large display, and simulation
software (Speed Dreams) was utilized for emotion stimulation.
Driving scenarios were configured to stimulate three
investigated emotions: neutral, stress, and anger. The neutral
scenario consists of casual driving at a speed of 80 km/h on a
circular roadway with a wide road and good vision [14]. The
stress scenario consists of a snowy mountain track with a
narrow road and poor vision, presumed to trigger sudden
turning, leading to vehicle drifting and impact [8][14][15][16].
Finally, the anger scenario requires the driver to complete a
designated driving task within a time limit. This task was
performed along with four artificial intelligence contestants,
with the aim of triggering aggressive road events, such as
tailgating, sudden overtaking, and reckless driving [17][18].
To further trigger driving anger, the subjects were also
informed that deductions would be made to their reward
money if they failed to accomplish the driving task within the
time frame; the full amount was still given at the end of
experiment",50m,-,-,-,-,BioRadio 150,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"neutral, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"Poor classification accuracy may arise because anger, fear,
or stress emotions fall at the same section according to the
valence–arousal and pleasant–unpleasant models [20][25].
Consequently, humans possess similar physiological
characteristics during these two emotions, leading to poor
classification accuracy. A number of research have also
demonstrated that the emotions of drivers differ considerably
in vulnerability to disturbance. Therefore, EDA measurements
for this experiment are still insufficient in detecting slight
physiological changes between anger and stress.",-,-,85,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
30,175,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H.",2016,IECBES 2016 - IEEE-EMBS Conference on Biomedical Engineering and Sciences,-,x,,-,Malaysia,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,20,4,22.83,-,-,Malasya,-,-,-,-,,,-,-,-,-,,-,-,,,-,-,-,,-,-,-,,-,,,,--,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,-,"-G25
Logitech steering wheel kit",-,-,,-,-,-,-,x,-,-,-,-,,,,,,,"A driving simulator consisting of a driving unit (G25
Logitech steering wheel kit), a large display, and simulation
software (Speed Dreams) was utilized for emotion stimulation.
Driving scenarios were configured to stimulate three
investigated emotions: neutral, stress, and anger. The neutral
scenario consists of casual driving at a speed of 80 km/h on a
circular roadway with a wide road and good vision [14]. The
stress scenario consists of a snowy mountain track with a
narrow road and poor vision, presumed to trigger sudden
turning, leading to vehicle drifting and impact [8][14][15][16].
Finally, the anger scenario requires the driver to complete a
designated driving task within a time limit. This task was
performed along with four artificial intelligence contestants,
with the aim of triggering aggressive road events, such as
tailgating, sudden overtaking, and reckless driving [17][18].
To further trigger driving anger, the subjects were also
informed that deductions would be made to their reward
money if they failed to accomplish the driving task within the
time frame; the full amount was still given at the end of
experiment",50m,-,-,-,-,BioRadio 150,-,-,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,2,"stress, anger",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"Poor classification accuracy may arise because anger, fear,
or stress emotions fall at the same section according to the
valence–arousal and pleasant–unpleasant models [20][25].
Consequently, humans possess similar physiological
characteristics during these two emotions, leading to poor
classification accuracy. A number of research have also
demonstrated that the emotions of drivers differ considerably
in vulnerability to disturbance. Therefore, EDA measurements
for this experiment are still insufficient in detecting slight
physiological changes between anger and stress.",-,-,70,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
31,176,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform.,2016,Iranian Journal of Medical Physics,x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,Iran,-,-,-,-,,,,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,x,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"happy, sad, scary, peaceful, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,95.1,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
31,177,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform.,2016,Iranian Journal of Medical Physics,x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,Iran,-,-,-,-,,,,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,x,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LA, HA, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,95.8,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
31,178,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform.,2016,Iranian Journal of Medical Physics,x,-,,-,Iran,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,Iran,-,-,-,-,,,,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,x,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LV, HV, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,97.9,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
32,179,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,x,-,,-,USA,-,-,-,-,x,-,x,-,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,x,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,128,-,-,-,-,64.84,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
32,180,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,x,-,,-,USA,-,-,-,-,x,-,x,-,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,-,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,x,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,128,-,-,-,-,63.28,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
32,181,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,x,-,,-,USA,-,-,-,-,x,-,x,-,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LL, HL",-,-,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,x,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,128,-,-,-,-,70,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
32,182,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,x,-,,-,USA,-,-,-,-,x,-,x,-,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LD, HD",-,-,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,x,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,128,-,-,-,-,59.38,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
32,183,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,x,-,,-,USA,-,-,-,-,x,-,x,-,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",-,-,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,x,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,128,-,-,-,-,33.59,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
32,184,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,x,-,,-,USA,-,-,-,-,x,-,x,-,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,8,"pleased, excited, annoying, nervous, sad, sleepy, calm, relaxed",-,-,-,-,-,-,-,,,,,,,,,,,-,-,,,-,-,-,-,-,,x,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,128,-,-,-,-,24.22,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
33,185,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,-,Macedonian,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,-,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"happiness, sadness, scary, peacefulness, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,79.53,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
33,186,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,-,Macedonian,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,-,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LA, HA, rest",-,-,-,-,-,-,,,,,,,,,,,,-,-,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,85.23,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
33,187,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,-,Macedonian,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,-,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LV, HV, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,85.53,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
33,188,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,-,Macedonian,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,-,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"happiness, sadness, scary, peacefulness, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,94.75,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
33,189,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,-,Macedonian,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,-,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LA, HA, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,94.63,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
33,190,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,-,Macedonian,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,11,11,22.73,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,x,-,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,-,15m,-,-,-,-,PowerLab,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LV, HV, rest",-,-,-,-,-,-,-,,,,,,,,,,,-,,-,,-,-,-,-,x,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,92.58,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
34,191,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516647","Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I.",Experimental analysis of emotion classification techniques.,2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2018",-,x,,-,Romania,x,-,-,-,-,-,,-,-,,,-,-,,,,,,,,,,,,,,,-,10,6,-,-,-,-,-,-,-,-,,,-,,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Shimmer3 GSR+ Unit sensor,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV, neutral",-,-,-,-,x,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,40,-,41,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
34,192,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516648","Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I.",Experimental analysis of emotion classification techniques.,2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2019",-,x,,-,Romania,x,-,-,-,-,-,,-,-,,,-,-,,,,,,,,,,,,,,,-,10,6,-,-,-,-,-,-,-,-,,,-,,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Shimmer3 GSR+ Unit sensor,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LV, HV, neutral",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,99,-,90,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
34,193,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516649","Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I.",Experimental analysis of emotion classification techniques.,2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2020",-,x,,-,Romania,x,-,-,-,-,-,,-,-,,,-,-,,,,,,,,,,,,,,,-,10,6,-,-,-,-,-,-,-,-,,,-,,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Shimmer3 GSR+ Unit sensor,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV, neutral",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,,-,-,-,-,x,-,-,-,-,-,-,-,44,-,63,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
34,194,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516649","Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I.",Experimental analysis of emotion classification techniques.,2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2020",-,x,,-,Romania,x,-,-,-,-,-,,-,-,,,-,-,,,,,,,,,,,,,,,-,10,6,-,-,-,-,-,-,-,-,,,-,,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,Shimmer3 GSR+ Unit sensor,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV, neutral",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,66,-,59,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
35,195,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.","Ferdinando, H., & Alasaarela, E.",Emotion recognition using cvxEDA-based features.,2018,"Journal of Telecommunication, Electronic and Computer Engineering",x,-,,-,Finland,-,-,-,-,x,-,-,x,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"calm, medium aroused, excited",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"Compare to some references [1], [3], [4], the proposed features used in this study offered better results for the 3-class problem, see highlighted results in Table 1 and 2. These are the second major findings in this study and will serve as baselines for future studies using the MAHNOB-HCI, especially for EDA-based features only. However, recognizing medium valence looked challenging, while low valence showed the easiest ones.
This study was limited by the absence of nonlinear features as Yang and Liu found that relationship between EDA signal and emotion is nonlinear [8]. Deeper studies using nonlinear features, e.g. Lanata et al. [6] proposed recurrent plot, deterministic chaos, and detrended fluctuation analysis, were left for future works.",-,-,77.8,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
35,196,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.","Ferdinando, H., & Alasaarela, E.",Emotion recognition using cvxEDA-based features.,2018,"Journal of Telecommunication, Electronic and Computer Engineering",x,-,,-,Finland,-,-,-,-,x,-,-,x,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LV, neutral valence, HV",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,x,-,x,"Compare to some references [1], [3], [4], the proposed features used in this study offered better results for the 3-class problem, see highlighted results in Table 1 and 2. These are the second major findings in this study and will serve as baselines for future studies using the MAHNOB-HCI, especially for EDA-based features only. However, recognizing medium valence looked challenging, while low valence showed the easiest ones.
This study was limited by the absence of nonlinear features as Yang and Liu found that relationship between EDA signal and emotion is nonlinear [8]. Deeper studies using nonlinear features, e.g. Lanata et al. [6] proposed recurrent plot, deterministic chaos, and detrended fluctuation analysis, were left for future works.",-,-,74.6,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
35,197,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.","Ferdinando, H., & Alasaarela, E.",Emotion recognition using cvxEDA-based features.,2018,"Journal of Telecommunication, Electronic and Computer Engineering",x,-,,-,Finland,-,-,-,-,x,-,-,x,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"LV, neutral valence, HV",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,x,"Compare to some references [1], [3], [4], the proposed features used in this study offered better results for the 3-class problem, see highlighted results in Table 1 and 2. These are the second major findings in this study and will serve as baselines for future studies using the MAHNOB-HCI, especially for EDA-based features only. However, recognizing medium valence looked challenging, while low valence showed the easiest ones.
This study was limited by the absence of nonlinear features as Yang and Liu found that relationship between EDA signal and emotion is nonlinear [8]. Deeper studies using nonlinear features, e.g. Lanata et al. [6] proposed recurrent plot, deterministic chaos, and detrended fluctuation analysis, were left for future works.",-,-,75.5,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
35,198,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.","Ferdinando, H., & Alasaarela, E.",Emotion recognition using cvxEDA-based features.,2018,"Journal of Telecommunication, Electronic and Computer Engineering",x,-,,-,Finland,-,-,-,-,x,-,-,x,-,,,-,-,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"calm, medium aroused, excited",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,-,x,-,x,"Compare to some references [1], [3], [4], the proposed features used in this study offered better results for the 3-class problem, see highlighted results in Table 1 and 2. These are the second major findings in this study and will serve as baselines for future studies using the MAHNOB-HCI, especially for EDA-based features only. However, recognizing medium valence looked challenging, while low valence showed the easiest ones.
This study was limited by the absence of nonlinear features as Yang and Liu found that relationship between EDA signal and emotion is nonlinear [8]. Deeper studies using nonlinear features, e.g. Lanata et al. [6] proposed recurrent plot, deterministic chaos, and detrended fluctuation analysis, were left for future works.",-,-,77.3,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
36,199,"Zhang, S., Liu, G., & Lai, X. (2015). Classification of evoked emotions using an artificial neural network based on single, short-term physiological signals. Journal of Advanced Computational Intelligence and Intelligent Informatics, 19(1), 118-126.","Zhang, S., Liu, G., & Lai, X. ","Classification of evoked emotions using an artificial neural network based on single, short-term physiological signals.",2015,Journal of Advanced Computational Intelligence and Intelligent Informatics,x,-,,-,China,x,-,-,-,-,-,-,-,-,,,-,-,,,,,,,,,,,,,,,-,255,117,22.1,-,19-25,China,Yes,-,-,-,,,-,x,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,x,x,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"anger, fear, grief, happiness, calmness",-,-,-,-,-,,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,x,,,,-,,,-,-,-,-,-,-,,,,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,82.29,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,,x,-,,
37,200,"Gjoreski, M., Lustrek, M., & Gams, M. (2018). Multi-task ensemble learning for affect recognition. 553–558. Scopus. https://doi.org/10.1145/3267305.3267308
","Gjoreski, M., Lustrek, M., & Gams, M.",Multi-task ensemble learning for affect recognition.,2018,UbiComp/ISWC 2018 - Adjunct Proceedings of the 2018 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2018 ACM International Symposium on Wearable Computers,-,x,,-,Slovenia,-,-,-,-,x,x,x,x,-,,,-,-,x,x,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,x,-,,-,-,-,-,-,-,-,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,,,,,,,,66.3,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,201,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,x,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,64,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,202,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,x,,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,66,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,203,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,x,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,x,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,64,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,204,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,x,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,63.3,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,205,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,x,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,59,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,206,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,x,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,60,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,207,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,x,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,64,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,208,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,50.3,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,209,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,52.5,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,210,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,x,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,50,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,211,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,48.5,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,212,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,49.6,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,213,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,53.7,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,214,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,54.2,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
38,215,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,78,,,,,,,,,,,,,,,,,,,,,,,,,
38,216,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,79.5,,,,,,,,,,,,,,,,,,,,,,,,,
38,217,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,x,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,80,,,,,,,,,,,,,,,,,,,,,,,,,
38,218,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,75,,,,,,,,,,,,,,,,,,,,,,,,,
38,219,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,79.5,,,,,,,,,,,,,,,,,,,,,,,,,
38,220,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,54,,,,,,,,,,,,,,,,,,,,,,,,,
38,221,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,76.5,,,,,,,,,,,,,,,,,,,,,,,,,
38,222,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,x,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,76.3,,,,,,,,,,,,,,,,,,,,,,,,,
38,223,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,x,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,71.2,,,,,,,,,,,,,,,,,,,,,,,,,
38,224,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,x,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,x,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,74.3,,,,,,,,,,,,,,,,,,,,,,,,,
38,225,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,x,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,74,,,,,,,,,,,,,,,,,,,,,,,,,
38,226,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,x,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,77.2,,,,,,,,,,,,,,,,,,,,,,,,,
38,227,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,x,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,80.3,,,,,,,,,,,,,,,,,,,,,,,,,
38,228,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,-,-,-,x,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,73.4,,,,,,,,,,,,,,,,,,,,,,,,,
38,229,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,x,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,61.1,,,,,,,,,,,,,,,,,,,,,,,,,
38,230,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,x,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,58.8,,,,,,,,,,,,,,,,,,,,,,,,,
38,231,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,x,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,x,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,60.9,,,,,,,,,,,,,,,,,,,,,,,,,
38,232,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,x,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,58.9,,,,,,,,,,,,,,,,,,,,,,,,,
38,233,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,x,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,60.5,,,,,,,,,,,,,,,,,,,,,,,,,
38,234,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,x,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,62.4,,,,,,,,,,,,,,,,,,,,,,,,,
38,235,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,-,x,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,58.3,,,,,,,,,,,,,,,,,,,,,,,,,
38,236,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,47.3,,,,,,,,,,,,,,,,,,,,,,,,,
38,237,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,47,,,,,,,,,,,,,,,,,,,,,,,,,
38,238,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,x,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,52.7,,,,,,,,,,,,,,,,,,,,,,,,,
38,239,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,x,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,51.4,,,,,,,,,,,,,,,,,,,,,,,,,
38,240,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,x,-,-,-,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,53.3,,,,,,,,,,,,,,,,,,,,,,,,,
38,241,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,45.4,,,,,,,,,,,,,,,,,,,,,,,,,
38,242,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,-,Slovenia,-,-,-,-,x,-,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,--,-,-,-,-,-,-,x,-,-,-,-,-,-,48.3,,,,,,,,,,,,,,,,,,,,,,,,,
39,243,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,x,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,58.12,-,-,-,-,,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
39,244,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,59.21,-,-,-,-,,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
39,245,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,-,-,-,-,x,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,71.53,-,-,-,-,,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
39,246,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LA, HA",-,x,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,71.4,-,-,-,-,,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
39,247,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,-,x,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,60.54,-,-,-,-,,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
39,248,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,-,-,-,-,x,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,59.2,-,-,-,-,,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
39,249,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,-,-,-,-,x,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,71.04,-,-,-,-,,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
39,250,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,-,Turkey,-,-,-,-,x,x,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV, HV",-,x,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,70.54,-,-,-,-,,-,-,-,-,-,-,,,,,,,,-,-,-,x,,,
40,251,"Martínez-Rodrigo, A., Zangróniz, R., Pastor, J. M., & Sokolova, M. V. (2017). Arousal level classification of the aging adult from electro-dermal activity: From hardware development to software architecture. Pervasive and Mobile Computing, 34, 46–59. Scopus. https://doi.org/10.1016/j.pmcj.2016.04.006","Martínez-Rodrigo, A., Zangróniz, R., Pastor, J. M., & Sokolova, M. V.",Arousal level classification of the aging adult from electro-dermal activity: From hardware development to software architecture.,2017,Pervasive and Mobile Computing,x,-,,-,Spain,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,-,21,12,-,-,-,-,Relies on other's questionnaire,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,x,-,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,9m,-,-,-,homemade,-,right,-,-,-,-,-,-,-,-,-,-,x,-,,-,dimensional,2,"sleepiness, stressed",x,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,-,-,-,-,-,,,-,,,,,-,-,-,-,-,-,-,-,,,,-,-,-,-,83.33,,,,,,,,,,,,,,,,,,,,,,,,,
41,252,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636","Milchevski, A., Rozza, A., & Taskovski, D.",Multimodal affective analysis combining regularized linear regression and boosted regression trees.,2015,"AVEC 2015 - Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015",-,x,,-,Macedonian,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,1,arousal,x,-,-,,,-,,,,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,0.09,-,,,,,,,-,-,-,x,,,
41,253,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636","Milchevski, A., Rozza, A., & Taskovski, D.",Multimodal affective analysis combining regularized linear regression and boosted regression trees.,2015,"AVEC 2015 - Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015",-,x,,-,Macedonian,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,1,arousal,-,-,-,,,-,,,,x,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,0.056,-,,,,,,,-,-,-,x,,,
41,254,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636","Milchevski, A., Rozza, A., & Taskovski, D.",Multimodal affective analysis combining regularized linear regression and boosted regression trees.,2015,"AVEC 2015 - Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015",-,x,,-,Macedonian,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,1,valence,x,-,-,,,-,,,,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,0.113,-,,,,,,,-,-,-,x,,,
41,255,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636","Milchevski, A., Rozza, A., & Taskovski, D.",Multimodal affective analysis combining regularized linear regression and boosted regression trees.,2015,"AVEC 2015 - Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015",-,x,,-,Macedonian,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,1,valence,-,-,-,,,-,,,,x,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,0.107,-,,,,,,,-,-,-,x,,,
42,256,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007","Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B.",Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data.,2015,Pattern Recognition Letters,x,-,,-,GErmany,-,-,-,-,x,-,-,-,-,-,-,-,x,-,-,-,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,1,arousal,-,-,-,,,-,,,,-,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,0.057,-,,,,,,,-,-,-,x,,,
42,257,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007","Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B.",Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data.,2015,Pattern Recognition Letters,x,-,,-,GErmany,-,-,-,-,x,-,-,-,-,-,-,-,x,-,-,-,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,1,valence,-,-,-,,,-,,,,-,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,0.109,-,,,,,,,-,-,-,x,,,
42,258,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007","Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B.",Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data.,2015,Pattern Recognition Letters,x,-,,-,GErmany,-,-,-,-,x,-,-,-,-,-,-,-,x,-,-,-,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,2,"arousal, valence",-,-,-,,,-,,,,-,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,0.053,-,,,,,,,-,-,-,x,,,
42,259,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007","Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B.",Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data.,2015,Pattern Recognition Letters,x,-,,-,GErmany,-,-,-,-,x,-,-,-,-,-,-,-,x,-,-,-,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,-,-,-,-,-,-,-,-,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,2,"arousal, valence",-,-,-,,,-,,,,-,-,-,-,-,-,-,-,-,?,?,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,0.122,-,,,,,,,-,-,-,x,,,
43,260,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011","Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T.","Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting.",2017,Frontiers in ICT,x,-,,-,Switzerland,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"LV,HV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,,,79.94,-,-,-,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,x,,,
43,261,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011","Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T.","Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting.",2017,Frontiers in ICT,x,-,,-,Switzerland,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,x,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,2,"LA, HA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,,,80.7,-,-,-,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,x,,,
44,262,"Barral, O., Kosunen, I., & Jacucci, G. (2017). No need to laugh out loud: Predicting humor appraisal of comic strips based on physiological signals in a realistic environment. ACM Transactions on Computer-Human Interaction, 24(6). Scopus. https://doi.org/10.1145/3157730","Barral, O., Kosunen, I., & Jacucci, G.",No need to laugh out loud: Predicting humor appraisal of comic strips based on physiological signals in a realistic environment.,2017,ACM Transactions on Computer-Human Interaction,x,-,,-,Finland,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,,,,,,,,,-,25,12,26.3,25,20-35,Finland,Yes,-,,-,,,-,-,-,-,-,-,-,-,,-,x,-,,-,-,-,,-,,,,x,x,,,,,,,,,,,,,-,-,-,-,x,-,-,-,x,-,-,-,-,,-,-,-,-,-,-,-,-,-,x,,,,,-,,,,,,-,ProComp Infinity,not dominant,x,x,x,x,-,-,-,-,-,-,-,-,,-,categorical,2,"funny, not funny",-,-,-,-,-,x,-,,,,,,,,,-,,-,-,-,,-,-,-,-,-,-,-,,,-,,,-,-,-,-,-,-,,,-,,,,-,-,-,-,-,-,-,-,-,,,,x," Electrodermal Activity. Interestingly, EDA seems to be the most generalizable signal as
it performed the best in the across prediction setup compared to the other physiological sources,
and also performed better than EDA in the within prediction setup. It is also interesting that the
low classification performances of some participants in the within setup (e.g., “P10,” “P17,” “P22”)
were largely improved in the across setup. One possible explanation is that EDA features are quite
generalizable across participants; for some participants, the effect might be small, and therefore the model’s performance improves when the amount of training data increases, even when the
data belongs to other participants.

Figure 5 shows the grand average across participants and trials of the EDA for the time-locked,
fixed-time windows (initial, end, and special windows). EDA is known to present a relatively high
latency as compared to other physiological signals. As a matter of fact, capturing the EDA responses to the humor appraisal was one of the main motivations to define the special window that
continues for 5 seconds after the end of a trial. The grand average of the EDA signal shows a very
large difference in the signal from 2 to 4 seconds after the end of a trial. More specifically, “Funny”
trials elicited higher EDA than “Not funny” trials. This is captured in the most relevant features
for the trained models (see Table 4), as the top EDA features are the ones capturing the amount of
activity (i.e., wS.sum.Eda) and amount of change (i.e., wS.diff.Eda, wS.diffsq.Eda) within the special
window


EDA
showed increased values for humorous content, as expected because of increased activity of the
sympathetic nervous system due to the feeling of amusement (Foster et al. 1998; Martin 2010)
(Figure 5). In addition, the results indicate that the most discriminative differences in the physiological signals related to humor appraisal occur in the later stages of the information consumption
process, as shown in the most highly weighted features of the predictive models (see Table 4). This
finding is compatible with the incongruity model of humor, which posits that humor results from
solving ambiguities and conceptual incongruities using alternative formulations to the discrepancy. To put it in colloquial terms, the punchline comes at the end (Polimeni and Reiss 2006).
As a matter of fact, the discriminative physiological differences were mostly found in the special
window, which continued for several seconds after the trial ended, overlapping with the next
stimulus. Initially, we expected this to be the case only for the EDA signal, which is known to have
a temporal delay; but the same post-decision changes were apparent in EEG and ECG features as
well. This holds important implications for the design of affective systems for humor appraisal, as
it proves the delayed nature of physiological responses related to humor appraisal, as well as their
capability to capture it, despite overlapping with the next stimulus",-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,x,,,
45,263,"Zhang, Q., Lai, X., & Liu, G. (2016). Emotion recognition of GSR based on an improved quantum neural network. 1, 488-492. Scopus. https://doi.org/10.1109/IHMSC.2016.66
","Zhang, Q., Lai, X., & Liu, G.", Emotion recognition of GSR based on an improved quantum neural network.,2016,"Proceedings - 2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2016",-,x,.,.,China,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,35,-,-,-,18-22,-,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,30s,,,,.,MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,categorical,5,"happy, grief, fear, angry, calm",,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,84.9,,,,,,,,,,,,,,,,,,,,,,,,,
,,"Lanatà, A., Valenza, G., & Scilingo, E. P. (2012). A novel EDA glove based on textile-integrated electrodes for affective computing. Medical & Biological Engineering & Computing, 50(11), 1163–1172. doi:10.1007/s11517-012-0921-9","Lanatá,A, Valenza,G y Scilingo.E.P", A novel EDA glove based on textile-integrated electrodes for affective computing,2012,International Federation for Medical and Biological Engineering,X,-,,-,Italy,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,--,35,-,-,-,21-24,-,Relies on other´s questionaire,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,-,X,IAPS,X,,,-,-,-,-,-,-,-,-,-,-,,,,,-,-,10 s,-,-,-,Yes,MP35 Biopac,,X,-,x,x,-,,-,-,x,X,-,,,-,dimensional,5,"neutral, arousal1, arousal2, arousal3, arousal4",-,-,-,X,-,,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,,-,-,,,-,,-,-,-,-,-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"Talk of  five arousal clases(p.9) and I don ´t know if is important. Also mentions quadratic discrminant clasiffer (p.7). Finally in the page number 8 mentions : ""We stopped the reduction process when the cumulative variance reached 95 %"". is this relevant ?  I think that no. In particular, we tested
several classifiers such as linear discriminant classifier
(LDC), mixture of gaussian (MOG), k-nearest neighbor
(k-NN), Kohonen sel- organizing map (KSOM), multilayer
perceptron (MLP), and quadratic discriminant classifier
(QDC). Among these, QDC has shown the highest recognition accuracy and consistency in both arousal and
valence multiclass. According to that and for the sake of
brevity we will report and discuss here only results from
the QDC. ",-,x
,,"Derefinko, K. J., Peters, J. R., Eisenlohr-Moul, T. A., Walsh, E. C., Adams, Z. W., & Lynam, D. R. (2014). Relations Between Trait Impulsivity, Behavioral Impulsivity, Physiological Arousal, and Risky Sexual Behavior Among Young Men. Archives of Sexual Behavior, 43(6), 1149–1158. doi:10.1007/s10508-014-0327-x","Derefinko, K. J., Peters, J. R., Eisenlohr-Moul, T. A., Walsh, E. C., Adams, Z. W., & Lynam, D. R."," Relations Between Trait Impulsivity, Behavioral Impulsivity, Physiological Arousal, and Risky Sexual Behavior Among Young Men",2014,Archives of Sexual Behavior,X,-,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,--,135,-,19.4,-,17-26,-,b,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,-,X,-,X,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,,BIopac GSR100C,,-,-,X,X,-,,-,-,-,X,,,,-,categorical,-,-,-,-,-,,-,,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,X,-,,,,,,,,,-,-,-,-,-,,-,-,X,-,109,X,-,-,-,-,-,-,,,,,-,-,,-,,,,,,,,,-,-,X,-,No predice emociones; visto con Tomy,,-
,,"Stephens, C. L., Christie, I. C., & Friedman, B. H. (2010). Autonomic specificity of basic emotions: Evidence from pattern classification and cluster analysis. Biological Psychology, 84(3), 463–473. doi:10.1016/j.biopsycho.2010.03.014","Stephens, C. L., Christie, I. C., & Friedman, B. H ",Autonomic specificity of basic emotions: Evidence from pattern classification and cluster analysis,2010,Biological Psychology,X,-,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,--,49,27,-,-,18-26,-,a,-,,,,,X,X,-,-,-,-,-,,,X,X,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,X,-,-,-,X,X,-,-,-,X,-,-,,,X,X,-,?,,-,-,-,,,,,,,-,-,-,69-149 y 61-247s,113s y 145s,-,,Biopac MP100,,X,-,-,-,-,,-,-,-,-,,,,X,categorical,-,-,,,,,,,,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,,,,,,,,,,,,,,-,-,-,-,,-,-,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,X,,Multimodal,,-
,,"Balconi, M., Vanutelli, M., & Finocchiaro, R. (2014). Multilevel analysis of facial expressions of emotion and script: self-report (arousal and valence) and psychophysiological correlates. Behavioral and Brain Functions, 10(1), 32. doi:10.1186/1744-9081-10-32 ","Balconi, M., Vanutelli, M., & Finocchiaro, R",Multilevel analysis of facial expressions of emotion and script: self-report (arousal and valence) and psychophysiological correlates.,2014,Behavioral and Brain,X,-,,-,Italy,X,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,26,15,-,-,6-11,-,a,X,,,,,-,X,X,X,-,-,-,,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,-,-,-,X,,,X,-,-,-,,-,-,-,,,,,,,-,-,15s,-,-,-,-,BioFeedback 2000,non-dominat,-,-,-,-,-,,-,-,X,-,-,,,-,categorical,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,Multimodal,https://sci-hub.se/https://doi.org/10.1186/1744-9081-10-32,-
,,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,42,19,27.5,-,20-50,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stroop color-word interference test’’ (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,3s,-,-,-,-,GSR2,left,X,-,-,-,-,,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,X,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,,-,-,-,-,43,,-,-,-,60,-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,,https://sci-hub.se/https://doi.org/10.1007/s10439-013-0880-9,x
,,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,42,19,27.5,-,20-50,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stroop color-word interference test’’ (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,3s,-,-,-,-,GSR2,left,X,-,-,-,-,,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,X,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,,-,-,-,-,43,,-,-,-,"63,57",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/https://doi.org/10.1007/s10439-013-0880-9,x
,,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,42,19,27.5,-,20-50,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stroop color-word interference test’’ (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,3s,-,-,-,-,GSR2,left,X,-,-,-,-,,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,,X,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,,-,-,-,-,43,,-,-,-,"60,71",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/https://doi.org/10.1007/s10439-013-0880-9,x
,,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,42,19,27.5,-,20-50,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stroop color-word interference test’’ (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,3s,-,-,-,-,GSR2,left,X,-,-,-,-,,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,,-,,,,,,,,,,,X,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,,-,-,-,-,43,,-,-,-,"63,57",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/https://doi.org/10.1007/s10439-013-0880-9,x
,,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,42,19,27.5,-,20-50,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stroop color-word interference test’’ (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,3s,-,-,-,-,GSR2,left,X,-,-,-,-,,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,X,-,-,-,-,,,,,,,,,-,-,-,-,-,,-,-,-,-,43,,-,-,-,"63,57",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/https://doi.org/10.1007/s10439-013-0880-9,x
,,"GOUIZI, K., BEREKSI REGUIG, F., & MAAOUI, C. (2011). Emotion recognition from physiological signals. Journal of Medical Engineering & Technology, 35(6-7), 300–307. doi:10.3109/03091902.2011.601784","GOUIZI, K., BEREKSI REGUIG, F., & MAAOUI, C. ",Emotion recognition from physiological signals,2011,Journal of Medical Engineering & Technology,X,-,,-,France,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,4,2,-,-,25-28,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,-,-,IAPS,X,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-, ProComp Infiniti,-,-,-,-,-,-,,-,-,-,-,-,,,-,categorical,6,"joy, sadness, fear, disgust, neutrality,  amusement",-,X,-,-,-,,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,,-,,,,,,,,,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,,"Is strange, because in the first page say ""review"" but in the paper take data and use SVM althougt haven`t values about of performance and doesn`t say duration elicitation stimuli",https://sci-hub.se/https://doi.org/10.3109/03091902.2011.601784,x
,,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2015). Affective visual stimuli: Characterization of the picture sequences impacts by means of nonlinear approaches. Basic and clinical neuroscience, 6(4), 209.","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",Affective visual stimuli: Characterization of the picture sequences impacts by means of nonlinear approaches.,2015,Basic and clinical neuroscience,X,-,,-,Iran,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,47,31,"21.90(f), 21.1(M)",,"19-25 (f),19-23(m) ",Irani,No,-,,,,,-,X,X,X,X,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,-,-,IAPS,X,,,-,-,-,-,,-,-,-,,,,,,,-,-,15s,-,-,-,-,-,left,-,-,-,-,-,,-,-,-,-,-,,,-,categorical,4,"relaxed, happy, sad, afraid",-,-,-,-,--,,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,X,-,-,-,-,-,X,-,-,-,-,-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,,multimodal,http://ris.nirp.ir/files/site1/rds_journals/198/article-198-95646.pdf,x
,,"Chaspari, T., Baucom, B., Timmons, A. C., Tsiartas, A., Del Piero, L. B., Baucom, K. J. W., … Narayanan, S. S. (2015). Quantifying EDA synchrony through joint sparse representation: A case-study of couples’ interactions. 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). doi:10.1109/icassp.2015.7178083 ","Theodora Chaspari, Brian Baucom, Adela C. Timmons, Andreas Tsiartas , Larissa Borofsky Del Piero , Katherine J.W. Baucom, Panayiotis Georgiou , Gayla Margolin , Shrikanth S. Narayanan",Quantifying EDA synchrony through joint sparse representation: A case-study of couples’ interactions,2015,"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",-,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,categorical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,"Usually I take the author name from APA citation, but in these case the authors in the citation wasn`t complete therefore take the name of title . Also the sample is ""20 couples"" participants is 40 ?; NO PREDICE",https://sci-hub.se/10.1109/icassp.2015.7178083,-
,,"Bornoiu, I.-V., Strungaru, R., & Grigore, O. (2015). Intelligent System for Emotion Recognition Based on Electrodermal Activity Processing. 6th European Conference of the International Federation for Medical and Biological Engineering, 70–73. doi:10.1007/978-3-319-11128-5_18 ","Bornoiu, I.-V., Strungaru, R., & Grigore, O",Intelligent System for Emotion Recognition Based on Electrodermal Activity Processing,2015,Medical and Biological Engineering,-,X,,-,Romania,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,25-65,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,X,-,-,X,-,Trier social stress test,X,,,-,X,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,Varioport,-,X,-,-,-,-,,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,X,-,-,,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,?,,-,-,-,"75,39",-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,X,-,The algorithm was trained with two thirds of the recordings chosen at random. The stressed/relaxed information was extracted from the signal generated by the observer. But don`t say the size sample !. Review if 75.39% is the precision...,https://sci-hub.se/10.1007/978-3-319-11128-5_18,x
,,"Cheng, J., & Liu, G.-Y. (2015). Affective detection based on an imbalanced fuzzy support vector machine. Biomedical Signal Processing and Control, 18, 118–126. doi:10.1016/j.bspc.2014.12.006","Cheng, J., & Liu, G.-Y.",Affective detection based on an imbalanced fuzzy support vector machine.,2015,Biomedical Signal Processing and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,,,-
,,"Hedger, N., Adams, W. J., & Garner, M. (2015). Autonomic arousal and attentional orienting to visual threat are predicted by awareness. Journal of Experimental Psychology: Human Perception and Performance, 41(3), 798–806. doi:10.1037/xhp0000051 ","Hedger, N., Adams, W. J., & Garner, M.",Autonomic arousal and attentional orienting to visual threat are predicted by awareness,2015,Human Perception and Performance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,categorical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,140,,,,,,,,,,,,,,,,,,,,,,,,,,X,,no predice ,,-
,,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,X,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,31,18,30.8,-,21-38,-,Yes,-,,,,x,,X,X,X,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,Mindfield eSense,not dominant,-,-,-,X,X,,-,-,-,-,-,,,-,categorical,2,"stress, not stress",,X,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,140,-,-,-,-,"75,80",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"The smoothing algorithm excluded nine signals due to signal degeneration (see [5] for details), thus our final dataset consists 142 records, which are used in subsequent analysis.",https://sci-hub.se/10.1145/2808435.2808450,x
,,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,X,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,31,18,30.8,-,21-38,-,Yes,-,,,,x,-,X,X,X,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,Mindfield eSense,not dominant,-,-,-,X,X,,-,-,-,-,-,,,-,dimensional,2,"stress, not stress",,-,X,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,140,-,-,-,-,"61,6 ",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"The smoothing algorithm excluded nine signals due to signal degeneration (see [5] for details), thus our final dataset consists 142 records, which are used in subsequent analysis.",https://sci-hub.se/10.1145/2808435.2808450,x
,,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,X,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,31,18,30.8,-,21-38,-,Yes,-,,,,x,-,X,X,X,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,Mindfield eSense,not dominant,-,-,-,X,X,,-,-,-,-,-,,,-,dimensional,2,"stress, not stress",,-,-,X,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,140,-,-,-,-,65,-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"The smoothing algorithm excluded nine signals due to signal degeneration (see [5] for details), thus our final dataset consists 142 records, which are used in subsequent analysis.",https://sci-hub.se/10.1145/2808435.2808450,x
,,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,X,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,31,18,30.8,-,21-38,-,Yes,-,,,,x,-,X,X,X,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,Mindfield eSense,not dominant,-,-,-,X,X,,-,-,-,-,-,,,-,dimensional,2,"stress, not stress",,-,-,-,X,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,140,-,-,-,-,"71,40",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"The smoothing algorithm excluded nine signals due to signal degeneration (see [5] for details), thus our final dataset consists 142 records, which are used in subsequent analysis.",https://sci-hub.se/10.1145/2808435.2808450,x
,,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,X,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,31,18,30.8,-,21-38,-,Yes,-,,,,x,-,X,X,X,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,Mindfield eSense,not dominant,-,-,-,X,X,,-,-,-,-,-,,,-,dimensional,2,"stress, not stress",,-,-,-,-,X,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,140,-,-,-,-,"65,10",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"The smoothing algorithm excluded nine signals due to signal degeneration (see [5] for details), thus our final dataset consists 142 records, which are used in subsequent analysis.",https://sci-hub.se/10.1145/2808435.2808450,x
,,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015, October). Stress recognition in human-computer interaction using physiological and self-reported data: a study of gender differences. In Proceedings of the 19th Panhellenic Conference on Informatics (pp. 323-328).","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",,2015,In Proceedings of the 19th Panhellenic Conference on Informatics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,categorical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Duplicate with Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450 ",,-
,,"Drungilas, D., Bielskis, A. A., & Denisov, V. (2010). An intelligent control system based on non-invasive man machine interaction. In Innovations in Computing Sciences and Software Engineering (pp. 63-68). Springer, Dordrecht.","Drungilas, D., Bielskis, A. A., & Denisov, V. ",An intelligent control system based on non-invasive man machine interaction,2010,Innovations in Computing Sciences and Software Engineering,-,X,,-,Lituania,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,categorical,8,"Fear, Surprise, Happy, Calmness, Sleepiness, Sad, Disgust, Anger",-,-,-,-,-,-,-,,,,,,,,,,,X,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,75,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,,,NO TENGO ACCESO
,,"Wu, G., Liu, G., & Hao, M. (2010). The Analysis of Emotion Recognition from GSR Based on PSO. 2010 International Symposium on Intelligence Information Processing and Trusted Computing. doi:10.1109/iptc.2010.60","Wu, G., Liu, G., & Hao, M.",The Analysis of Emotion Recognition from GSR Based on PSO,2010,2010 International Symposium on Intelligence Information Processing and Trusted Computing,-,X,,-,China,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,254,-,-,-,-,-,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,X,-,-,-,X,-,-,x,-,-,-,-,-,-,-,-,-,,,,,,,-,-,-,240-300s,-,-,-,MP150 Biopac,,,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,6,"happy, Suprise, Disgust, Grief, Angry, Fear",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,66%,-,,,,-,,-,,,,,,,,,-,-,X,-,El 66% es el promedio del promedio de la celda average de Rin 1 de IHPSO (tabla II); recognition rate es sensitivity por que el total de las veces que se presenta una emocion cuantas me reconoce como positiva; de todas las veces que aparece happy cuantas son correctas,https://sci-hub.se/10.1109/iptc.2010.60,X
,,"Giakoumis, D., Tzovaras, D., Moustakas, K., & Hassapis, G. (2011). Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features. IEEE Transactions on Affective Computing, 2(3), 119–133. doi:10.1109/t-affc.2011.4 ","Giakoumis, D., Tzovaras, D., Moustakas, K., & Hassapis, G.",Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features,2011,IEEE Transactions on Affective Computing,X,-,,-,Grecia ,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,19,4,-,-,23-44,-,Yes,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,x,,,,x,,,,,,,,,,-,-,-,-,X,-,-,-,X,-,-,-,X,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-,-,-,Procomp5 Infiniti,left,-,-,-,-,X,X,-,-,-,-,-,,,-,categorical,2,"bored, not bored",-,-,-,-,X,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,-,-,-,"82,10",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"Trials usually lasted from one-half to 8 minutes, with the majority of them lasting around half a minute. SEE TABLE 6",https://sci-hub.se/10.1109/T-AFFC.2011.4,x
,,"Safta, I., Grigore, O., & Căruntu, C.(2011). Emotion Detection Using Psycho-Physiological Signal Processing. Computer, 3, 4.","Safta, I., Grigore, O., & Căruntu, C",Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features,2011,2011 7TH INTERNATIONAL SYMPOSIUM ON ADVANCED TOPICS IN ELECTRICAL ENGINEERING (ATEE),-,X,,-,Romania,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,13,-,-,-,20-25,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,-,X,IAPS,X,,,-,-,-,-,,-,-,-,,,,,,,-,-,7s,-,-,-,-,Varioport-B,not dominant,X,-,-,-,-,-,-,-,-,-,-,,,-,dimensional,2,"pleaseant, unpleaseant ",-,-,X,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,,-,-,-,-,X,-,-,-,-,-,-,-,-,-,66%,67%,34%,33%,-,-,,-,,,,,,,,,-,-,X,-,The incorrect clasification ratio ????,https://www.researchgate.net/profile/Ovidiu-Grigore/publication/236681909_Emotion_detection_using_psycho-physiological_signal_processing/links/570396cb08ae646a9da9a73b/Emotion-detection-using-psycho-physiological-signal-processing.pdf,x
,,"Lee, H., Choi, Y. S., Lee, S., & Park, I. P. (2012, January). Towards unobtrusive emotion recognition for affective social communication. In 2012 IEEE Consumer Communications and Networking Conference (CCNC) (pp. 260-264). IEEE.","Lee, H., Choi, Y. S., Lee, S., & Park, I. P.",Towards unobtrusive emotion recognition for affective social communication.,2012,IEEE Consumer Communications and Networking Conference (CCNC),-,X,,-,Corea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,No tiene EDA,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.714.1220&rep=rep1&type=pdf,-
,,"Quazi, M. T., Mukhopadhyay, S. C., Suryadevara, N. K., & Huang, Y. M. (2012). Towards the smart sensors based human emotion recognition. 2012 IEEE International Instrumentation and Measurement Technology Conference Proceedings. doi:10.1109/i2mtc.2012.6229646",,Towards the smart sensors based human emotion recognition,2012,IEEE International Instrumentation and Measurement Technology Conference Proceedings,-,X,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,X,-,-,-,X,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,"Está en proceso, no tiene medidas de performance. Usa knn, left-hand, ",https://sci-hub.st/https://doi.org/10.1109/I2MTC.2012.6229646,-
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,X,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,-,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,X,-,Stroop ,-,,X,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,-,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,X,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"63,33",-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,"See table 3, ""The features derived from the GSR signal only achieved an average classification accuracy of 60.66% (Phase 2)""",https://d1wqtxts1xzle7.cloudfront.net/55993907/pub271-with-cover-page-v2.pdf?Expires=1635779458&Signature=EswPTSvP8rChFVtyiNSL-y8bAwtuOSmO2gEHmM9gUw0vmuvn~4Yzx92A7cBYOcOw9fQO9M1nNyOzTGs3xi3ordsBQoV6ClRHGbxEDtv4nSMSavSj3E3ErZKVPxFA-fPznVZG520u~-g6GgymxIiUhWFVoseMfJ1IhQl3zbJS1zeS0ZbWmPNo77O8wTUQim5742wzBIeVHxJCg9wVTFULzzey5utZiqj5ApvZUMGz9cpJRawkyrN71rrgzH3Ubpo-VCLuaMNj-2j4AiiNlR5oIxlAvKYNlA7AdOFfPFj4lURPOLbx6mDMlPKpWAiP2~pQMIEnTV6lMYiguIe1-G-ITQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA,x
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,X,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,-,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,X,-,Stroop ,-,,X,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,-,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,-,-,,,,,,,,,,,X,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"61,67",-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,"See table 3, ""The features derived from the GSR signal only achieved an average classification accuracy of 60.66% (Phase 2)""",https://d1wqtxts1xzle7.cloudfront.net/55993907/pub271-with-cover-page-v2.pdf?Expires=1635779458&Signature=EswPTSvP8rChFVtyiNSL-y8bAwtuOSmO2gEHmM9gUw0vmuvn~4Yzx92A7cBYOcOw9fQO9M1nNyOzTGs3xi3ordsBQoV6ClRHGbxEDtv4nSMSavSj3E3ErZKVPxFA-fPznVZG520u~-g6GgymxIiUhWFVoseMfJ1IhQl3zbJS1zeS0ZbWmPNo77O8wTUQim5742wzBIeVHxJCg9wVTFULzzey5utZiqj5ApvZUMGz9cpJRawkyrN71rrgzH3Ubpo-VCLuaMNj-2j4AiiNlR5oIxlAvKYNlA7AdOFfPFj4lURPOLbx6mDMlPKpWAiP2~pQMIEnTV6lMYiguIe1-G-ITQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA,x
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,X,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,-,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,X,-,Stroop ,-,,X,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,-,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,-,X,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,60,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,"See table 3, ""The features derived from the GSR signal only achieved an average classification accuracy of 60.66% (Phase 2)""",https://d1wqtxts1xzle7.cloudfront.net/55993907/pub271-with-cover-page-v2.pdf?Expires=1635779458&Signature=EswPTSvP8rChFVtyiNSL-y8bAwtuOSmO2gEHmM9gUw0vmuvn~4Yzx92A7cBYOcOw9fQO9M1nNyOzTGs3xi3ordsBQoV6ClRHGbxEDtv4nSMSavSj3E3ErZKVPxFA-fPznVZG520u~-g6GgymxIiUhWFVoseMfJ1IhQl3zbJS1zeS0ZbWmPNo77O8wTUQim5742wzBIeVHxJCg9wVTFULzzey5utZiqj5ApvZUMGz9cpJRawkyrN71rrgzH3Ubpo-VCLuaMNj-2j4AiiNlR5oIxlAvKYNlA7AdOFfPFj4lURPOLbx6mDMlPKpWAiP2~pQMIEnTV6lMYiguIe1-G-ITQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA,x
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,X,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,-,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,X,-,Stroop ,-,,X,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,-,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,X,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"57,22",-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,"See table 3, ""The features derived from the GSR signal only achieved an average classification accuracy of 60.66% (Phase 2)""",https://d1wqtxts1xzle7.cloudfront.net/55993907/pub271-with-cover-page-v2.pdf?Expires=1635779458&Signature=EswPTSvP8rChFVtyiNSL-y8bAwtuOSmO2gEHmM9gUw0vmuvn~4Yzx92A7cBYOcOw9fQO9M1nNyOzTGs3xi3ordsBQoV6ClRHGbxEDtv4nSMSavSj3E3ErZKVPxFA-fPznVZG520u~-g6GgymxIiUhWFVoseMfJ1IhQl3zbJS1zeS0ZbWmPNo77O8wTUQim5742wzBIeVHxJCg9wVTFULzzey5utZiqj5ApvZUMGz9cpJRawkyrN71rrgzH3Ubpo-VCLuaMNj-2j4AiiNlR5oIxlAvKYNlA7AdOFfPFj4lURPOLbx6mDMlPKpWAiP2~pQMIEnTV6lMYiguIe1-G-ITQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA,x
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,X,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,-,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,X,-,Stroop ,-,,X,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,-,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,X,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"61,11",-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,"See table 3, ""The features derived from the GSR signal only achieved an average classification accuracy of 60.66% (Phase 2)""",https://d1wqtxts1xzle7.cloudfront.net/55993907/pub271-with-cover-page-v2.pdf?Expires=1635779458&Signature=EswPTSvP8rChFVtyiNSL-y8bAwtuOSmO2gEHmM9gUw0vmuvn~4Yzx92A7cBYOcOw9fQO9M1nNyOzTGs3xi3ordsBQoV6ClRHGbxEDtv4nSMSavSj3E3ErZKVPxFA-fPznVZG520u~-g6GgymxIiUhWFVoseMfJ1IhQl3zbJS1zeS0ZbWmPNo77O8wTUQim5742wzBIeVHxJCg9wVTFULzzey5utZiqj5ApvZUMGz9cpJRawkyrN71rrgzH3Ubpo-VCLuaMNj-2j4AiiNlR5oIxlAvKYNlA7AdOFfPFj4lURPOLbx6mDMlPKpWAiP2~pQMIEnTV6lMYiguIe1-G-ITQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA,x
,,"AlZoubi, O., D’Mello, S. K., & Calvo, R. A. (2012). Detecting Naturalistic Expressions of Nonbasic Affect Using Physiological Signals. IEEE Transactions on Affective Computing, 3(3), 298–310. doi:10.1109/t-affc.2012.4","AlZoubi, O., D’Mello, S. K., & Calvo, R. A",Detecting Naturalistic Expressions of Nonbasic Affect Using Physiological Signals,2012,IEEE Transactions on Affective Computing,X,-,,-,Australia,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,27,8,26.7,-,-,-,Yes,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,X,-,,,,x,,,,x,,,x,x,x,x,x,,,-,-,-,-,X,X,-,-,X,-,Interaccion con Auto-Tutor ,-,,,-,-,-,-,,-,X,-,,,,,,,,-,2700s,-,-,-,-,BIOPAC-MP150,left,-,-,X,X,-,-,-,-,-,-,-,,,-,categorical,7,"Flow, boredom, neutral, curiosity, confusion, delight, frustration",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,Multimodal,https://sci-hub.se/10.1109/t-affc.2012.4,x
,,"Gu, Y., Wong, K.-J., & Tan, S.-L. (2012). Analysis of physiological responses from multiple subjects for emotion recognition. 2012 IEEE 14th International Conference on e-Health Networking, Applications and Services (Healthcom). doi:10.1109/healthcom.2012.6379388 ","Gu, Y., Wong, K.-J., & Tan, S.-L",Analysis of physiological responses from multiple subjects for emotion recognition,2012,"IEEE 14th International Conference on e-Health Networking, Applications and Services (Healthcom)",-,X,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,18-25,-,a,-,,,,,PANAS ??,X,X,X,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,-,-,-,-,X,-,X,,,-,-,-,-,,-,-,-,,,,,,,-,-,6s,-,-,-,,,,,,,,,,,,,,,,,,categorical,-,"baja valencia, neutral, alta valencia, bajo arousal, neutral, alto arousal ",,-,-,-,X,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,Multimodal,,-
,,"Li, B., Feng, S., Xiong, W., & Hu, W. (2012). Scaring or pleasing. Proceedings of the 20th ACM International Conference on Multimedia - MM ’12. doi:10.1145/2393347.2396487","Li, B., Feng, S., Xiong, W., & Hu, W.",Scaring or pleasing,2012,Proceedings of the 20th ACM International Conference on Multimedia - MM ’12,-,X,,-,China,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,categorical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,No tiene performance,,x
,,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ","Cheng, J., & Liu, G.",Computing nonlinear features of skin conductance to build the affective detection model,2013,"International Conference on Communications, Circuits and Systems (ICCCAS)",-,X,,-,China,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,20,-,-,-,No,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,X,-,-,-,X,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,BIOPAC-MP150,-,-,-,-,-,-,-,-,-,-,-,-,,,-,categorical,2,"hapinees, sadness and fear",-,X,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,-,-,-,-,-,-,92.33%,-,14.31%,,-,-,,-,,,,,,,,,-,-,X,-,"For a binary classification, True Positive Rate (TPR) and False Positive Rate (FPR) are often used as criterion to evaluate identification capability of the classifier. From the Table I we will find the TPR of happiness is highest, and achieved 92.33%, the TPR of sadness and fear is low, respectively, 75% and 70.71%. This situation is caused by the differences in the number of samples. ",,x
,,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ","Cheng, J., & Liu, G.",Computing nonlinear features of skin conductance to build the affective detection model,2013,"International Conference on Communications, Circuits and Systems (ICCCAS)",-,X,,-,China,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,20,-,-,-,No,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,X,-,-,-,X,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,BIOPAC-MP150,-,-,-,-,-,-,-,-,-,-,-,-,,,-,categorical,2," sadness and fears , hapiness",-,X,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,-,-,-,-,-,-,75.00%,-,13.42%,,-,-,,-,,,,,,,,,-,-,X,-,"For a binary classification, True Positive Rate (TPR) and False Positive Rate (FPR) are often used as criterion to evaluate identification capability of the classifier. From the Table I we will find the TPR of happiness is highest, and achieved 92.33%, the TPR of sadness and fear is low, respectively, 75% and 70.71%. This situation is caused by the differences in the number of samples. ",,x
,,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ","Cheng, J., & Liu, G.",Computing nonlinear features of skin conductance to build the affective detection model,2013,"International Conference on Communications, Circuits and Systems (ICCCAS)",-,X,,-,China,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,20,-,-,-,No,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,X,-,-,-,X,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,BIOPAC-MP150,-,-,-,-,-,-,-,-,-,-,-,-,,,-,categorical,2,"fear and hapiness, sadness",-,X,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,-,-,-,-,-,-,70.71%,-,17.24%,,-,-,,-,,,,,,,,,-,-,X,-,"For a binary classification, True Positive Rate (TPR) and False Positive Rate (FPR) are often used as criterion to evaluate identification capability of the classifier. From the Table I we will find the TPR of happiness is highest, and achieved 92.33%, the TPR of sadness and fear is low, respectively, 75% and 70.71%. This situation is caused by the differences in the number of samples. ",,x
,,"Buodo, G., Moscardino, U., Scrimin, S., Altoè, G., & Palomba, D. (2013). Parenting Stress and Externalizing Behavior Symptoms in Children: The Impact of Emotional Reactivity. Child Psychiatry & Human Development, 44(6), 786–797. doi:10.1007/s10578-013-0371-0 ","Buodo, G., Moscardino, U., Scrimin, S., Altoè, G., & Palomba, D.",Parenting Stress and Externalizing Behavior Symptoms in Children: The Impact of Emotional Reactivity,2013,Child Psychiatry & Human Development,X,-,,-,Italia,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,67,32,-,,9-12,Italia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,X,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,6-10s,-,-,-,,non-dominant,X,-,-,-,-,-,-,-,-,-,-,,,,categorical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,no predice emociones,https://sci-hub.se/10.1007/s10578-013-0371-0,-
,,"Leite, I., Henriques, R., Martinho, C., & Paiva, A. (2013). Sensors in the wild: Exploring electrodermal activity in child-robot interaction. 2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI). doi:10.1109/hri.2013.6483500 ","Leite, I., Henriques, R., Martinho, C., & Paiva, A.",Sensors in the wild: Exploring electrodermal activity in child-robot interaction,,8th ACM/IEEE International Conference on Human-Robot Interaction (HRI),-,X,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,38,23,-,-,8-9,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,X,-,Interaccion con un robot en un ambiente escolar,-,,,-,-,-,-,,-,-,-,,,,,,,-,-,,,,,,Q-Sensor,,,,,,,,,,,,,,,,categorical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,no predice emociones,https://sci-hub.se/10.1109/HRI.2013.6483500,-
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,16,26.8,-,24-34,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stoop color word interfence test (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,,-,-,-,X,X,,,,,,,,,,dimensional,2,"relaxed, stress",-,-,X,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,-,-,-,-,"65,79",-,-,-,-,-,,,-,,-,,,,,,,,,-,-,X,-,Entrena con fases ,https://sci-hub.se/10.1109/T-AFFC.2012.25,x
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,16,26.8,-,24-34,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stoop color word interfence test (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,-,-,,,,,,,,,,,X,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"54,61",-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,,x
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,16,26.8,-,24-34,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stoop color word interfence test (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,-,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,"58,55",-,-,-,-,-,,-,-,,-,,,,,,,,,-,,X,-,-,,x
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,16,26.8,-,24-34,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stoop color word interfence test (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,X,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,,-,-,-,"55,92",-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,,x
,,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,X,-,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,30,16,26.8,-,24-34,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,Stoop color word interfence test (SCWT),-,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-,-,-,GSR-2,,-,-,-,X,X,-,-,-,-,-,-,,,-,dimensional,2,"relaxed, stress",-,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,X,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"55,26",-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,,x
,,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.","Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G.",Pervasive and unobtrusive emotion sensing for human mental health,2013,7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops,-,X,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,4,-,-,-,-,-,Yes,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,X,-,-,-,X,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,-,300-480s,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,,,-,categorical,4,"Amusement, fear, relax, sadness",-,-,X,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"70,20",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"amusement, fear, relax, and
sadness",http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.712.6416&rep=rep1&type=pdf,x
,,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.","Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G.",Pervasive and unobtrusive emotion sensing for human mental health,2013,7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops,-,X,,-,USA,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,4,-,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,X,X,-,-,-,X,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,-,300-480s,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,,,-,categorical,4,"Amusement, fear, relax, sadness",-,-,X,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"59,25",-,-,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,"amusement, fear, relax, and
sadness",http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.712.6416&rep=rep1&type=pdf,x
,,"Moghimi, S., Chau, T., & Guerguerian, A.-M. (2013). Using prefrontal cortex near-infrared spectroscopy and autonomic nervous system activity for identifying music-induced emotions. 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER). doi:10.1109/ner.2013.6696175","Moghimi, S., Chau, T., & Guerguerian, A.-M.",Using prefrontal cortex near-infrared spectroscopy and autonomic nervous system activity for identifying music-induced emotions,2013,6th International IEEE/EMBS Conference on Neural Engineering (NER),-,X,,-,-,X,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,10,5,25,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,X,-,-,,,-,X,-,-,,-,-,-,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,"MULTIMODAL; The average heart rate and PVA signals within the 45 sec period of exposure to music were also
included as features to represent cardiovascular response. It is not clear if the stimulus lasts 45 s",https://sci-hub.se/10.1109/NER.2013.6696175,-
,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,X,,-,Portugal,-,X,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,26,-,21,-,19-24,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,robot interaction ,-,10-30s,-,-,-,Affectiva-QSensors5,-,-,-,-,-,-,-,-,-,-,-,X,,,-,categorical,5,"empathy, expectation, positive-surprise, stress, frustration",-,-,X,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/10.1109/ACII.2013.14,x
,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,X,,-,Portugal,-,X,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,26,-,21,-,19-24,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,robot interaction ,-,10-30s,-,-,-,Affectiva-QSensors5,,,,-,-,-,-,-,-,-,-,X,,,-,categorical,5,"empathy, expectation, positive-surprise, stress, frustration",-,-,,-,-,X,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,,,,,,,,-,-,-,-,x,"Additionally, we showed that psychological traits can guide this task by correcting profile-driven differences, opening a new direction on how to measure affective interactions. The quantitative assessment shows that emotion recognition significantly improves when adopting more flexible methods to mine the electrodermal signal. Additionally, feature correlation
analysis and discriminative mining of generative models show meaningful differences among emotional stimuli and experimental settings. These answers to the target research questions trigger new implications not only for psychophysiology and neuroscience, but especially to human-robot and social interaction research.",-,-,-,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/10.1109/ACII.2013.14,x
,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,X,,-,Portugal,-,X,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,26,-,21,-,19-24,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,robot interaction ,-,10-30s,-,-,-,Affectiva-QSensors5,-,,-,-,-,,-,-,-,-,-,X,,,-,categorical,5,"empathy, expectation, positive-surprise, stress, frustration",-,-,-,-,-,-,X,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,,-,-,-,-,-,-,-,-,x,"Additionally, we showed that psychological traits can guide this task by correcting profile-driven differences, opening a new direction on how to measure affective interactions. The quantitative assessment shows that emotion recognition significantly improves when adopting more flexible methods to mine the electrodermal signal. Additionally, feature correlation
analysis and discriminative mining of generative models show meaningful differences among emotional stimuli and experimental settings. These answers to the target research questions trigger new implications not only for psychophysiology and neuroscience, but especially to human-robot and social interaction research.",-,,-,-,,-,-,,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/10.1109/ACII.2013.14,x
,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,X,,-,Portugal,-,X,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,26,-,21,-,19-24,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,robot interaction ,-,10-30s,-,-,-,Affectiva-QSensors5,-,-,-,-,-,-,-,-,-,-,-,X,,,-,categorical,5,"empathy, expectation, positive-surprise, stress, frustration",-,X,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,x,"Additionally, we showed that psychological traits can guide this task by correcting profile-driven differences, opening a new direction on how to measure affective interactions. The quantitative assessment shows that emotion recognition significantly improves when adopting more flexible methods to mine the electrodermal signal. Additionally, feature correlation
analysis and discriminative mining of generative models show meaningful differences among emotional stimuli and experimental settings. These answers to the target research questions trigger new implications not only for psychophysiology and neuroscience, but especially to human-robot and social interaction research.",-,-,-,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/10.1109/ACII.2013.14,x
,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,X,,-,Portugal,-,X,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,26,-,21,-,19-24,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,robot interaction ,-,10-30s,-,-,-,Affectiva-QSensors5,,,,,,,,,,,,X,,,,categorical,5,"empathy, expectation, positive-surprise, stress, frustration",X,-,-,-,-,-,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,x,"Additionally, we showed that psychological traits can guide this task by correcting profile-driven differences, opening a new direction on how to measure affective interactions. The quantitative assessment shows that emotion recognition significantly improves when adopting more flexible methods to mine the electrodermal signal. Additionally, feature correlation
analysis and discriminative mining of generative models show meaningful differences among emotional stimuli and experimental settings. These answers to the target research questions trigger new implications not only for psychophysiology and neuroscience, but especially to human-robot and social interaction research.",-,-,-,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/10.1109/ACII.2013.14,x
,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,X,,-,Portugal,-,X,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,13,5,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,robot interaction ,-,10-30s,-,-,-,Affectiva-QSensors5,,,,,,,,,,,,X,,,,categorical,5,"empathy, expectation, positive-surprise, stress, frustration",X,-,-,-,-,-,-,,,,,,,,x,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,x,"Additionally, we showed that psychological traits can guide this task by correcting profile-driven differences, opening a new direction on how to measure affective interactions. The quantitative assessment shows that emotion recognition significantly improves when adopting more flexible methods to mine the electrodermal signal. Additionally, feature correlation
analysis and discriminative mining of generative models show meaningful differences among emotional stimuli and experimental settings. These answers to the target research questions trigger new implications not only for psychophysiology and neuroscience, but especially to human-robot and social interaction research.",-,-,44,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/10.1109/ACII.2013.14,x
,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,X,,-,Portugal,-,X,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,13,5,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,robot interaction ,-,10-30s,-,-,-,Affectiva-QSensors5,,,,,,,,,,,,X,,,,categorical,5,"empathy, expectation, positive-surprise, stress, frustration",X,-,-,-,-,-,-,,,,,,,,x,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,x,"Additionally, we showed that psychological traits can guide this task by correcting profile-driven differences, opening a new direction on how to measure affective interactions. The quantitative assessment shows that emotion recognition significantly improves when adopting more flexible methods to mine the electrodermal signal. Additionally, feature correlation
analysis and discriminative mining of generative models show meaningful differences among emotional stimuli and experimental settings. These answers to the target research questions trigger new implications not only for psychophysiology and neuroscience, but especially to human-robot and social interaction research.",-,-,43,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/10.1109/ACII.2013.14,x
,,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,X,,-,Portugal,-,X,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,26,-,21,-,19-24,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,X,-,-,-,,,-,-,-,-,,-,-,-,,,,,,,-,robot interaction ,-,10-30s,-,-,-,Affectiva-QSensors5,,,,,,,,,,,,X,,,,categorical,5,"empathy, expectation, positive-surprise, stress, frustration",X,-,-,-,-,-,-,,,,,,,,x,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,x,"Additionally, we showed that psychological traits can guide this task by correcting profile-driven differences, opening a new direction on how to measure affective interactions. The quantitative assessment shows that emotion recognition significantly improves when adopting more flexible methods to mine the electrodermal signal. Additionally, feature correlation
analysis and discriminative mining of generative models show meaningful differences among emotional stimuli and experimental settings. These answers to the target research questions trigger new implications not only for psychophysiology and neuroscience, but especially to human-robot and social interaction research.",-,-,44,-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,https://sci-hub.se/10.1109/ACII.2013.14,x
,,"Kapp, D., Schaaff, K., Ottenbacher, J., Heuer, S., & Stork, W. (2014). Evaluation of environmental effects on the measurement of electrodermal activity under real-life conditions. BIOMEDICAL ENGINEERING-BIOMEDIZINISCHE TECHNIK, 59, S239-.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No predice,https://sci-hub.se/10.1515/bmt-2014-5002,-
,,"Li, S., Guo, R., He, L., Gao, W., Qi, H., & Owens, G. (2014). MoodMagician. Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems - SenSys ’14. doi:10.1145/2668332.2668371","Li, S., Guo, R., He, L., Gao, W., Qi, H., & Owens, G.",MoodMagician,2014,Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems,-,X,,-,USA,x,-,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,-,4,-,-,-,-,-,No,-,,,,,-,-,-,-,-,-,-,,,-,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,,-,,-,-,X,X,-,-,-,X,-,-,X,,-,-,-,-,,-,-,-,,,,,,,-,-,-,240s-300s,-,-,-,Shimmer,-,-,-,-,-,-,-,-,-,-,-,-,,,-,categorical,4,"amusement,fear, relax, sadness",-,-,-,-,-,X,-,,,,,,,,,,,-,-,-,,-,,,,,,,,,-,,,-,-,-,-,-,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,"54,08",-,-,-,-,-,,-,-,,-,,,,,,,,,-,-,X,-,-,-,X
,,"Saha, S., Saha, S., & Zahir, N. E. B. M. (2014). Significance of orienting reflex on emotional resilience in explaining high performance in soccer players. International Medical Journal, 21(5), 459-462.","Saha, S., Saha, S., & Zahir, N. E. B. M.",Significance of orienting reflex on emotional resilience in explaining high performance in soccer players,2014, International Medical Journal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,No predice,,NO TENGO ACCESO
,,"Eum, Y. J., Jang, E. H., Park, B. J., Choi, S. S., Kim, S. H., & Sohn, J. H. (2011). Emotion recognition by ANS responses evoked by negative emotion. In 2011 2nd International Conference on Engineering and Industries (ICEI) (pp. 1-4). IEEE.","Eum, Y. J., Jang, E. H., Park, B. J., Choi, S. S., Kim, S. H., & Sohn, J. H.",Emotion recognition by ANS responses evoked by negative emotion,2011,In 2011 2nd International Conference on Engineering and Industries (ICEI),-,X,,-,Korea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,multimodal ,,-
,,"Park, B. J., Jang, E. H., Kim, S. H., & Chung, M. A. (2012, December). Emotion induction and emotion recognition using their physiological signals. In 2012 7th International Conference on Computing and Convergence Technology (ICCCT) (pp. 1252-1255). IEEE.","Park, B. J., Jang, E. H., Kim, S. H., & Chung, M. A.",Emotion induction and emotion recognition using their physiological signals,2012,In 2012 7th International Conference on Computing and Convergence Technology (ICCCT),-,X,,-,Korea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,multimodal ,,-
,,"Jang, E. H., Park, B. J., Kim, S., Huh, C., Eum, Y., & Sohn, J. (2012, January). Emotion recognition through ANS responses evoked by negative emotions. In The Fifth International Conference on Advances in Computer-Human Interactions (ACHI) (pp. 218-223).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,multimodal ,,-
,,"Park, B. J., Jang, E. H., Kim, S. H., Huh, C., Chung, M., & Sohn, J. H. (2013). Emotion recognition using autonomic nervous system responses. In Proceedings of The Sixth International Conference on Advances in Computer-Human Interactions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,multimodal ,,-
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,X,,,,China,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,,,,,,,,,,,,x,,,,,,-,-,-,-,-,,,86.92,,85.96,,,,,,,,,,,,,,,,,,x,,,https://www.mdpi.com/2078-2489/11/4/212/htm,,
,,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,X,,,,China,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,,,,,,,,,,,,,x,,,,,-,-,-,-,-,,,75.58,,74.71,,,,,,,,,,,,,,,,,,x,,,https://www.mdpi.com/2078-2489/11/4/212/htm,,
,,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,X,,,,China,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,,,,,,,,,,,,x,x,,,,,-,-,-,-,-,,,81.34,,80.7,,,,,,,,,,,,,,,,,,x,,,https://www.mdpi.com/2078-2489/11/4/212/htm,,
,,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,X,,,,China,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,,,,,,,,,,,,x,,,,,,-,-,-,-,-,,,86.73,,85.71,,,,,,,,,,,,,,,,,,x,,,https://www.mdpi.com/2078-2489/11/4/212/htm,,
,,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,X,,,,China,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,,,,,,,,,,,,,x,,,,,-,-,-,-,-,,,82.12,,81.46,,,,,,,,,,,,,,,,,,x,,,https://www.mdpi.com/2078-2489/11/4/212/htm,,
,,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,X,,,,China,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,,,,,,,,,,,,x,x,,,,,-,-,-,-,-,,,82.53,,80.82,,,,,,,,,,,,,,,,,,x,,,https://www.mdpi.com/2078-2489/11/4/212/htm,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,59,60,58,60,,,,,,,,,,,,,,,,,x,,,,https://www.mdpi.com/1424-8220/19/7/1659/htm,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,74,76,75,75,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,44,48,42,44,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,80,80,80,80,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,85,85,85,85,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,55,53,51,56,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,68,70,70,70,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,37,43,35,39,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,74,76,75,75,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,81,81,81,81,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,41,70,45,42,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,64,65,65,65,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,27,43,33,27,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,72,73,72,73,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,78,78,78,78,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,44,50,40,44,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,69,70,69,69,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,36,31,28,36,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,75,76,76,75,,,,,,,,,,,,,,,,,x,,,,,
,,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,X,,,,Germany,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Additionally, analyzing the results of the state-of-art, clearly, feature engineering for
subject-independent and subject-dependent human emotion detection based on EDA does not lead
to high performance. In particular, when the number of classes is higher than two. This is because
extracting the sympathetic response patterns which are part of each emotion is difficult. Furthermore,
when trying to overcome this fact by analyzing more basic features such as level, response amplitude,
rate, rise time, and recovery time, they discard flexible elicited behavior which might improve emotion
recognition. Therefore, it has been proven in this work that DL can overcome this drawback quite well.
Regarding the point of testing the proposed model using different datasets from different labs,
it is because human emotions do not form similar patterns. Consequently, the research community
should develop generalized models to recognize human emotions, where subjects, elicitation materials,
and physiological sensors brands are different from the ones involved in the initial training.",,,82,83,83,82,,,,,,,,,,,,,,,,,x,,,,,
,,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.","J, Seo; TH, Laine; KA, Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data.
",2019,Sensors,X,,,,Korea,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,28,15,23.62,,20-34,Korea,-,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,x,,,,,,,,,,,,,x,X,X,,,,,,,x,,,,,,,,,,,,,,,,,,00 m 07 ss,,,,-,Grove GSR sensor produced by Seeed,-,x,-,x,x,-,-,-,x,-,-,-,-,,-,dimensional,2,"weak boredom, strong boredom",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Another major finding is that EEG and GSR appear to correlate with boredom, thus supporting
the conclusion of Bench and Lench [34] that boredom and autonomic nervous system are linked.",,,68.33,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.","J, Seo; TH, Laine; KA, Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data.
",2019,Sensors,X,,,,Korea,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,28,15,23.62,,20-34,Korea,-,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,x,,,,,,,,,,,,,X,X,X,,,,,,,x,,,,,,,,,,,,,,,,,,00 m 07 ss,,,,-,Grove GSR sensor produced by Seeed,-,x,-,x,x,-,-,-,x,-,-,-,-,,-,dimensional,2,"weak boredom, strong boredom",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Another major finding is that EEG and GSR appear to correlate with boredom, thus supporting
the conclusion of Bench and Lench [34] that boredom and autonomic nervous system are linked.",,,66.86,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.","J, Seo; TH, Laine; KA, Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data.
",2019,Sensors,X,,,,Korea,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,28,15,23.62,,20-34,Korea,-,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,x,,,,,,,,,,,,,X,X,X,,,,,,,x,,,,,,,,,,,,,,,,,,00 m 07 ss,,,,-,Grove GSR sensor produced by Seeed,-,x,-,x,x,-,-,-,x,-,-,-,-,,-,dimensional,2,"weak boredom, strong boredom",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Another major finding is that EEG and GSR appear to correlate with boredom, thus supporting
the conclusion of Bench and Lench [34] that boredom and autonomic nervous system are linked.",,,70.03,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE,,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,-,-,-,-,-,Yes,SAM,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,x,x,x,,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,-,BIOPAC MP36,not dominant,x,-,-,-,x,-,x,-,-,-,-,-,,-,dimensional,4,"Low Relaxation Response, Medium Relaxation Response, high relaxation Response, no relaxation response",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,77.3,78.5,79.7,,,,,,,,,,,,,,,,,x,,,https://ieeexplore.ieee.org/abstract/document/8680625,,
,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE,,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,-,-,-,-,-,Yes,SAM,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,x,x,x,,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,-,BIOPAC MP36,not dominant,x,-,-,-,x,-,x,-,-,-,-,-,,-,dimensional,4,"Low Relaxation Response, Medium Relaxation Response, high relaxation Response, no relaxation response",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,99.2,99.2,99.2,,,,,,,,,,,,,,,,,x,,,https://ieeexplore.ieee.org/abstract/document/8680625,,
,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE,,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,-,-,-,-,-,Yes,SAM,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,x,x,x,,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,-,BIOPAC MP36,not dominant,x,-,-,-,x,-,x,-,-,-,-,-,,-,dimensional,4,"Low Relaxation Response, Medium Relaxation Response, high relaxation Response, no relaxation response",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,96.9,96.9,96.9,,,,,,,,,,,,,,,,,x,,,https://ieeexplore.ieee.org/abstract/document/8680625,,
,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE,,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,-,-,-,-,-,Yes,SAM,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,x,x,x,,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,-,BIOPAC MP36,not dominant,x,-,-,-,x,-,x,-,-,-,-,-,,-,dimensional,4,"Low Relaxation Response, Medium Relaxation Response, high relaxation Response, no relaxation response",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,85.9,84,82.3,,,,,,,,,,,,,,,,,x,,,https://ieeexplore.ieee.org/abstract/document/8680625,,
,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE,,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,-,-,-,-,-,Yes,SAM,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,x,x,x,,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,-,BIOPAC MP36,not dominant,x,-,-,-,x,-,x,-,-,-,-,-,,-,dimensional,4,"Low Relaxation Response, Medium Relaxation Response, high relaxation Response, no relaxation response",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,95.3,95.3,95.3,,,,,,,,,,,,,,,,,x,,,https://ieeexplore.ieee.org/abstract/document/8680625,,
,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE,,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,-,-,-,-,-,Yes,SAM,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,x,x,x,,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,-,BIOPAC MP36,not dominant,x,-,-,-,x,-,x,-,-,-,-,-,,-,dimensional,4,"Low Relaxation Response, Medium Relaxation Response, high relaxation Response, no relaxation response",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,81.4,74,73.9,,,,,,,,,,,,,,,,,x,,,https://ieeexplore.ieee.org/abstract/document/8680625,,
,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE,,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,-,-,-,-,-,Yes,SAM,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,x,x,x,,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,-,BIOPAC MP36,not dominant,x,-,-,-,x,-,x,-,-,-,-,-,,-,dimensional,4,"Low Relaxation Response, Medium Relaxation Response, high relaxation Response, no relaxation response",x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,95.2,95.1,95.1,,,,,,,,,,,,,,,,,x,,,https://ieeexplore.ieee.org/abstract/document/8680625,,
,,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE,,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,-,-,-,-,-,Yes,SAM,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,x,x,x,,,x,x,,,x,,,x,,,,,,,x,,,,,,,,40 m 00 s,,,,-,BIOPAC MP36,not dominant,x,-,-,-,x,-,x,-,-,-,-,-,,-,dimensional,4,"Low Relaxation Response, Medium Relaxation Response, high relaxation Response, no relaxation response",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,89.2,87.8,86.4,,,,,,,,,,,,,,,,,x,,,https://ieeexplore.ieee.org/abstract/document/8680625,,
,,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.","Sharma, V.; Prakash, N.R.; Kalra, P.","Audio-video emotional response mapping based upon Electrodermal Activity
",2019,Biomedical Signal Processing and Control,X,,,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,x,,,,,,,,,,,,,,,1m 0s,,,,-,-,not dominant,x,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,79,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.","Sharma, V.; Prakash, N.R.; Kalra, P.","Audio-video emotional response mapping based upon Electrodermal Activity
",2019,Biomedical Signal Processing and Control,X,,,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,x,,,,,,,,,,,,,,,1m 0s,,,,-,-,not dominant,x,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,69.8,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.","Sharma, V.; Prakash, N.R.; Kalra, P.","Audio-video emotional response mapping based upon Electrodermal Activity
",2019,Biomedical Signal Processing and Control,X,,,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,x,,,,,,,,,,,,,,,1m 0s,,,,-,-,not dominant,x,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"high dominance, low dominance",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,71.4,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Dar, M. N., Akram, M. U., Khawaja, S. G., & Pujari, A. N. (2020). Cnn and lstm-based emotion charting using physiological signals. Sensors, 20(16), 4551.","Dar, M.N.; Akram, M.U.; Khawaja, S.G.; Pujari, A.N.","Cnn and lstm-based emotion charting using physiological signals
",2020,Sensors,X,,,,Pakistan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,X,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,51 s–150 s 8amigos),,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Another
reason for the better performance of AMIGOS as compared to DREAMER is due to the nature of
the self-assessment acquisition process. Self-assessment for the AMIGOS dataset was obtained on
a scale of 1–9 for arousal and valence separately. However, for the DREAMER dataset, self-assessment rom subjects was acquired on the scale of 1–5 for both valence and arousal. The scale of 1–5 not only
exhibits half the freedom of choice on an intensity scale of emotion but also restricts the imbalance
created by avoiding the midpoint between 1–5 scale as participants can only provide integer data for
the intensity of arousal and valence. However, AMIGOS gives participants the liberty to self-assess
in a floating-point number for the scale of 1–9, hence better categorization of emotion can be made
which implied better performance of the algorithm on this dataset comparatively",,,63.67,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Greco, A., Marzi, C., Lanata, A., Scilingo, E. P., & Vanello, N. (2019, July). Combining electrodermal activity and speech analysis towards a more accurate emotion recognition system. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 229-232). IEEE.","A, Greco; C, Marzi; A, Lanata; EP, Scilingo; N, Vanello","Combining Electrodermal Activity and Speech Analysis towards a more Accurate Emotion Recognition System.
",2019,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,,X,,,Italy,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,18,12,,,,Italy,Yes,,,,,,,X,x,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,x,,,,x,,,,,,,,,,,,,,,,,0m 2s,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Particularly, although EDA is one of
the most popular signals for measuring emotional arousal,
and the same processing methods have been successfully
applied in previous studies using emotional videos, images,
sounds, and touch [28]–[30], the recognition accuracy obtained in this case was not much higher than 50%. The cause
could be found in the altered respiration activity induced
by speech that affects ANS dynamics and covers up the
sympathetic arousal response",,,52.04,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,72,55.78,71.41,99.24,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,58.52,54.74,51.19,51.76,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,54.5,54.79,69.92,96.61,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,51.9,53.67,61.32,71.53,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,52.7,55.05,68.88,91.99,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,50.8,56.86,57.51,58.18,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,75,66.1,79.3,99.2,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,54.05,49.88,49.87,49.86,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,64.4,66.05,78.25,95.9,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,65.5,64.34,78.3,100,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,63.3,64.14,77.13,96.71,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",x,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,63.28,64.19,77.25,96.96,,,,,,,,,,,,,,,,,x,,,,,
,,"Lee, S., Lee, T., Yang, T., Yoon, C., & Kim, S. P. (2020). Detection of drivers’ anxiety invoked by driving situations using multimodal biosignals. Processes, 8(2), 155.","Lee, S.; Lee, T.; Yang, T.; Yoon, C.; Kim, S.P.","Detection of drivers' anxiety invoked by driving situations using multimodal biosignals
",2020,Processes,X,,,,Korea,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,31,15,23.26,-,-,-,No,,,,,,,,,,,,,,,,X,,,,,,,,,,X,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,0m 30s,,,,-,Empatica E4,-,-,-,-,-,-,-,-,-,-,-,x,-,,-,categorical,2,"anxiety, neutral",x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,42.53,,,,,,,,,,,,,,,,,,,,x,,,https://sci-hub.se/https://doi.org/10.3390/pr8020155,,
,,"García-Faura, Á., Hernández-García, A., Fernández-Martínez, F., Díaz-de-María, F., & San-Segundo, R. (2019, January). Emotion and attention: Audiovisual models for group-level skin response recognition in short movies. In Web Intelligence (Vol. 17, No. 1, pp. 29-40). IOS Press.","García-Faura, A.; Hernández-García, A.; Fernández-Martínez, F.; Díaz-De-María, F.; San-Segundo, R.","Emotion and attention: Audiovisual models for group-level skin response recognition in short movies
",2019,Web Intelligence,X,,,,Spain,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,23.11,-,17-60,-,No,,,,,,,X,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,x,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,-,"Sociograph
",-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,x,,,,,,,,,,,,,,,,,,,,,-,-,,,74.86,,,,,,,,,,,,,,,,,,,,,,,,,
,,"Wei, W., Jia, Q., Feng, Y., & Chen, G. (2018). Emotion recognition based on weighted fusion strategy of multichannel physiological signals. Computational intelligence and neuroscience, 2018.","W, Wei; Q, Jia; Y, Feng; G, Chen","Emotion Recognition Based on Weighted Fusion Strategy of Multichannel Physiological Signals.
",2018,Computational Intelligence and Neuroscience,X,,,,China,,,,,X,,,X,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"sadness, happiness, disgust, neutral, fear",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,57.69,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,X,,,Egypt,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"happy, neutral, sad",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,93,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,X,,,Egypt,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"happy, neutral, sad",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,92.3,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,X,,,Egypt,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"happy, neutral, sad",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,93,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,X,,,Egypt,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"excited, calm, sleepy",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,91.3,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,X,,,Egypt,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"excited, calm, sleepy",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,90.3,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,X,,,Egypt,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"excited, calm, sleepy",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,92,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.","Tung, K.; Liu, P.-K.; Chuang, Y.-C.; Wang, S.-H.; Wu, A.-Y.","Entropy-assisted multi-modal emotion recognition framework based on physiological signals
",2019,Conference on biomedical engineering and science,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,69.2,,,,,,,,,,,,,,,,,,x,,,,,
,,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.","Tung, K.; Liu, P.-K.; Chuang, Y.-C.; Wang, S.-H.; Wu, A.-Y.","Entropy-assisted multi-modal emotion recognition framework based on physiological signals
",2019,Conference on biomedical engineering and science,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,x,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,79.6,,,,,,,,,,,,,,,,,,x,,,,,
,,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,X,,,,Iran,,,,,X,,,,,,,,,,,,,,,,,,X,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"stressful, normal, relaxing",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,81.81,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,X,,,,Iran,,,,,X,,,,,,,,,,,,,,,,,,X,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"stressful, normal, relaxing",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,71.21,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,X,,,,Iran,,,,,X,,,,,,,,,,,,,,,,,,X,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"stressful, normal, relaxing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,67.05,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,X,,,,Iran,,,,,X,,,,,,,,,,,,,,,,,,X,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"stressful, normal, relaxing",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,70.16,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,X,,,,Iran,,,,,X,,,,,,,,,,,,,,,,,,X,,,,,,-,-,-,-,-,-,,,,,,,,,,,,,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,3,"stressful, normal, relaxing",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,73.2,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.","Sun, X.; Hong, T.; Li, C.; Ren, F.","Hybrid spatiotemporal models for sentiment classification via galvanic skin response
",2019,Neurocomputing,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,100,45,20,,,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,00 m 120 s,,homemade,-,-,x,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,6,"sad, angry, happy, surprise, fear, disgust ",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,46.94,43.16,41.46,,,,,,,,,,,,,,,,,x,,,,,
,,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.","Sun, X.; Hong, T.; Li, C.; Ren, F.","Hybrid spatiotemporal models for sentiment classification via galvanic skin response
",2019,Neurocomputing,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,100,45,20,,,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,00 m 120 s,,homemade,-,-,x,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,6,"sad, angry, happy, surprise, feadr, disgust ",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,71.32,72.06,76.99,,,,,,,,,,,,,,,,,x,,,,,
,,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.","Sun, X.; Hong, T.; Li, C.; Ren, F.","Hybrid spatiotemporal models for sentiment classification via galvanic skin response
",2019,Neurocomputing,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,100,45,20,,,,- ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,00 m 120 s,,homemade,-,-,x,-,x,x,-,-,-,-,-,-,-,-,,-,categorical,6,"sad, angry, happy, surprise, feadr, disgust ",,,,,,,,,,,,,,,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,71.89,74.28,77.34,,,,,,,,,,,,,,,,,x,,,,,
,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,X,,,Switzerland,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,53.8,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,X,,,Switzerland,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,64,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,X,,,Switzerland,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,77.6,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,X,,,Switzerland,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,82.2,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,X,,,Switzerland,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,55.3,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,X,,,Switzerland,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,64.4,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,X,,,Switzerland,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,68.2,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,X,,,Switzerland,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,69.4,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,21.21,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,18.63,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,31.19,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,58.7,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,46.78,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,60.24,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,21.21,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,18.63,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,7,"joy, funny, anger, fear, disgust, sad, neutrality",,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,31.19,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, anger",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,42.45,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, anger",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,42.43,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, anger",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,63.95,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, fear",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,50.7,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, fear",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,45.37,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, fear",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,33.67,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, disgust",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,39.06,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, disgust",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,39.58,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, disgust",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,57.71,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, sad",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,45.08,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, sad",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,41.73,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"joy, neutral, sad",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,63.16,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, anger",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,48.68,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, anger",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,47.56,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, anger",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,67.69,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, fear",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,47.71,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, fear",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,47.39,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, fear",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,62.42,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, disgust",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,46.23,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, disgust",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,43.76,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, disgust",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,63.55,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, sad",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,44.06,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, sad",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,46.25,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE,,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,162,76,23.21,,18-29,,Yes,X,,X,X,,,x,x,x,x,,,,,,X,,,x,x,x,,,x,,x,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,,-,BIOPAC,-,x,-,x,x,-,-,-,-,-,x,-,-,,-,categorical,3,"funny, neutral, sad",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,"Finally, significant correlates were found between the participant ratings and EEG signals. The spearman correlation
coefficients analysis of this database demonstrated that the
energy in prefrontal part is positively correlated with the level
of positive emotion states and negative correlated with the
level of negative emotion states. The difference of coefficients distribution between positive emotions and negative
emotions was apparent to be distinguished and The coefficient distributions among negative emotions were similar,
which validated that negative emotion categories are to a
large extent overlapped.",,,61.99,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.","Thammasan, N.; Hagad, J.L.; Fukui, K.-I.; Numao, M.","Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals
",2018,"Int. Conf. Affect. Comput. Intel. Interact. Workshops Demos, ACIIW",,X,,,Japan,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,9,1,27.96,,22-32,,Yes,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,,,,,,,,,X,,,,,,,,,,,,,,,,00 m 10 s - 01m 30 s,,,-,imec,left,-,-,-,-,-,-,-,-,-,-,x,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,27.3,,,x,,,,,
,,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.","Thammasan, N.; Hagad, J.L.; Fukui, K.-I.; Numao, M.","Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals
",2018,"Int. Conf. Affect. Comput. Intel. Interact. Workshops Demos, ACIIW",,X,,,Japan,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,9,1,27.96,,22-32,,Yes,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,,,,,,,,,X,,,,,,,,,,,,,,,,00 m 10 s - 01m 30 s,,,-,imec,left,-,-,-,-,-,-,-,-,-,-,x,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,37.1,,,x,,,,,
,,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.","Pinto, G.; Carvalho, J.M.; Barros, F.; Soares, S.C.; Pinho, A.J.; Brás, S.","Multimodal emotion evaluation: A physiological model for cost-effective emotion classification
",2020,Sensors,X,,,,Portugal,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,55,37,21,,18-28,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BIOPAC MP160,left,x,-,x,x,-,-,-,-,x,-,-,-,,-,categorical,3,"neutral, fear, happy",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"This research analyzed the physiological component of emotion in different emotional conditions
(fear, happiness and neutral), using automatic systems for emotion identification. Our results suggest
that the ECG signal seems to be the most informative in emotion stratification. The use of facial EMG
in emotion is dependent on monitoring two (or more) muscles, allowing to identify facial expression
changes by corresponding muscular contractions. Nevertheless, if all signals are used on emotion
identification, a higher accuracy is achieved, since all signals are representative of different information.
This physiological model of emotions has important research and clinical implications, by providing
valuable information about the value and weight of physiological signals for emotional classification,
which can critically drive effective evaluation, monitoring, and intervention regarding emotional
processing and regulation, considering multiple contexts",,,,,58,,,,,,,,,,,,,,,,,,x,,,,,
,,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.","Pinto, G.; Carvalho, J.M.; Barros, F.; Soares, S.C.; Pinho, A.J.; Brás, S.","Multimodal emotion evaluation: A physiological model for cost-effective emotion classification
",2020,Sensors,X,,,,Portugal,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,55,37,21,,18-28,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BIOPAC MP160,left,x,-,x,x,-,-,-,-,x,-,-,-,,-,categorical,3,"neutral, fear, happy",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"This research analyzed the physiological component of emotion in different emotional conditions
(fear, happiness and neutral), using automatic systems for emotion identification. Our results suggest
that the ECG signal seems to be the most informative in emotion stratification. The use of facial EMG
in emotion is dependent on monitoring two (or more) muscles, allowing to identify facial expression
changes by corresponding muscular contractions. Nevertheless, if all signals are used on emotion
identification, a higher accuracy is achieved, since all signals are representative of different information.
This physiological model of emotions has important research and clinical implications, by providing
valuable information about the value and weight of physiological signals for emotional classification,
which can critically drive effective evaluation, monitoring, and intervention regarding emotional
processing and regulation, considering multiple contexts",,,,,62,,,,,,,,,,,,,,,,,,x,,,,,
,,"Raheel, A., Majid, M., Alnowami, M., & Anwar, S. M. (2020). Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia. Sensors, 20(14), 4037.","Raheel, A.; Majid, M.; Alnowami, M.; Anwar, S.M.","Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia
",2020,Sensors,X,,,,Pakistan,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,10,21.1,,,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,00 m 21 s - 00 m 58 s,,,-,Shimmer GSR,-,x,-,-,-,-,-,-,-,-,-,-,-,,--,categorical,4,"happy, relaxed, angry, sad",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"While our study shows that with TEM, the emotion
recognition accuracy increases, which could mean that the users were able to better feel the emotions
as the video content intended to deliver. The use of physiological sensors also ensures that the true
sensation of emotion is detected which is subjectively independent of users.",,,72.61,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.","Liu, Y.; Jiang, C.","Recognition of Shooter's Emotions under Stress Based on Affective Computing
",2019,IEEE,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,10,4,,,19-25,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BioNeuro multichannel biofeedback instrument,-,x,-,x,x,x,-,-,-,-,-,-,-,,-,categorical,4,"happiness, sadness, anger, fear",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,65.38,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.","Liu, Y.; Jiang, C.","Recognition of Shooter's Emotions under Stress Based on Affective Computing
",2019,IEEE,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,10,4,,,19-25,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BioNeuro multichannel biofeedback instrument,-,x,-,x,x,x,-,-,-,-,-,-,-,,-,categorical,4,"happiness, sadness, anger, fear",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,87.5,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.","Liu, Y.; Jiang, C.","Recognition of Shooter's Emotions under Stress Based on Affective Computing
",2019,IEEE,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,10,4,,,19-25,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BioNeuro multichannel biofeedback instrument,-,x,-,x,x,x,-,-,-,-,-,-,-,,-,categorical,4,"happiness, sadness, anger, fear",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,78.22,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.","Liu, Y.; Jiang, C.","Recognition of Shooter's Emotions under Stress Based on Affective Computing
",2019,IEEE,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,10,4,,,19-25,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BioNeuro multichannel biofeedback instrument,-,x,-,x,x,x,-,-,-,-,-,-,-,,-,categorical,4,"happiness, sadness, anger, fear",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,89,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).","Zhang, K.; Zhang, H.; Li, S.; Yang, C.; Sun, L.","The PMEmo dataset for music emotion recognition
",2018,ICMR - Proc. ACM Int. Conf. Multimed. Retr.,,X,,,China,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,457,236,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,,,,,,,,x,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,13.9,6.3,,,,,x,,,,,
,,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).","Zhang, K.; Zhang, H.; Li, S.; Yang, C.; Sun, L.","The PMEmo dataset for music emotion recognition
",2018,ICMR - Proc. ACM Int. Conf. Multimed. Retr.,,X,,,China,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,457,236,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,x,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,14.1,1.7,,,,,x,,,,,
,,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).","Zhang, K.; Zhang, H.; Li, S.; Yang, C.; Sun, L.","The PMEmo dataset for music emotion recognition
",2018,ICMR - Proc. ACM Int. Conf. Multimed. Retr.,,X,,,China,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,457,236,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,,,,,,,,x,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,18.6,1.1,,,,,x,,,,,
,,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).","Zhang, K.; Zhang, H.; Li, S.; Yang, C.; Sun, L.","The PMEmo dataset for music emotion recognition
",2018,ICMR - Proc. ACM Int. Conf. Multimed. Retr.,,X,,,China,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,457,236,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,BIOPAC,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,x,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,19.4,4,,,,,x,,,,,
,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,X,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,12,,,20-26,,Yes,X,X,X,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BIOPAC’s MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,-,-,-,-,-,,,62,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,X,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,12,,,20-26,,Yes,X,X,X,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BIOPAC’s MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,-,-,-,-,-,,,77,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,X,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,12,,,20-26,,Yes,X,X,X,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BIOPAC’s MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,-,-,-,-,-,,,71,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,X,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,12,,,20-26,,Yes,X,X,X,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,BIOPAC’s MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,78,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,X,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,12,,,20-26,,Yes,X,X,X,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,-,BIOPAC’s MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,-,-,-,-,-,,,51,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,X,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,12,,,20-26,,Yes,X,X,X,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,-,BIOPAC’s MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,-,-,-,-,-,,,65,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,X,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,12,,,20-26,,Yes,X,X,X,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,-,BIOPAC’s MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,-,-,-,-,-,,,51,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,X,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,12,,,20-26,,Yes,X,X,X,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,-,BIOPAC’s MP150,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,4,"HAHV, HALV, LAHV, LALV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,62,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.","Santamaria-Granados, L.; Munoz-Organero, M.; Ramirez-Gonzalez, G.; Abdulhay, E.; Arunkumar, N.","Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)
",2019,IEEE,x,,,,Colombia,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,x,x,x,x,x,x,,,,x,x,,x,x,x,x,x,,,,x,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Physiological datasets with a large number of instances are
optimal for the proposed experiments since these directly
influence the emotion prediction, a the greater the number
of instances, the more effective the model. Consequently,
several annotations of arousal and valence must be recorded,
since, when subjecting a participant to the stimulus of a short
video, it can manifest different levels of emotion during of
experiment.",,,69,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.","Santamaria-Granados, L.; Munoz-Organero, M.; Ramirez-Gonzalez, G.; Abdulhay, E.; Arunkumar, N.","Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)
",2019,IEEE,,,,,Colombia,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,,,,,,,,x,x,x,x,x,x,,,,x,x,,x,x,x,x,x,,,,x,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Physiological datasets with a large number of instances are
optimal for the proposed experiments since these directly
influence the emotion prediction, a the greater the number
of instances, the more effective the model. Consequently,
several annotations of arousal and valence must be recorded,
since, when subjecting a participant to the stimulus of a short
video, it can manifest different levels of emotion during of
experiment.",,,67,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,,,,,,,x,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,3.7,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,x,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,2.7,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,x,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,2.6,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,,,x,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,2.6,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,,,,,,,,,,,x,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,3.5,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,valence,,,,,,,x,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,1.9,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,,,,,,,x,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,4.7,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,x,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,2.3,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,x,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,4.3,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,,,x,,,,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,4.3,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,,,,,,,,,,,x,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,5.5,,,,,,x,,,,,
,,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,X,,,,China,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,21,,,,20-36,,x,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,arousal,,,,,,,x,,,,,,,,,,,,-,-,-,-,-,,,,,,,,,,,,,,,,,2.7,,,,,,x,,,,,
,,"Liapis, A., Katsanos, C., Karousos, N., Xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).","Liapis, A.; Katsanos, C.; Karousos, N.; Xenos, M.; Orphanoudakis, T.","UDSP+: Stress detection based on user-reported emotional ratings and wearable skin conductance sensor
",2019,UbiComp/ISWC - Adjun. Proc. ACM Int. Jt. Conf. Pervasive Ubiquitous Comput. Proc. ACM Int. Symp. Wearable Comput.,,X,,,Greece,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,24,10,32.3,-,18-45,,x,,,,,x,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,,,,,,,,,-,NeXus-10,not dominant,x,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"stress, not stress",,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,86.4,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Liapis, A., Katsanos, C., Karousos, N., Xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).","Liapis, A.; Katsanos, C.; Karousos, N.; Xenos, M.; Orphanoudakis, T.","UDSP+: Stress detection based on user-reported emotional ratings and wearable skin conductance sensor
",2019,UbiComp/ISWC - Adjun. Proc. ACM Int. Jt. Conf. Pervasive Ubiquitous Comput. Proc. ACM Int. Symp. Wearable Comput.,,X,,,Greece,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,24,10,32.3,,18-45,,x,,,,,x,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,,,,,,,,,-,NeXus-10,not dominant,x,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"stress, not stress",,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,85.2,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,X,,,China,,,,,X,,,,,,,,,,,,,,,,,X,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,32-245 s,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"amusement, sadness, anger, disgust, fear",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"The proposed framework has enhanced the performance of
emotion recognition. The reason might be as follows. First, as
human being has the multivariate characteristics, it is difficult
to accurately reflect the emotional changes by means of
specific peripheral physiological signal. ECG, EMG and SCL
might complement each other to reflect the emotional changes
well. Second, the most emotion-related physiological features
might be discovered by using feature selection. Furthermore,
the adverse interference between different physiological signals could be totally avoided with decision fusion. As for
future work, the following attempts are deserved. We will try
to extract the unseen features with deep neural networks to
form multi-level feature set in order to get rid of the problem
of weak generalization ability caused by using features in lowdimension space. On the other hand, other dataset could be
considered, such as DEAP and MAHNOB to evaluate and
optimize our framework. Last but not least, as feature selection
methods greatly affect the recognition results, we will try
more feature selection methods. And further experiments on
classifying extensive emotion states would be conducted in the
next stage.",,,70.1,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,X,,,China,,,,,X,,,,,,,,,,,,,,,,,X,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"amusement, sadness, anger, disgust, fear",,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"The proposed framework has enhanced the performance of
emotion recognition. The reason might be as follows. First, as
human being has the multivariate characteristics, it is difficult
to accurately reflect the emotional changes by means of
specific peripheral physiological signal. ECG, EMG and SCL
might complement each other to reflect the emotional changes
well. Second, the most emotion-related physiological features
might be discovered by using feature selection. Furthermore,
the adverse interference between different physiological signals could be totally avoided with decision fusion. As for
future work, the following attempts are deserved. We will try
to extract the unseen features with deep neural networks to
form multi-level feature set in order to get rid of the problem
of weak generalization ability caused by using features in lowdimension space. On the other hand, other dataset could be
considered, such as DEAP and MAHNOB to evaluate and
optimize our framework. Last but not least, as feature selection
methods greatly affect the recognition results, we will try
more feature selection methods. And further experiments on
classifying extensive emotion states would be conducted in the
next stage.",,,65.83,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,X,,,China,,,,,X,,,,,,,,,,,,,,,,,X,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"amusement, sadness, anger, disgust, fear",,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,,,,,,,,,,-,-,-,x,"The proposed framework has enhanced the performance of
emotion recognition. The reason might be as follows. First, as
human being has the multivariate characteristics, it is difficult
to accurately reflect the emotional changes by means of
specific peripheral physiological signal. ECG, EMG and SCL
might complement each other to reflect the emotional changes
well. Second, the most emotion-related physiological features
might be discovered by using feature selection. Furthermore,
the adverse interference between different physiological signals could be totally avoided with decision fusion. As for
future work, the following attempts are deserved. We will try
to extract the unseen features with deep neural networks to
form multi-level feature set in order to get rid of the problem
of weak generalization ability caused by using features in lowdimension space. On the other hand, other dataset could be
considered, such as DEAP and MAHNOB to evaluate and
optimize our framework. Last but not least, as feature selection
methods greatly affect the recognition results, we will try
more feature selection methods. And further experiments on
classifying extensive emotion states would be conducted in the
next stage.",,,71.2,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,X,,,China,,,,,X,,,,,,,,,,,,,,,,,X,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"amusement, sadness, anger, disgust, fear",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,,,,,,,,,,,-,-,-,x,"The proposed framework has enhanced the performance of
emotion recognition. The reason might be as follows. First, as
human being has the multivariate characteristics, it is difficult
to accurately reflect the emotional changes by means of
specific peripheral physiological signal. ECG, EMG and SCL
might complement each other to reflect the emotional changes
well. Second, the most emotion-related physiological features
might be discovered by using feature selection. Furthermore,
the adverse interference between different physiological signals could be totally avoided with decision fusion. As for
future work, the following attempts are deserved. We will try
to extract the unseen features with deep neural networks to
form multi-level feature set in order to get rid of the problem
of weak generalization ability caused by using features in lowdimension space. On the other hand, other dataset could be
considered, such as DEAP and MAHNOB to evaluate and
optimize our framework. Last but not least, as feature selection
methods greatly affect the recognition results, we will try
more feature selection methods. And further experiments on
classifying extensive emotion states would be conducted in the
next stage.",,,63.19,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,X,,,China,,,,,X,,,,,,,,,,,,,,,,,X,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"amusement, sadness, anger, disgust, fear",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"The proposed framework has enhanced the performance of
emotion recognition. The reason might be as follows. First, as
human being has the multivariate characteristics, it is difficult
to accurately reflect the emotional changes by means of
specific peripheral physiological signal. ECG, EMG and SCL
might complement each other to reflect the emotional changes
well. Second, the most emotion-related physiological features
might be discovered by using feature selection. Furthermore,
the adverse interference between different physiological signals could be totally avoided with decision fusion. As for
future work, the following attempts are deserved. We will try
to extract the unseen features with deep neural networks to
form multi-level feature set in order to get rid of the problem
of weak generalization ability caused by using features in lowdimension space. On the other hand, other dataset could be
considered, such as DEAP and MAHNOB to evaluate and
optimize our framework. Last but not least, as feature selection
methods greatly affect the recognition results, we will try
more feature selection methods. And further experiments on
classifying extensive emotion states would be conducted in the
next stage.",,,63.26,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,X,,,China,,,,,X,,,,,,,,,,,,,,,,,X,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,categorical,5,"amusement, sadness, anger, disgust, fear",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"The proposed framework has enhanced the performance of
emotion recognition. The reason might be as follows. First, as
human being has the multivariate characteristics, it is difficult
to accurately reflect the emotional changes by means of
specific peripheral physiological signal. ECG, EMG and SCL
might complement each other to reflect the emotional changes
well. Second, the most emotion-related physiological features
might be discovered by using feature selection. Furthermore,
the adverse interference between different physiological signals could be totally avoided with decision fusion. As for
future work, the following attempts are deserved. We will try
to extract the unseen features with deep neural networks to
form multi-level feature set in order to get rid of the problem
of weak generalization ability caused by using features in lowdimension space. On the other hand, other dataset could be
considered, such as DEAP and MAHNOB to evaluate and
optimize our framework. Last but not least, as feature selection
methods greatly affect the recognition results, we will try
more feature selection methods. And further experiments on
classifying extensive emotion states would be conducted in the
next stage.",,,51.13,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press."," N, Ganapathy; R, Swaminathan;","Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network
",2020,Studies in health technology and informatics,X,,,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,72.08,74.7,,83.23,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press."," N, Ganapathy; R, Swaminathan;","Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network
",2020,Studies in health technology and informatics,X,,,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,68.9,50.96,,61.26,,,,,,,,,,,,,,,,,x,,,,,
,,"Yasemin, M., Sarıkaya, M. A., & Ince, G. (2019, July). Emotional state estimation using sensor fusion of EEG and EDA. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 5609-5612). IEEE.","Mine Yasemin, Mehmet Ali Sarikaya, Gokhan Ince","Emotional State Estimation using Sensor Fusion of EEG and EDA.",2019,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,,X,,,Turkey,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,10,5,,,21-28,,Yes,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,"we conducted the experiments with
audiovisual stimuli to induce emotions by playing videos.
We have used a video prepared in [21], which contains
several kinds of movie clips and binaural beats 1. As stimuli,
we chose movies which have high effect to arouse human
emotions. A binaural beat is an auditory sensation, which
appears when two slightly different sounds are received to",-,-,-,-,-,Q Sensor by Afectiva,-,-,-,-,-,-,-,-,-,-,-,x,-,,-,categorical,3,"funny, horror, weepy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,73.8,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Ghiasi, S., Greco, A., Barbieri, R., Scilingo, E. P., & Valenza, G. (2020). Assessing autonomic function from electrodermal activity and heart rate variability during cold-pressor test and emotional challenge. Scientific reports, 10(1), 1-13.","Ghiasi, S.; Greco, A.; Barbieri, R.; Scilingo, E.P.; Valenza, G.","Assessing Autonomic Function from Electrodermal Activity and Heart Rate Variability During Cold-Pressor Test and Emotional Challenge
",2020,Scientific Reports,X,,,,Italy,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,26,8,26,,21-38,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,1M 30S,-,-,-,-,BIOPAC MP35,dominant,x,-,-,x,x,-,-,-,-,x,-,-,,-,dimensional,2,"pleasent, unpleasent",,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x," relevant information on the ANS activity can be retrieved from the superimposed phasic behavior of
HRV time-varying bispectral measures. Furthermore, the proposed new indices characterizing the cardiovascular
control through EDA and HRV seem to provide a more effective indicator of the sympathovagal balance then
traditional indices from HRV series onl",,,68.48,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).","Gümüslü, E.; Erol Barkana, D.; Köse, H.","Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems",2020,International Conference on Multimodal Interaction,,X,,,Turkey,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,15,10,21.4,,20-23,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,IAPS,X,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,-,-,0M 06S,-,-,-,-,sensors produced by Thought Technology,-,x,-,x,-,x,-,-,-,x,-,-,-,,-,dimensional,2,"pleasant, unpleasant",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,57.41,,57,,,,,,,,,,,,,,,,,,x,,,,,
,,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).","Gümüslü, E.; Erol Barkana, D.; Köse, H.","Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems",2020,International Conference on Multimodal Interaction,,X,,,Turkey,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,15,10,21.4,,20-23,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,IAPS,X,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,-,-,0M 06S,-,-,-,-,sensors produced by Thought Technology,-,x,-,x,-,x,-,-,-,x,-,-,-,,-,dimensional,2,"pleasant, neutral",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,56.37,,56,,,,,,,,,,,,,,,,,,x,,,,,
,,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).","Gümüslü, E.; Erol Barkana, D.; Köse, H.","Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems",2020,International Conference on Multimodal Interaction,,X,,,Turkey,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,15,10,21.4,,20-23,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,IAPS,X,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,-,-,0M 06S,-,-,-,-,sensors produced by Thought Technology,-,x,-,x,-,x,-,-,-,x,-,-,-,,-,dimensional,2,"neutral, unpleasant",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,56.55,,57,,,,,,,,,,,,,,,,,,x,,,,,
,,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).","Gümüslü, E.; Erol Barkana, D.; Köse, H.","Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems",2020,International Conference on Multimodal Interaction,,X,,,Turkey,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,15,10,21.4,,20-23,,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,IAPS,X,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,-,-,0M 06S,-,-,-,-,sensors produced by Thought Technology,-,x,-,x,-,x,-,-,-,x,-,-,-,,-,dimensional,3,"pleasant, neutral, unpleasant",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,38.32,,5,,,,,,,,,,,,,,,,,,x,,,,,
,,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).","Katada, S.; Okada, S.; Hirano, Y.; Komatani, K.","Is She Truly Enjoying the Conversation?: Analysis of Physiological Signals toward Adaptive Dialogue Systems",2020,International Conference on Multimodal Interaction,,X,,,Japón,,,,,X,,,,,,,,,,,,,,,,X,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dimensional,2,"high enjoy, low enjoy",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,,,61.6,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).","Katada, S.; Okada, S.; Hirano, Y.; Komatani, K.","Is She Truly Enjoying the Conversation?: Analysis of Physiological Signals toward Adaptive Dialogue Systems",2020,International Conference on Multimodal Interaction,,X,,,Japón,,,,,X,,,,,,,,,,,,,,,,X,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dimensional,2,"high enjoy, low enjoy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,,,-,-,-,-,,,62.2,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,X,,,Taiwan,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,-,-,-,-,,-,X,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,Shimmer3,,x,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,59.71,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,X,,,Taiwan,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,-,-,-,-,,-,X,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,59.77,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,X,,,Taiwan,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,-,-,-,-,,-,X,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,,x,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,60.16,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,X,,,Taiwan,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,-,-,-,-,,-,X,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,59.78,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,X,,,Taiwan,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,-,-,-,-,,-,X,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,60,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,X,,,Taiwan,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,-,-,-,-,,-,X,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,,x,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,60.47,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Rahman, J. S., Hossain, M. Z., & Gedeon, T. (2019, December). Measuring Observers' EDA Responses to Emotional Videos. In Proceedings of the 31st Australian Conference on Human-Computer-Interaction (pp. 457-461).","Rahman, J.S.; Zakir Hossain, M.; Gedeon, T.","Measuring observers' eda responses to emotional videos
",2019,Australian Conference on Human-Computer-Interaction,,X,,,Australia,,,,,,,,,,,,,,,,,,,,X,,,,,,,,,20,14,23,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 2S - 0M3S,-,-,-,Empatica E4,-,-,-,-,-,-,-,-,-,-,-,x,-,,-,categorical,7,"surprise, sad, neutral, happy, fear, disgust, anger",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"the initial analysis also shows some noticeable difference
of our data driven arousal model from our observers’ perspective, when compared to the (abstract) standard models
in the literature. The data-derived model with neutral as the
baseline is quite similar to the standard abstract model, with
the only changes being Happy and Sad changing sides as
Low/High arousal. Further analysis will be conducted and
evaluated to identify the reasons. Questions to be answered
are whether the dataset was biased, whether our 20 participants were somehow different from the expected population
reaction, or whether the abstract model is just incorrect. It
is also important to point out that EDA activity can vary
according to the difference in stimuli types, participants’ age,
gender etc [6]. Also the number of samples might be considered small, although experiments have shown that it is
reasonable [9]. Arguably, it makes more sense to use the overall average reaction to be the baseline between high and low
arousal, which spreads the emotional reactions over a wider
range. This differs more from the standard model.",,,94.8,75.3,84.2,95.8,94.7,,,,,,,,,,,,,,95.2,,x,,,,,
,,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.","Rahim, A.; Sagheer, A.; Nadeem, K.; Dar, M.N.; Rahim, A.; Akram, U.","Emotion Charting Using Real-time Monitoring of Physiological Signals
",2020,International Conference on Robotics and Automation in Industry,,X,,,Pakistan,,,,,x,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,"Shimmer3
GSR+",left,x,-,x,x,-,-,-,x,-,-,-,-,,-,categorical,7,"anger, disgust, fear, happy, neutral, sad, surprise",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,92.7,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.","Rahim, A.; Sagheer, A.; Nadeem, K.; Dar, M.N.; Rahim, A.; Akram, U.","Emotion Charting Using Real-time Monitoring of Physiological Signals
",2020,International Conference on Robotics and Automation in Industry,,X,,,Pakistan,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,"Shimmer3
GSR+",left,x,-,x,x,-,-,-,x,-,-,-,-,,-,categorical,7,"anger, disgust, fear, happy, neutral, sad, surprise",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,68,,,,,,,,,,,,,,,,,,,,x,,,,,
,,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.","Yin, G.; Sun, S.; Zhang, H.; Yu, D.; Li, C.; Zhang, K.; Zou, N.","User Independent Emotion Recognition with Residual Signal-Image Network",2020,IEEE International Conference on Image Processing,,X,,,China,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,-,-,-,-,,-,X,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,55.92,,58.83,,,,,,,,,,,,,,,,,,x,,,,,
,,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.","Yin, G.; Sun, S.; Zhang, H.; Yu, D.; Li, C.; Zhang, K.; Zou, N.","User Independent Emotion Recognition with Residual Signal-Image Network",2020,IEEE International Conference on Image Processing,,X,,,China,,,,,X,,,,,,,,,,,,,,X,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,X,-,-,-,-,-,-,-,,-,X,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,57.24,,60.12,,,,,,,,,,,,,,,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,56.6,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,55.5,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,58.7,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,56,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,62.8,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,54.4,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,53.5,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,55,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,54,,,,x,,,,,
,,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,X,,,Taiwan,,,,,X,,X,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,X,-,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 250S - 14M 0S,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"Second, our error analysis suggests that
the types of stimuli could also be a key component in affecting
the physiological responses and potentially inducing the bias
in the self emotion assessment. Through better understanding
the relationship of these multiple perspectives of emotion
annotations and the measured physiological responses could
help enhance the robustness of affective recognition module
that can be integrated for many human behavior modeling
applications ",,,,,,,,,,,,,,,,,,,56.7,,,,x,,,,,
,,"Kołodziej, M., Tarnowski, P., Majkowski, A., & Rak, R. J. (2019). Electrodermal activity measurements for detection of emotional arousal. Bulletin of the Polish Academy of Sciences. Technical Sciences, 67(4).","Kołodziej, M.; Tarnowski, P.; Majkowski, A.; Rak, R.J.","Electrodermal activity measurements for detection of emotional arousal
",2020,Bulletin of the Polish Academy of Sciences: Technical Sciences,X,,,,Poland,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,22,,20,,,Poland,Yes,,,,,,,X,X,X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 4.9S - 0M 71.1S,-,-,homemade,-,left,x,-,x,x,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,x,"The best features that were repeated
in the selection results were: MaxAmpPeak, VarAmpPeak,
StdAmpPeak, MaxAbsAmpPeak, VarSC, StdSC, ActivitySC,
MaxDeltaForward, MaxDeltaBack, KurtosisAmpPeak,
SkewnessAmpPeak. These features are related to the maximum
values, energy or statistical properties of the phasic component.
The results indicate that such features should be used in the
analysis of the EDA for the level of arousal recognition. Of
great importance is the quality of the recorded SC signals and
the pre-processing methods. In conjunction with the features
of other physiological signals (such as ECG, EEG, and EMG),
the proposed analysis can produce better results.",,,79,78.38,77.85,77.33,77.78,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.","Ganapathy, N.; Swaminathan, R.","Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network",2020,Studies in health technology and informatics,X,,,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,65.63,65.76,72.63,81.11,,,,,,,,,,,,,,,,,x,,,,,
,,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.","Ganapathy, N.; Swaminathan, R.","Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network",2020,Studies in health technology and informatics,X,,,,India,,,,,X,X,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,Yes,,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,68.75,45.41,50,55.6,,,,,,,,,,,,,,,,,x,,,,,
,,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.","Subramanian, R.; Wache, J.; Abadi, M.K.; Vieriu, R.L.; Winkler, S.; Sebe, N.","Ascertain: Emotion and personality recognition using commercial sensors
",2018,IEEE Transactions on Affective Computing,X,,,,India,,,,,X,,,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 52S - 0M 127 S,-,-,-,-,left,-,-,x,x,-,-,-,-,x,-,-,-,,-,dimensional,2,"HV, LV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,64,,,,,,,,,,,,,,,,,,x,,,,,
,,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.","Subramanian, R.; Wache, J.; Abadi, M.K.; Vieriu, R.L.; Winkler, S.; Sebe, N.","Ascertain: Emotion and personality recognition using commercial sensors
",2018,IEEE Transactions on Affective Computing,X,,,,India,,,,,X,,,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 52S - 0M 127 S,-,-,-,-,left,-,-,x,x,-,-,-,-,x,-,-,-,,-,dimensional,2,"HV, LV",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,68,,,,,,,,,,,,,,,,,,x,,,,,
,,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.","Subramanian, R.; Wache, J.; Abadi, M.K.; Vieriu, R.L.; Winkler, S.; Sebe, N.","Ascertain: Emotion and personality recognition using commercial sensors
",2018,IEEE Transactions on Affective Computing,X,,,,India,,,,,X,,,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 52S - 0M 127 S,-,-,-,-,left,-,-,x,x,-,-,-,-,x,-,-,-,,-,dimensional,2,"HA, LA",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,61,,,,,,,,,,,,,,,,,,x,,,,,
,,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.","Subramanian, R.; Wache, J.; Abadi, M.K.; Vieriu, R.L.; Winkler, S.; Sebe, N.","Ascertain: Emotion and personality recognition using commercial sensors
",2018,IEEE Transactions on Affective Computing,X,,,,India,,,,,X,,,,,X,,,,,,,,,,,,,,,,,,,-,-,-,-,-,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,X,X,X,-,-,-,X,-,-,X,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,-,0M 52S - 0M 127 S,-,-,-,-,left,-,-,x,x,-,-,-,-,x,-,-,-,,-,dimensional,2,"HA, LA",,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,-,-,-,,,,,66,,,,,,,,,,,,,,,,,,x,,,,,
,,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).","Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. ",In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures,2019,CSEDU,x,,,,Germany,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,27,,,,,,Yes,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,IAPS,x,,,,,,,,,,,,,,,,,,,00 m 06 ss,,,,,"(BITalino (r)evolution Plugged
Kit BT",,,,,,,,,,,,,,,,dimensional,4,"HAHV, HALV, LAHV, LALV",,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,"por las dudas comento que este es el paper horrible que no se entendian las medidas de performance, por eso no se anotaron",,
,,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).","Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. ",In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures,2019,CSEDU,x,,,,Germany,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,27,,,,,,Yes,x,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x,,,,,,IAPS,x,,,,,,,,,,,,,,,,,,,00 m 06 ss,,,,,"(BITalino (r)evolution Plugged
Kit BT",,,,,,,,,,,,,,,,dimensional,4,"HAHV, HALV, LAHV, LALV",,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,,