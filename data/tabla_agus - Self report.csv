paper_id,model_id,apa_citation,use_questionnaite,affective_questionaire_SAM,affective_questionaire_PSS,affective_questionaire_PANAS,affective_questionaire_DES,affective_questionaire_affective_grid,is_dimensional,dimension_valence,dimension_arousal,dimension_dominance,dimension_like_or_dislike,dimension_familiarity,dimension_stress,dimension_engagement,is_categorial,Anger,Stress,Disgust,Fear,Sadness,Surprise,Happiness,Pleasant,Unpleasant,Anxiety,Neutral,Funny,Horror,Weepy,Boredom,Relaxation,Amusement,Confusion,Curiosity,Delight,flow/engagement,Frustration,Tenderness,Joy
1,1,"Zangróniz, R., Martínez-Rodrigo, A., Pastor, J. M., López, M. T., & Fernández-Caballero, A. (2017). Electrodermal Activity Sensor for Classification of Calm/Distress Condition. Sensors (Basel, Switzerland), 17(10), E2324. https://doi.org/10.3390/s17102324
",Relies on other's questionnaire,x,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
2,2,"Liu, M., Fan, D., Zhang, x., & Gong, x. (2017). Human Emotion Recognition Based on Galvanic Skin Response Signal Feature Selection and SVM. 157–160. Scopus. https://doi.org/10.1109/ICSCSE.2016.0051
",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
3,3,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
3,4,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
4,5,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061905",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
4,6,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061906",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
4,7,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061907",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
4,8,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061908",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
4,9,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061909",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
5,10,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384",Relies on other's questionnaire,x,-,,,,x,x,x,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
5,11,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384",Relies on other's questionnaire,x,-,,,,x,x,x,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
6,12,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
6,13,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",No,-,-,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
6,14,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",No,-,-,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
6,15,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",No,-,-,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
6,16,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",No,-,-,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
6,17,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",No,-,-,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
6,18,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",No,-,-,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
6,19,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",No,-,-,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,20,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,21,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,22,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,23,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,24,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,25,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,26,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,27,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,28,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,29,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,30,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,31,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,32,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,33,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,34,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,35,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,36,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,37,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,38,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
7,39,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
8,40,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
8,41,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
8,42,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
9,43,"Amalan, S., Shyam, A., Anusha, A. S., Preejith, S. P., Tony, A., Jayaraj, J., & Mohanasankar, S. (2018). Electrodermal Activity based Classification of Induced Stress in a Controlled Setting. MeMeA 2018 - 2018 IEEE International Symposium on Medical Measurements and Applications, Proceedings. Scopus. https://doi.org/10.1109/MeMeA.2018.8438703",No,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
10,44,"Machot, F. A., Ali, M., Ranasinghe, S., Mosa, A. H., & Kyandoghere, K. (2018). Improving subject-independent human emotion recognition using electrodermal activity sensors for active and assisted living. 222–228. Scopus. https://doi.org/10.1145/3197768.3201523",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
11,45,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589",Relies on other's questionnaire,x,-,,,,x,x,x,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
11,46,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589",Relies on other's questionnaire,x,-,,,,x,x,x,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
12,47,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
12,48,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
12,49,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
12,50,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
13,51,"Setyohadi, D. B., Kusrohmaniah, S., Gunawan, S. B., Pranowo, & Prabuwono, A. S. (2018). Galvanic skin response data classification for emotion detection. International Journal of Electrical and Computer Engineering, 8(5), 4004–4014. Scopus. https://doi.org/10.11591/ijece.v8i5.pp4004-4014",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,52,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,53,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,54,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,55,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,56,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,57,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,58,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,59,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,60,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,61,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,62,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,63,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,64,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,65,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
14,66,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,-,-,,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,67,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,68,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,69,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,70,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,71,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,72,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,73,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,74,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,75,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,76,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,77,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,78,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,79,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,80,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,81,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,82,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,83,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,84,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,85,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,86,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,87,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,88,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,89,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,90,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,91,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
15,92,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
16,93,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
16,94,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
17,95,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,96,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,97,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,98,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,99,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,100,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,101,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,102,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,103,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,104,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,105,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,106,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,107,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,108,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,109,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,110,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,111,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,112,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,113,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,114,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,115,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,116,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,117,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,118,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,119,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,120,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,121,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,122,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,123,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,124,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,125,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
17,126,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
18,127,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533",Yes,x,-,x,-,-,x,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18,128,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533",Yes,x,-,x,-,-,x,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19,129,"Hernández-García, A., Fernández-Martínez, F., & Díaz-De-maría, F. (2017). Emotion and attention: Predicting electrodermal activity through video visual descriptors. 914–923. Scopus. https://doi.org/10.1145/3106426.3109418",No,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
20,130,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
20,131,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
21,132,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,133,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,134,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,135,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,136,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,137,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,138,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,139,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,140,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,141,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,142,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,143,"xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,144,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,145,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,146,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23,147,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
23,148,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
24,149,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677",Relies on  other's questionnaire,-,-,-,,,x,x,x,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
24,150,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677",Relies on  other's questionnaire,-,-,-,,,x,x,x,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
25,153,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
25,151,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25,152,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26,154,"Zhang, Q., Lai, x., & Liu, G. (2016). Emotion recognition of GSR based on an improved quantum neural network. 1, 488–492. Scopus. https://doi.org/10.1109/IHMSC.2016.66",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
27,155,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040x",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
27,156,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040x",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
27,157,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040x",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
28,158,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,159,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,160,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,161,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,162,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,163,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,164,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,165,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,166,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29,167,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",Yes,-,-,-,,,x,x,x,-,x,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
29,168,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",Yes,-,-,-,,,x,x,x,-,x,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
29,169,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",Yes,-,-,-,,,x,x,x,-,x,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
29,170,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",Yes,-,-,-,,,x,x,x,-,x,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
29,171,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",Yes,-,-,-,,,x,x,x,-,x,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
29,172,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",Yes,-,-,-,,,x,x,x,-,x,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
30,173,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",-,-,-,-,,,-,-,-,,-,-,,,-,-,,-,-,-,,-,,,,--,,,,,,,,,,,,,
30,174,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",-,-,-,-,,,-,-,-,,-,-,,,-,-,,-,-,-,,-,,,,--,,,,,,,,,,,,,
30,175,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",-,-,-,-,,,-,-,-,,-,-,,,-,-,,-,-,-,,-,,,,--,,,,,,,,,,,,,
31,176,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
31,177,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
31,178,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
32,179,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
32,180,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
32,181,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
32,182,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
32,183,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
32,184,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
33,185,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
33,186,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
33,187,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
33,188,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
33,189,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
33,190,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
34,191,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516647",-,-,-,-,,,,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
34,192,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516648",-,-,-,-,,,,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
34,193,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516649",-,-,-,-,,,,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
34,194,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516649",-,-,-,-,,,,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
35,195,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
35,196,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
35,197,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
35,198,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
36,199,"Zhang, S., Liu, G., & Lai, x. (2015). Classification of evoked emotions using an artificial neural network based on single, short-term physiological signals. Journal of Advanced Computational Intelligence and Intelligent Informatics, 19(1), 118-126.",Yes,-,-,-,,,x,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
37,200,"Gjoreski, M., Lustrek, M., & Gams, M. (2018). Multi-task ensemble learning for affect recognition. 553–558. Scopus. https://doi.org/10.1145/3267305.3267308
",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
38,201,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
38,202,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
38,203,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
38,204,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
38,205,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
38,206,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
38,207,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
38,236,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
38,237,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
38,238,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
38,239,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
38,240,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
38,241,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
38,242,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
38,229,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
38,230,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
38,231,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
38,232,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
38,233,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
38,234,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
38,235,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
38,208,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
38,209,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
38,210,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
38,211,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
38,212,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
38,213,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
38,214,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
38,215,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,216,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,217,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,218,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,219,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,220,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,221,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,222,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,223,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,224,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,225,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,226,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,227,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
38,228,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
39,243,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
39,244,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
39,245,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
39,246,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
39,247,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
39,248,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
39,249,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
39,250,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
40,251,"Martínez-Rodrigo, A., Zangróniz, R., Pastor, J. M., & Sokolova, M. V. (2017). Arousal level classification of the aging adult from electro-dermal activity: From hardware development to software architecture. Pervasive and Mobile Computing, 34, 46–59. Scopus. https://doi.org/10.1016/j.pmcj.2016.04.006",Relies on other's questionnaire,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
41,252,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
41,253,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
41,254,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
41,255,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
42,256,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",Yes,x,-,x,-,-,x,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42,257,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",Yes,x,-,x,-,-,x,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42,258,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",Yes,x,-,x,-,-,x,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42,259,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",Yes,x,-,x,-,-,x,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43,260,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
43,261,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011",-,-,-,-,,,-,-,-,-,-,-,-,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
44,262,"Barral, O., Kosunen, I., & Jacucci, G. (2017). No need to laugh out loud: Predicting humor appraisal of comic strips based on physiological signals in a realistic environment. ACM Transactions on Computer-Human Interaction, 24(6). Scopus. https://doi.org/10.1145/3157730",Yes,-,,-,,,-,-,-,-,-,-,-,,x,-,,-,-,-,,-,,,,x,x,,,,,,,,,,,,
45,263,"Zhang, Q., Lai, x., & Liu, G. (2016). Emotion recognition of GSR based on an improved quantum neural network. 1, 488-492. Scopus. https://doi.org/10.1109/IHMSC.2016.66
",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46,264,"Lanatà, A., Valenza, G., & Scilingo, E. P. (2012). A novel EDA glove based on textile-integrated electrodes for affective computing. Medical & Biological Engineering & Computing, 50(11), 1163–1172. doi:10.1007/s11517-012-0921-9",Relies on other´s questionaire,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
47,265,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
47,266,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
47,267,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
47,268,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
47,269,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
48,270,"GOUIZI, K., BEREKSI REGUIG, F., & MAAOUI, C. (2011). Emotion recognition from physiological signals. Journal of Medical Engineering & Technology, 35(6-7), 300–307. doi:10.3109/03091902.2011.601784",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
49,271,"Bornoiu, I.-V., Strungaru, R., & Grigore, O. (2015). Intelligent System for Emotion Recognition Based on Electrodermal Activity Processing. 6th European Conference of the International Federation for Medical and Biological Engineering, 70–73. doi:10.1007/978-3-319-11128-5_18 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
50,272,"Liapis, A., Katsanos, C., Sotiropoulos, D., xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",Yes,-,,,,x,x,x,x,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
50,273,"Liapis, A., Katsanos, C., Sotiropoulos, D., xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",Yes,-,,,,x,x,x,x,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
50,274,"Liapis, A., Katsanos, C., Sotiropoulos, D., xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",Yes,-,,,,x,x,x,x,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
50,275,"Liapis, A., Katsanos, C., Sotiropoulos, D., xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",Yes,-,,,,x,x,x,x,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
50,276,"Liapis, A., Katsanos, C., Sotiropoulos, D., xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",Yes,-,,,,x,x,x,x,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
51,277,"Drungilas, D., Bielskis, A. A., & Denisov, V. (2010). An intelligent control system based on non-invasive man machine interaction. In Innovations in Computing Sciences and Software Engineering (pp. 63-68). Springer, Dordrecht.",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
52,278,"Wu, G., Liu, G., & Hao, M. (2010). The Analysis of Emotion Recognition from GSR Based on PSO. 2010 International Symposium on Intelligence Information Processing and Trusted Computing. doi:10.1109/iptc.2010.60",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53,279,"Giakoumis, D., Tzovaras, D., Moustakas, K., & Hassapis, G. (2011). Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features. IEEE Transactions on Affective Computing, 2(3), 119–133. doi:10.1109/t-affc.2011.4 ",Yes,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,x,,,,x,,,,,,,,,
54,280,"Safta, I., Grigore, O., & Căruntu, C.(2011). Emotion Detection Using Psycho-Physiological Signal Processing. Computer, 3, 4.",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
55,281,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
55,282,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
55,283,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
55,284,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
55,285,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
56,286,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ",No,-,-,-,-,-,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
56,287,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ",No,-,-,-,-,-,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
56,288,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ",No,-,-,-,-,-,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
57,289,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
57,290,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
57,291,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
57,292,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
57,293,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
58,294,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.",Yes,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
58,295,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
59,296,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
59,297,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
59,298,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
59,299,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
59,300,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
59,301,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
59,302,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
59,303,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
60,304,"Li, S., Guo, R., He, L., Gao, W., Qi, H., & Owens, G. (2014). MoodMagician. Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems - SenSys ’14. doi:10.1145/2668332.2668371",No,-,,,,,-,-,-,-,-,-,,,-,-,,-,-,-,,-,,,,-,,,,,,,,,,,,,
61,305,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
61,306,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
61,307,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
61,308,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
61,309,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
61,310,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,316,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,317,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,318,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,319,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,320,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,321,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,322,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,323,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,324,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,325,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
62,311,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,312,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,313,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,314,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,315,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,326,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,327,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,328,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,329,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
62,330,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
63,331,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.",-,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,x,,,,,,,,,
63,332,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.",-,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,x,,,,,,,,,
63,333,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.",-,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,x,,,,,,,,,
64,334,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",Yes,SAM,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
64,335,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",Yes,SAM,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
64,336,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",Yes,SAM,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
64,337,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",Yes,SAM,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
64,338,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",Yes,SAM,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
64,339,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",Yes,SAM,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
64,340,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",Yes,SAM,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
64,341,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.",Yes,SAM,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
65,342,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
65,343,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
65,344,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
66,345,"Dar, M. N., Akram, M. U., Khawaja, S. G., & Pujari, A. N. (2020). Cnn and lstm-based emotion charting using physiological signals. Sensors, 20(16), 4551.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
67,346,"Greco, A., Marzi, C., Lanata, A., Scilingo, E. P., & Vanello, N. (2019, July). Combining electrodermal activity and speech analysis towards a more accurate emotion recognition system. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 229-232). IEEE.",Yes,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68,347,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,348,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,349,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,350,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,351,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,352,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,353,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,354,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,355,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,356,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,357,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
68,358,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
69,359,"Lee, S., Lee, T., Yang, T., Yoon, C., & Kim, S. P. (2020). Detection of drivers’ anxiety invoked by driving situations using multimodal biosignals. Processes, 8(2), 155.",No,,,,,,,,,,,,,,x,,,,,,,,,,x,,,,,,,,,,,,,,
70,360,"García-Faura, Á., Hernández-García, A., Fernández-Martínez, F., Díaz-de-María, F., & San-Segundo, R. (2019, January). Emotion and attention: Audiovisual models for group-level skin response recognition in short movies. In Web Intelligence (Vol. 17, No. 1, pp. 29-40). IOS Press.",No,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
71,361,"Wei, W., Jia, Q., Feng, Y., & Chen, G. (2018). Emotion recognition based on weighted fusion strategy of multichannel physiological signals. Computational intelligence and neuroscience, 2018.",Yes,x,,,,,x,x,x,x,-,-,-,-,x,x,-,x,x,x,x,-,-,-,x,x,-,-,-,-,-,x,-,-,-,-,-,-,x
72,362,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
72,363,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
72,364,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
72,365,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
72,366,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
72,367,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
73,368,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
73,369,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
74,370,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",,,,,,,,,,,,,,,x,,x,,,,,,,,,,,,,,,,,,,,,,
74,371,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",,,,,,,,,,,,,,,x,,x,,,,,,,,,,,,,,,,,,,,,,
74,372,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",,,,,,,,,,,,,,,x,,x,,,,,,,,,,,,,,,,,,,,,,
74,373,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",,,,,,,,,,,,,,,x,,x,,,,,,,,,,,,,,,,,,,,,,
74,374,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",,,,,,,,,,,,,,,x,,x,,,,,,,,,,,,,,,,,,,,,,
75,375,"Sun, x., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75,376,"Sun, x., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75,377,"Sun, x., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.",- ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
76,378,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
76,379,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
76,380,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
76,381,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
76,382,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
76,383,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
76,384,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
76,385,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
77,386,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,387,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,388,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,389,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,390,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,391,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,392,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,393,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,394,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,395,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,396,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,397,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,398,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,399,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,400,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,401,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,402,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,403,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,404,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,405,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,406,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,407,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,408,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,409,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,410,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,411,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,412,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,413,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,414,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,415,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,416,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,417,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
77,418,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, x., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",Yes,x,,x,x,,x,x,x,x,,,,,x,,,x,x,x,,,x,,x,,,,,,,,,,,,,,
78,419,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.",Yes,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78,420,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.",Yes,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79,421,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79,422,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
80,423,"Raheel, A., Majid, M., Alnowami, M., & Anwar, S. M. (2020). Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia. Sensors, 20(14), 4037.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,424,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,425,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,426,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,427,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82,428,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82,429,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82,430,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82,431,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,432,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",Yes,x,x,x,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,433,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",Yes,x,x,x,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,434,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",Yes,x,x,x,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,435,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",Yes,x,x,x,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,436,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",Yes,x,x,x,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,437,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",Yes,x,x,x,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,438,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",Yes,x,x,x,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,439,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",Yes,x,x,x,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84,440,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
84,441,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
85,442,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,443,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,444,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,445,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,446,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,447,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,448,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,449,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,450,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,451,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,452,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,453,"Zhang, L. K., Sun, S. Q., xing, B. x., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86,454,"Liapis, A., Katsanos, C., Karousos, N., xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).",x,,,,,x,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86,455,"Liapis, A., Katsanos, C., Karousos, N., xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).",x,,,,,x,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87,456,"xie, J., xu, x., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",Yes,-,-,-,-,-,x,x,x,,,,,,x,x,,x,x,x,,,,,,,,,,,,x,,,,,,,
87,457,"xie, J., xu, x., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",Yes,-,-,-,-,-,x,x,x,,,,,,x,x,,x,x,x,,,,,,,,,,,,x,,,,,,,
87,458,"xie, J., xu, x., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",Yes,-,-,-,-,-,x,x,x,,,,,,x,x,,x,x,x,,,,,,,,,,,,x,,,,,,,
87,459,"xie, J., xu, x., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",Yes,-,-,-,-,-,x,x,x,,,,,,x,x,,x,x,x,,,,,,,,,,,,x,,,,,,,
87,460,"xie, J., xu, x., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",Yes,-,-,-,-,-,x,x,x,,,,,,x,x,,x,x,x,,,,,,,,,,,,x,,,,,,,
87,461,"xie, J., xu, x., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",Yes,-,-,-,-,-,x,x,x,,,,,,x,x,,x,x,x,,,,,,,,,,,,x,,,,,,,
88,462,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
88,463,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
89,464,"Yasemin, M., Sarıkaya, M. A., & Ince, G. (2019, July). Emotional state estimation using sensor fusion of EEG and EDA. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 5609-5612). IEEE.",Yes,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90,465,"Ghiasi, S., Greco, A., Barbieri, R., Scilingo, E. P., & Valenza, G. (2020). Assessing autonomic function from electrodermal activity and heart rate variability during cold-pressor test and emotional challenge. Scientific reports, 10(1), 1-13.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,466,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,467,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,468,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,469,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
92,470,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).",-,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,x
92,471,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).",-,,,,,,,,,,,,,,x,,,,,,,,,,,,,,,,,,,,,,,,x
93,472,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,473,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,474,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,475,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,476,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,477,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
94,478,"Rahman, J. S., Hossain, M. Z., & Gedeon, T. (2019, December). Measuring Observers' EDA Responses to Emotional Videos. In Proceedings of the 31st Australian Conference on Human-Computer-Interaction (pp. 457-461).",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
95,479,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
95,480,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.",-,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
96,481,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
96,482,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.",Yes,-,-,-,-,-,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97,483,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,484,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,485,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,486,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,487,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,488,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,489,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,490,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,491,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
97,492,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",Yes,x,-,x,-,-,x,x,x,x,x,x,-,-,,,,,,,,,,,,,,,,,,,,,,,,,
98,493,"Kołodziej, M., Tarnowski, P., Majkowski, A., & Rak, R. J. (2019). Electrodermal activity measurements for detection of emotional arousal. Bulletin of the Polish Academy of Sciences. Technical Sciences, 67(4).",Yes,,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
99,494,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
99,495,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.",Yes,x,,,,,x,x,x,x,x,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-
100,496,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
100,497,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
100,498,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
100,499,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",Yes,,,,,,x,x,x,-,x,x,-,x,,,,,,,,,,,,,,,,,,,,,,,,,
101,500,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).",Yes,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
101,501,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).",Yes,x,,,,,x,x,x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,