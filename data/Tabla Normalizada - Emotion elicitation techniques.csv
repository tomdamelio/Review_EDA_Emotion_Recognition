paper_id,model_id,apa_citation,is_multimodal,modality_visual,modality_auditory,modality_somatosensory,task_type_active,task_type_pasive,technique_name,visual_pictures,visual_videos,visual_words,visual_other,auditory_miusic,auditory_other,technique_clasif_multiple_techniques,technique_clasif_driving,technique_clasif_Imagination techniques /memory recall,technique_clasif_Social interactions,technique_clasif_Virtual Reality,technique_clasif_Meditation,technique_clasif_Reading,technique_clasif_Ux,technique_clasif_TEM clips (Tactile Enhanced Multimedia),technique_clasif_Videogame,technique_clasif_Puzzle,technique_description,elicitation_duration,elicitation_duration_range,elicitation_duration_mean,elicitation_duration_median
1,1,"Zangróniz, R., Martínez-Rodrigo, A., Pastor, J. M., López, M. T., & Fernández-Caballero, A. (2017). Electrodermal Activity Sensor for Classification of Calm/Distress Condition. Sensors (Basel, Switzerland), 17(10), E2324. https://doi.org/10.3390/s17102324
",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,"The participant sits in front of the experimentation monitor and the wearable is put in the wrist of the non-dominant hand (see Figure 1). In this regard, the experimentation monitor consists of a high resolution, 28 inch screen. When the technician verifies the proper functioning of the wearable and its communication with the software, the experiment starts. Firstly, the participant has to carefully read the general instructions of the experiment. Then, ten pictures that are labelled with high arousal and low valence are shown consecutively during 6 s each to the participant. Silences consisting of blank images with a fixed duration of 1 s are inserted before each picture used from the database. The pictures are selected randomly from the set of images that fulfil the condition. Therefore, the segment used for subsequent analysis is 70 s long (10 pictures 6 s duration, plus one blank image before each picture). In this sense, a single presentation of many stimuli presented for a short period of time might favour the continuity of emotional state [29]. Afterwards, a distracting task is presented to the participant so that his/her emotional state comes to neutral. Next, the experiment continues by showing randomly another set of ten images from IAPS that fulfil the condition to be part of those previously labelled as low arousal and high valence. Therefore, two segments of data from each individual are finally obtained, one for calm condition and another for distress condition. Again, silences are used before each picture. Lastly, the distracting task is offered again.",6s,-,-,-
2,2,"Liu, M., Fan, D., Zhang, X., & Gong, X. (2017). Human Emotion Recognition Based on Galvanic Skin Response Signal Feature Selection and SVM. 157–160. Scopus. https://doi.org/10.1109/ICSCSE.2016.0051
",x,,,-,-,x,-,-,,,-,-,-,-,,-,-,-,,,,,,,"All subjects are Junior in Jilin University of
Changchun, China. Materials for eliciting emotions are 4
video fragments cut elaborately from large amounts of
movies representing 4 different emotions, such as happiness,
grief, anger, and fear. Each fragment last 4-5 minutes and
was connected with a short video which also last 4-5
minutes for calm recovery. And we add the emotion when
people stay in calm. So there are 5 emotions that make the
pattern recognition.",-,4-5 m,-,-
3,3,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
3,4,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
4,5,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061905",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
4,6,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061906",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
4,7,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061907",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
4,8,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061908",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
4,9,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061909",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
5,10,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,-,6s,-,-,-
5,11,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,-,6s,-,-,-
6,12,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",,,,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,-,-,3m-5m,-,-
6,13,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",,,,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,-,-,3m-5m,-,-
6,14,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",,,,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,-,-,3m-5m,-,-
6,15,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",,,,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,-,-,3m-5m,-,-
6,16,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",,,,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,-,-,3m-5m,-,-
6,17,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",,,,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,-,-,3m-5m,-,-
6,18,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",,,,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,-,-,3m-5m,-,-
6,19,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014",,,,-,x,-,Rapid-ABC play protocol,,-,-,,-,,,-,-,x,-,-,-,-,-,-,-,-,-,3m-5m,-,-
7,20,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,21,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,22,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,23,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,24,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,25,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,26,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,27,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,28,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,29,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,30,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,31,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,32,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,33,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,34,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,35,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,36,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,37,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,38,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
7,39,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985",,,,-,x,,TSST,-,,,-,-,-,x,,-,x,,,,,,,,,-,-,-,-
8,41,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180",,,,,x,-,-,,,,,,,,,,,,,,,,,,"The intention is to collect data from users in their natural environment, over a prolonged period.",-,-,-,-
8,40,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180",,,,,x,,,,,,,,,,,,,,,,,,,,"The intention is to collect data from users in their natural environment, over a prolonged period.",-,-,-,-
8,42,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180",,,,,x,,,,,,,,,,,,,,,,,,,,"The intention is to collect data from users in their natural environment, over a prolonged period.",-,-,-,-
9,43,"Amalan, S., Shyam, A., Anusha, A. S., Preejith, S. P., Tony, A., Jayaraj, J., & Mohanasankar, S. (2018). Electrodermal Activity based Classification of Induced Stress in a Controlled Setting. MeMeA 2018 - 2018 IEEE International Symposium on Medical Measurements and Applications, Proceedings. Scopus. https://doi.org/10.1109/MeMeA.2018.8438703",,,,,x,-,Trier Social Stress Test (TSST),-,,,-,-,-,x,,-,x,-,-,,,,,,"A participant is given 3-5 minutes to prepare for a 5-minute presentation, followed by a 5-minute mental arithmetic task of counting numbers backwards continuously in large steps. The unanticipated element is that a large audience enters the room after the preparatory phase trying to induce stress in the form of stage fright. In the end, the participant is briefed and made to relax.",15m,-,-,-
10,44,"Machot, F. A., Ali, M., Ranasinghe, S., Mosa, A. H., & Kyandoghere, K. (2018). Improving subject-independent human emotion recognition using electrodermal activity sensors for active and assisted living. 222–228. Scopus. https://doi.org/10.1145/3197768.3201523",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
11,45,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589",x,,,-,-,x,-,,,,,,,,,,,,,,,,,,,,,,
11,46,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589",,x,,,,x,,,x,,,,x,,,,,,,,,,,,,-,-,-,
12,47,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",,,,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-
12,48,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",,,,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-
12,49,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",,,,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-
12,50,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076",,,,-,x,-,-,-,,,-,-,-,x,x,-,-,-,-,,,,,,"Precise tuning of driving scenario was performed on speed dreams for neutral, stress,
and anger stimulation. Neutral scenario was tuned to promote casual driving at a speed of
80 km/h (Chen, 2013). Driving track at this speed was composed of a wide circular
roadway with good vision. Stress driving was performed at a snowy mountain track with
narrow road and poor vision. Stress was triggered by causing the vehicle to drift via
sudden turning (Cohen and Wills, 1985; Yamaguchi et al., 2006; Öz et al., 2010; Chen,
2013). Anger stimulation places time constraint on the driver. The drivers also face
demanding situations, such as tailgating, sudden over-taking, and reckless driving
(Stephens and Groeger, 2006; Laing, 2010). Neutral and stress driving were performed
monotonously, whereas angry driving was performed simultaneously with four artificial
intelligence opponents.
The subjects were provided with three minutes of training period to familiarise them
with the control of driving unit. The subjects then performed three sets of control,
driving, and recovery sessions. Each session was composed of a pre-designed
emotion-stimulating scenario. In the control and recovery sessions, the subjects were
required to close their eyes and rest with their palms facing upward",50m,-,-,-
13,51,"Setyohadi, D. B., Kusrohmaniah, S., Gunawan, S. B., Pranowo, & Prabuwono, A. S. (2018). Galvanic skin response data classification for emotion detection. International Journal of Electrical and Computer Engineering, 8(5), 4004–4014. Scopus. https://doi.org/10.11591/ijece.v8i5.pp4004-4014",,,,-,x,-,-,-,,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
14,52,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,53,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,54,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,55,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,56,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,57,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,58,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,59,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,60,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,61,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,62,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,63,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,64,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,65,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
14,66,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017",-,x,-,-,-,x,Flickr,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"One hundred and thirty-two micro-videos in GIF format from Video2GIF dataset (Gygli et al., 2016) were randomly selected and annotated on similar scales on MTurk. In our experiments, we displayed the images in full screen mode. GIFs do not have adequate resolution when displayed in a full-screen mode. Hence, we extracted the higher quality equivalent from the source YouTube videos and re-encoded them to our desired format (1,920 × 1,080) with no sound. Forty micro-videos were selected to cover the whole spectrum in terms of average interestingness, pleasantness and coping potential. We opted for using GIFs due to their short duration, unimodality (only visual) and higher level of engagement (Bakhshi et al., 2016). GIFs are not always encoded with the same frame rate as the original video and contain loopiness, therefore we re-encoded them in 1.5x speed and repeated the sequence twice. Micro-videos were in average 11 s long.",25m,-,-,-
15,67,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,,,-,-,x,-,x,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-
15,68,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,69,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,70,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,71,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,72,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,73,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,74,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,75,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,76,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,77,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,78,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,,,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-
15,79,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,,,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-
15,80,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,,,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-
15,81,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,,,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-
15,82,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,,,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-
15,83,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",,,,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,-,-,-,-,-
15,84,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",,,,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,-,-,-,-,-
15,85,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",,,,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,-,-,-,-,-
15,86,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,87,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,88,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,89,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
15,90,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,,,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-
15,91,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",x,,,-,-,x,-,-,x,,-,-,-,x,-,-,-,-,-,,,,,,-,-,-,-,-
15,92,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067",,,,-,x,-,-,-,-,,-,-,-,-,x,-,-,-,-,,,,,,-,-,-,-,-
16,93,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
16,94,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
17,95,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,96,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,97,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,98,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,99,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,100,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,101,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,102,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,103,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,104,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,105,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,106,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,107,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,108,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,109,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,110,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,111,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,112,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,113,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,114,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,115,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,116,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,117,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,118,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,119,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,120,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,121,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,122,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,123,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,124,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,125,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
17,126,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9",x,,,-,-,x,-,-,x,,-,x,-,-,-,-,-,-,-,,,,,,"Totally, 56 short musical excerpts
(14 stimuli per each category) were selected from Vieillard
et al. [19].",15m,-,-,-
18,127,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533",,,,,x,-,survival task,-,-,-,-,-,-,-,-,-,x,-,-,,,,,,,,,15m,
18,128,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533",,,,,x,-,survival task,-,-,-,-,-,-,-,-,-,x,-,-,,,,,,,,,15m,
19,129,"Hernández-García, A., Fernández-Martínez, F., & Díaz-De-maría, F. (2017). Emotion and attention: Predicting electrodermal activity through video visual descriptors. 914–923. Scopus. https://doi.org/10.1145/3106426.3109418",x,,,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,"In order to collect ground truth data for the present study, the
EDA was measured on 22 subjects, 10 men and 12 women, with ages
between 21 and 59 years old, while they watched a concantenation
of videos. The data set consists of 44 videos with an average duration
of 44 seconds (σ = 21) which belong to the awarded spots at the 2002
Cannes International Advertising Festival. The whole sequence of
spots was projected in a movie theater while the EDA was recorded
on each subject by means of the Sociograph device",-,-,44s,-
20,130,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
20,131,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
21,132,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,133,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,134,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,135,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,136,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,137,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,138,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,139,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,140,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,141,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,142,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
21,143,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,"The affective
pictures are presented to the participants in a slide show
presentation, where each emotion group lasts 30 seconds.
To investigate the influence of human movement on emotion
recognition using physiological signals, we measured the physiological
responses of the participants in two scenarios: i) after
the participants have been sitting calmly for a while, and ii)
after the participants have performed physical activity. In the
second scenario, the experiments are repeated four times, with
the participant watching a different set of pictures each time.
The experiments are summarized in Table I.",-,-,-,-
22,145,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",-,x,-,-,-,x,-,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,5s,-,-,-
22,146,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",x,,,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,1m-2-m,-,-
22,144,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072",x,,,-,-,x,,-,,,,,,,,,,,,,,,,,,,,,
23,147,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
23,148,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
24,149,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677",-,-,x,-,-,x,IADS,-,-,,-,-,-,-,-,-,-,-,-,,,,,,"During the
experiment, participants were seated in a comfortable chair
in a controlled environment while listening to the IADS
sounds. Each subject was left alone in the room where the
experiment took place for the whole duration (29 minutes).
The acoustic stimulation was performed by using headphones
while the subject’s eyes were closed, to avoid any kind of
visual interference.
The experimental protocol consisted of 8 sessions: after
an initial resting session of 5 minutes, three arousal sessions
alternated with neutral sessions (see Figure 2). Each arousal
and neutral session was different from the others in regard
to the arousal level (labeled as N (neutral), L (low), M
(medium) and H (high)). We selected four arousal ranges that
were not overlapped. Such levels were set according to the
IADS scores reported in table I. Within each arousing session,
the acoustic stimuli were selected to have both negative and
positive valence. Each neutral session lasted 1 minute and 28
seconds, while the three arousal sessions had a duration of
3 minutes and 40 seconds, 4 minutes, and 5 minutes and 20
seconds, respectively. The different duration of each arousal
session is due to the different length of acoustic stimuli having
the same range of positive and negative valence.",-, 1 m 28 s - 5 m 20 s,-,-
24,150,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677",-,-,x,-,-,x,IADS,-,-,,-,-,-,-,-,-,-,-,-,,,,,,"During the
experiment, participants were seated in a comfortable chair
in a controlled environment while listening to the IADS
sounds. Each subject was left alone in the room where the
experiment took place for the whole duration (29 minutes).
The acoustic stimulation was performed by using headphones
while the subject’s eyes were closed, to avoid any kind of
visual interference.
The experimental protocol consisted of 8 sessions: after
an initial resting session of 5 minutes, three arousal sessions
alternated with neutral sessions (see Figure 2). Each arousal
and neutral session was different from the others in regard
to the arousal level (labeled as N (neutral), L (low), M
(medium) and H (high)). We selected four arousal ranges that
were not overlapped. Such levels were set according to the
IADS scores reported in table I. Within each arousing session,
the acoustic stimuli were selected to have both negative and
positive valence. Each neutral session lasted 1 minute and 28
seconds, while the three arousal sessions had a duration of
3 minutes and 40 seconds, 4 minutes, and 5 minutes and 20
seconds, respectively. The different duration of each arousal
session is due to the different length of acoustic stimuli having
the same range of positive and negative valence.",-, 1 m 28 s - 5 m 20 s,-,-
25,151,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",-,x,-,-,-,x,,x,-,,-,-,-,-,-,-,-,-,-,,,,,,each picture display contained five pictures and was displayed for 4 seconds each,4s,-,-,-
25,152,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",x,,,-,-,x,,-,x,,-,-,-,-,-,-,-,-,-,,,,,,,4s,-,-,-
25,153,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.",-,-,x,-,-,x,,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
26,154,"Zhang, Q., Lai, X., & Liu, G. (2016). Emotion recognition of GSR based on an improved quantum neural network. 1, 488–492. Scopus. https://doi.org/10.1109/IHMSC.2016.66",,,,,x,-,-,,,,,,,,,,,,,,,,,,"In the experiment, the process of collecting
the signal of the emotions was playing neutral materials
which were to keep mood in peace for 2 minutes,
introducing the materials background and the target
emotion for 30 seconds, and playing the materials of the
target emotion for 5 minutes. If the subject felt that a
corresponding emotion was generated, the button next to
the seat should be pressed to make a mark. Finally, after
collecting the signal of target emotion, the subjects
needed to evaluate the effectiveness of emotion had been
induced, which the type of the emotion had been induced
and intensity of emotion (very strong, strong, weak, very
weak).",,,,
27,155,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"56 short musical excerpts with 14 stimuli
per each emotional set were selected, which was
alidated by Vieillard et al.36 They consisted of a melody
with accompaniment composed in piano timbre, and
followed the rules of the Western tonal system",15m,-,-,-
27,156,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"56 short musical excerpts with 14 stimuli
per each emotional set were selected, which was
alidated by Vieillard et al.36 They consisted of a melody
with accompaniment composed in piano timbre, and
followed the rules of the Western tonal system",15m,-,-,-
27,157,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"56 short musical excerpts with 14 stimuli
per each emotional set were selected, which was
alidated by Vieillard et al.36 They consisted of a melody
with accompaniment composed in piano timbre, and
followed the rules of the Western tonal system",15m,-,-,-
28,158,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
28,159,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
28,160,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
28,161,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
28,162,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
28,163,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
28,164,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
28,165,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
28,166,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,2m34s,-,-,-
29,167,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",-,-,x,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-
29,168,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",-,-,x,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-
29,169,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",-,-,x,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-
29,170,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",-,-,x,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-
29,171,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",-,-,x,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-
29,172,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059",-,-,x,-,-,x,Robin,-,-,,-,x,-,-,-,-,-,-,-,,,,,,"The participants
were asked to listen to music excerpts, originally generated
by Robin [10], an algorithmic composer that generates western
classical-like music with affective connotation in real time. The
stimuli are weakly affective being evident from the fact that facial
expressions in the dataset are negligible if not absent.",-,-,-,-
30,173,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",-,,,-,x,-,"-G25
Logitech steering wheel kit",-,-,,-,-,-,-,x,-,-,-,-,,,,,,"A driving simulator consisting of a driving unit (G25
Logitech steering wheel kit), a large display, and simulation
software (Speed Dreams) was utilized for emotion stimulation.
Driving scenarios were configured to stimulate three
investigated emotions: neutral, stress, and anger. The neutral
scenario consists of casual driving at a speed of 80 km/h on a
circular roadway with a wide road and good vision [14]. The
stress scenario consists of a snowy mountain track with a
narrow road and poor vision, presumed to trigger sudden
turning, leading to vehicle drifting and impact [8][14][15][16].
Finally, the anger scenario requires the driver to complete a
designated driving task within a time limit. This task was
performed along with four artificial intelligence contestants,
with the aim of triggering aggressive road events, such as
tailgating, sudden overtaking, and reckless driving [17][18].
To further trigger driving anger, the subjects were also
informed that deductions would be made to their reward
money if they failed to accomplish the driving task within the
time frame; the full amount was still given at the end of
experiment",50m,-,-,-
30,174,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",-,,,-,x,-,"-G25
Logitech steering wheel kit",-,-,,-,-,-,-,x,-,-,-,-,,,,,,"A driving simulator consisting of a driving unit (G25
Logitech steering wheel kit), a large display, and simulation
software (Speed Dreams) was utilized for emotion stimulation.
Driving scenarios were configured to stimulate three
investigated emotions: neutral, stress, and anger. The neutral
scenario consists of casual driving at a speed of 80 km/h on a
circular roadway with a wide road and good vision [14]. The
stress scenario consists of a snowy mountain track with a
narrow road and poor vision, presumed to trigger sudden
turning, leading to vehicle drifting and impact [8][14][15][16].
Finally, the anger scenario requires the driver to complete a
designated driving task within a time limit. This task was
performed along with four artificial intelligence contestants,
with the aim of triggering aggressive road events, such as
tailgating, sudden overtaking, and reckless driving [17][18].
To further trigger driving anger, the subjects were also
informed that deductions would be made to their reward
money if they failed to accomplish the driving task within the
time frame; the full amount was still given at the end of
experiment",50m,-,-,-
30,175,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",-,,,-,x,-,"-G25
Logitech steering wheel kit",-,-,,-,-,-,-,x,-,-,-,-,,,,,,"A driving simulator consisting of a driving unit (G25
Logitech steering wheel kit), a large display, and simulation
software (Speed Dreams) was utilized for emotion stimulation.
Driving scenarios were configured to stimulate three
investigated emotions: neutral, stress, and anger. The neutral
scenario consists of casual driving at a speed of 80 km/h on a
circular roadway with a wide road and good vision [14]. The
stress scenario consists of a snowy mountain track with a
narrow road and poor vision, presumed to trigger sudden
turning, leading to vehicle drifting and impact [8][14][15][16].
Finally, the anger scenario requires the driver to complete a
designated driving task within a time limit. This task was
performed along with four artificial intelligence contestants,
with the aim of triggering aggressive road events, such as
tailgating, sudden overtaking, and reckless driving [17][18].
To further trigger driving anger, the subjects were also
informed that deductions would be made to their reward
money if they failed to accomplish the driving task within the
time frame; the full amount was still given at the end of
experiment",50m,-,-,-
31,176,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
31,177,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
31,178,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
32,179,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
32,180,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
32,181,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
32,182,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
32,183,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
32,184,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
33,185,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
33,186,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
33,187,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
33,188,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
33,189,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
33,190,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001",-,-,x,-,-,x,-,-,-,,-,x,-,-,-,-,-,-,-,,,,,,-,15m,-,-,-
34,191,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516647",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
34,192,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516648",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
34,193,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516649",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
34,194,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516649",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
35,195,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
35,196,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
35,197,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
35,198,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
36,199,"Zhang, S., Liu, G., & Lai, X. (2015). Classification of evoked emotions using an artificial neural network based on single, short-term physiological signals. Journal of Advanced Computational Intelligence and Intelligent Informatics, 19(1), 118-126.",x,,,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
37,207,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
37,208,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
37,209,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
37,210,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
37,211,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
37,212,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
37,213,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
37,214,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",,,,-,x,-,-,-,-,-,,-,,-,x,-,-,-,-,-,-,-,-,-,-,30m,-,-,-
37,215,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",,,,-,x,-,-,-,-,-,,-,,-,x,-,-,-,-,-,-,-,-,-,-,30m,-,-,-
37,216,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",,,,-,x,-,-,-,-,-,,-,,-,x,-,-,-,-,-,-,-,-,-,-,30m,-,-,-
37,217,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",,,,-,x,-,-,-,-,-,,-,,-,x,-,-,-,-,-,-,-,-,-,-,30m,-,-,-
37,218,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",,,,-,x,-,-,-,-,-,,-,,-,x,-,-,-,-,-,-,-,-,-,-,30m,-,-,-
37,219,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",,,,-,x,-,-,-,-,-,,-,,-,x,-,-,-,-,-,-,-,-,-,-,30m,-,-,-
37,220,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",,,,-,x,-,-,-,-,-,,-,,-,x,-,-,-,-,-,-,-,-,-,-,30m,-,-,-
37,221,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
37,222,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
37,223,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
37,224,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
37,225,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
37,226,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
37,227,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
37,228,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
37,229,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
37,230,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
37,231,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
37,232,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
37,233,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
37,234,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
37,235,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
37,236,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
37,237,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
37,238,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
37,239,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
37,240,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
37,241,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
37,200,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
37,201,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
37,202,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
37,203,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
37,204,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
37,205,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
37,206,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
38,242,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
38,243,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
38,244,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
38,245,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
38,246,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
38,247,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
38,248,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
38,249,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
39,250,"Martínez-Rodrigo, A., Zangróniz, R., Pastor, J. M., & Sokolova, M. V. (2017). Arousal level classification of the aging adult from electro-dermal activity: From hardware development to software architecture. Pervasive and Mobile Computing, 34, 46–59. Scopus. https://doi.org/10.1016/j.pmcj.2016.04.006",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,-,-,-,-,,,,,,-,9m,-,-,-
40,251,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
40,252,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
40,253,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
40,254,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636",-,-,-,-,-,-,-,-,-,,-,-,-,-,-,-,-,-,-,,,,,,-,-,-,-,-
41,255,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",,,,,x,-,survival task,-,-,-,-,-,-,-,-,-,x,-,-,,,,,,,,,15m,
41,256,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",,,,,x,-,survival task,-,-,-,-,-,-,-,-,-,x,-,-,,,,,,,,,15m,
41,257,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",,,,,x,-,survival task,-,-,-,-,-,-,-,-,-,x,-,-,,,,,,,,,15m,
41,258,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007",,,,,x,-,survival task,-,-,-,-,-,-,-,-,-,x,-,-,,,,,,,,,15m,
42,259,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,884s,
42,260,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,,884s,
43,261,"Barral, O., Kosunen, I., & Jacucci, G. (2017). No need to laugh out loud: Predicting humor appraisal of comic strips based on physiological signals in a realistic environment. ACM Transactions on Computer-Human Interaction, 24(6). Scopus. https://doi.org/10.1145/3157730",,,,-,x,-,-,-,-,,-,-,-,-,-,-,-,-,-,x,,,,,,,,,
44,262,"Lanatà, A., Valenza, G., & Scilingo, E. P. (2012). A novel EDA glove based on textile-integrated electrodes for affective computing. Medical & Biological Engineering & Computing, 50(11), 1163–1172. doi:10.1007/s11517-012-0921-9",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,-,-,-,-,-,-,,,,,-,10 s,-,-,-
45,263,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",,x,,-,x,-,Stroop color-word interference test’’ (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,3s,-,-,-
45,264,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",,x,,-,x,-,Stroop color-word interference test’’ (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,3s,-,-,-
45,265,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",,x,,-,x,-,Stroop color-word interference test’’ (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,3s,-,-,-
45,266,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",,x,,-,x,-,Stroop color-word interference test’’ (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,3s,-,-,-
45,267,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ",,x,,-,x,-,Stroop color-word interference test’’ (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,3s,-,-,-
46,268,"GOUIZI, K., BEREKSI REGUIG, F., & MAAOUI, C. (2011). Emotion recognition from physiological signals. Journal of Medical Engineering & Technology, 35(6-7), 300–307. doi:10.3109/03091902.2011.601784",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
47,269,"Bornoiu, I.-V., Strungaru, R., & Grigore, O. (2015). Intelligent System for Emotion Recognition Based on Electrodermal Activity Processing. 6th European Conference of the International Federation for Medical and Biological Engineering, 70–73. doi:10.1007/978-3-319-11128-5_18 ",,,,-,x,-,Trier social stress test,-,,,-,-,-,-,,-,x,-,,,,,,,-,-,-,-,-
48,270,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",,,,-,x,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,x,,,,-,-,-,-,-
48,271,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",,,,-,x,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,x,,,,-,-,-,-,-
48,272,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",,,,-,x,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,x,,,,-,-,-,-,-
48,273,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",,,,-,x,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,x,,,,-,-,-,-,-
48,274,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450",,,,-,x,-,Visit website with interruptions,-,,,-,-,-,-,,-,-,-,,,x,,,,-,-,-,-,-
49,275,"Drungilas, D., Bielskis, A. A., & Denisov, V. (2010). An intelligent control system based on non-invasive man machine interaction. In Innovations in Computing Sciences and Software Engineering (pp. 63-68). Springer, Dordrecht.",-,-,,,,,,,,,,,,,,,,,,,,,,,,,,,
50,276,"Wu, G., Liu, G., & Hao, M. (2010). The Analysis of Emotion Recognition from GSR Based on PSO. 2010 International Symposium on Intelligence Information Processing and Trusted Computing. doi:10.1109/iptc.2010.60",x,,,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,,,,,,,-,-,240-300s,-,-
51,277,"Giakoumis, D., Tzovaras, D., Moustakas, K., & Hassapis, G. (2011). Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features. IEEE Transactions on Affective Computing, 2(3), 119–133. doi:10.1109/t-affc.2011.4 ",,,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,x,-,-,-,-,-,-
52,278,"Safta, I., Grigore, O., & Căruntu, C.(2011). Emotion Detection Using Psycho-Physiological Signal Processing. Computer, 3, 4.",-,x,-,-,-,x,IAPS,x,,,-,-,-,-,,-,-,-,,,,,,,-,7s,-,-,-
53,279,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",,x,,-,x,-,Stroop ,-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
53,280,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",,x,,-,x,-,Stroop ,-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
53,281,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",,x,,-,x,-,Stroop ,-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
53,282,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",,x,,-,x,-,Stroop ,-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
53,283,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.",,x,,-,x,-,Stroop ,-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
54,284,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ",x,,,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
54,285,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ",x,,,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
54,286,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ",x,,,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
55,287,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",,x,,-,x,-,Stoop color word interfence test (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
55,288,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",,x,,-,x,-,Stoop color word interfence test (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
55,289,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",,x,,-,x,-,Stoop color word interfence test (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
55,290,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",,x,,-,x,-,Stoop color word interfence test (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
55,291,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25",,x,,-,x,-,Stoop color word interfence test (SCWT),-,,x,-,-,-,-,,-,-,-,,,,,,,-,-,-,-,-
56,292,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.",x,,,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,300-480s,-,-
56,293,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.",x,,,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,300-480s,-,-
57,294,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",,,,-,x,-,-,-,,,-,-,-,-,,-,x,-,,,,,,,robot interaction ,-,10-30s,-,-
57,295,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",,,,-,x,-,-,-,,,-,-,-,-,,-,x,-,,,,,,,robot interaction ,-,10-30s,-,-
57,296,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",,,,-,x,-,-,-,,,-,-,-,-,,-,x,-,,,,,,,robot interaction ,-,10-30s,-,-
57,297,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",,,,-,x,-,-,-,,,-,-,-,-,,-,x,-,,,,,,,robot interaction ,-,10-30s,-,-
57,298,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",,,,-,x,-,-,-,,,-,-,-,-,,-,x,-,,,,,,,robot interaction ,-,10-30s,-,-
57,299,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",,,,-,x,-,-,-,,,-,-,-,-,,-,x,-,,,,,,,robot interaction ,-,10-30s,-,-
57,300,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",,,,-,x,-,-,-,,,-,-,-,-,,-,x,-,,,,,,,robot interaction ,-,10-30s,-,-
57,301,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ",,,,-,x,-,-,-,,,-,-,-,-,,-,x,-,,,,,,,robot interaction ,-,10-30s,-,-
58,302,"Li, S., Guo, R., He, L., Gao, W., Qi, H., & Owens, G. (2014). MoodMagician. Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems - SenSys ’14. doi:10.1145/2668332.2668371",x,,,-,-,x,-,-,x,,-,-,-,-,,-,-,-,,,,,,,-,-,240s-300s,-,-
59,303,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
59,304,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
59,305,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
59,306,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
59,307,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
59,308,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,309,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,310,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,311,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,312,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,313,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,314,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,315,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,316,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,317,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,318,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,319,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,320,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,321,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,322,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,323,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
60,324,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,325,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,326,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,327,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
60,328,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
61,329,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,00 m 07 ss,,,
61,330,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,00 m 07 ss,,,
61,331,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,00 m 07 ss,,,
62,332,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.", ,,,, ,x,,,,,,,,,,,,,x,,,,,,,40 m 00 s,,,
62,333,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.", ,,,, ,x,,,,,,,,,,,,,x,,,,,,,40 m 00 s,,,
62,334,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.", ,,,, ,x,,,,,,,,,,,,,x,,,,,,,40 m 00 s,,,
62,335,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.", ,,,, ,x,,,,,,,,,,,,,x,,,,,,,40 m 00 s,,,
62,336,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.", ,,,, ,x,,,,,,,,,,,,,x,,,,,,,40 m 00 s,,,
62,337,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.", ,,,, ,x,,,,,,,,,,,,,x,,,,,,,40 m 00 s,,,
62,338,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.", ,,,, ,x,,,,,,,,,,,,,x,,,,,,,40 m 00 s,,,
62,339,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.", ,,,, ,x,,,,,,,,,,,,,x,,,,,,,40 m 00 s,,,
63,340,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
63,341,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
63,342,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
64,343,"Dar, M. N., Akram, M. U., Khawaja, S. G., & Pujari, A. N. (2020). Cnn and lstm-based emotion charting using physiological signals. Sensors, 20(16), 4551.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
65,344,"Greco, A., Marzi, C., Lanata, A., Scilingo, E. P., & Vanello, N. (2019, July). Combining electrodermal activity and speech analysis towards a more accurate emotion recognition system. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 229-232). IEEE.",,x,,,,x,,,,x,,,,,,,,,,,,,,,,0m 2s,,,
66,345,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,346,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,347,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,348,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,349,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,350,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,351,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,352,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,353,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,354,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,355,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
66,356,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
67,357,"Lee, S., Lee, T., Yang, T., Yoon, C., & Kim, S. P. (2020). Detection of drivers’ anxiety invoked by driving situations using multimodal biosignals. Processes, 8(2), 155.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,0m 30s,,,
68,358,"García-Faura, Á., Hernández-García, A., Fernández-Martínez, F., Díaz-de-María, F., & San-Segundo, R. (2019, January). Emotion and attention: Audiovisual models for group-level skin response recognition in short movies. In Web Intelligence (Vol. 17, No. 1, pp. 29-40). IOS Press.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
69,359,"Wei, W., Jia, Q., Feng, Y., & Chen, G. (2018). Emotion recognition based on weighted fusion strategy of multichannel physiological signals. Computational intelligence and neuroscience, 2018.",x,,,-,-,x,-,-,x,-,-,-,,-,-,-,-,-,-,-,-,-,-,-,,-,34.9s - 117 s,"81,4 s",-
70,360,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
70,361,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
70,362,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
70,363,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
70,364,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
70,365,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
71,366,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
71,367,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
72,368,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",x,,,,,x,,,x,,,,x,,,,,,,,,,,,,,,,
72,369,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",x,,,,,x,,,x,,,,x,,,,,,,,,,,,,,,,
72,370,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",x,,,,,x,,,x,,,,x,,,,,,,,,,,,,,,,
72,371,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",x,,,,,x,,,x,,,,x,,,,,,,,,,,,,,,,
72,372,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.",x,,,,,x,,,x,,,,x,,,,,,,,,,,,,,,,
73,373,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,00 m 120 s,
73,374,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,00 m 120 s,
73,375,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,00 m 120 s,
74,376,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
74,377,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
74,378,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
74,379,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
74,380,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
74,381,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
74,382,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
74,383,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
75,384,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,385,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,386,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,387,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,388,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,389,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,390,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,391,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,392,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,393,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,394,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,395,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,396,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,397,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,398,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,399,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,400,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,401,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,402,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,403,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,404,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,405,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,406,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,407,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,408,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,409,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,410,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,411,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,412,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,413,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,414,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,415,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
75,416,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,02 m 30 s - 05 m 00 s,,
76,417,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.",,,x,,,,,,,,,x,,,,,,,,,,,,,,,00 m 10 s - 01m 30 s,,
76,418,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.",,,x,,,,,,,,,x,,,,,,,,,,,,,,,00 m 10 s - 01m 30 s,,
77,419,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.",,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
77,420,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.",,x,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
78,421,"Raheel, A., Majid, M., Alnowami, M., & Anwar, S. M. (2020). Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia. Sensors, 20(14), 4037.",,,,,,,,,,,,,,,,,,,,,,x,,,,,00 m 21 s - 00 m 58 s,,
79,422,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
79,423,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
79,424,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
79,425,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
80,426,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
80,427,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
80,428,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
80,429,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
81,430,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
81,431,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
81,432,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
81,433,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",x,,,,,x,,,x,,,,,,,,,,,,,,,,,,,,
81,434,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,
81,435,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,
81,436,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,
81,437,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.",,,,,,,,,,,,,,,,,,x,,,,,,,,,,,
82,438,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
82,439,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
83,440,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,441,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,442,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,443,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,444,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,445,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,446,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,447,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,448,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,449,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,450,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
83,451,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.",x,,,,,x,,,x,,,x,,,,,,,,,,,,,,,,,
84,452,"Liapis, A., Katsanos, C., Karousos, N., Xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).",,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
84,453,"Liapis, A., Katsanos, C., Karousos, N., Xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).",,,,,x,,,,,,,,,,,,,,,,x,,,,,,,,
85,454,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",x,,,.,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,32-245s,68s,
85,455,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",x,,,.,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,32-245s,68s,
85,456,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",x,,,.,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,32-245s,68s,
85,457,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",x,,,.,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,32-245s,68s,
85,458,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",x,,,.,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,32-245s,68s,
85,459,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.",x,,,.,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,32-245s,68s,
86,460,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
86,461,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
87,462,"Yasemin, M., Sarıkaya, M. A., & Ince, G. (2019, July). Emotional state estimation using sensor fusion of EEG and EDA. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 5609-5612). IEEE.",x,,,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,"we conducted the experiments with
audiovisual stimuli to induce emotions by playing videos.
We have used a video prepared in [21], which contains
several kinds of movie clips and binaural beats 1. As stimuli,
we chose movies which have high effect to arouse human
emotions. A binaural beat is an auditory sensation, which
appears when two slightly different sounds are received to",-,-,-,-
88,463,"Ghiasi, S., Greco, A., Barbieri, R., Scilingo, E. P., & Valenza, G. (2020). Assessing autonomic function from electrodermal activity and heart rate variability during cold-pressor test and emotional challenge. Scientific reports, 10(1), 1-13.",x,,,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,1M 30S,-,-,-
89,464,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,-,0M 06S,-,-,-
89,465,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,-,0M 06S,-,-,-
89,466,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,-,0M 06S,-,-,-
89,467,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).",-,x,-,-,-,x,IAPS,x,-,,-,-,-,-,-,--,-,-,-,-,-,-,,,-,0M 06S,-,-,-
90,468,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90,469,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,470,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
91,471,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
91,472,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
91,473,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
91,474,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
91,475,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
92,476,"Rahman, J. S., Hossain, M. Z., & Gedeon, T. (2019, December). Measuring Observers' EDA Responses to Emotional Videos. In Proceedings of the 31st Australian Conference on Human-Computer-Interaction (pp. 457-461).",x,,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,0M 2S - 0M3S,-,-
93,477,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
93,478,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.",-,x,-,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,,,-,3m
94,479,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
94,480,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.",-,-,x,-,-,x,-,-,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,,
95,481,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,482,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,483,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,484,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,485,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,486,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,487,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,488,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,489,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
95,490,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.",x,,,-,-,x,-,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,00m 52 s - 23m 58s,,
96,491,"Kołodziej, M., Tarnowski, P., Majkowski, A., & Rak, R. J. (2019). Electrodermal activity measurements for detection of emotional arousal. Bulletin of the Polish Academy of Sciences. Technical Sciences, 67(4).",x,,,-,-,x,-,-,x,,-,-,-,-,-,-,-,-,-,-,-,-,,,-,-,0M 4.9S - 0M 71.1S,-,-
97,492,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
97,493,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.",x,,,-,-,x,-,-,x,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,,1m,-,-,-
98,494,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
98,495,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
98,496,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
98,497,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.",x,,,-,-,x,,-,x,-,-,-,x,-,-,-,-,-,-,-,-,-,-,-,-,-,51s –127 s,-,-
99,498,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).",,x,,,,x,IAPS,x,,,,,,,,,,,,,,,,,,00 m 06 ss,,,
99,499,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).",,x,,,,x,IAPS,x,,,,,,,,,,,,,,,,,,00 m 06 ss,,,