{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis de revisión sistematica de modelos computacionales que predicen estados afectivos utilizando actividad electrodermica.                                                                                                            \n",
    "Pre- registro: https://osf.io/ewuaf/  \n",
    "Cantidad de papers N = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sympy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenpyxl\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m \u001b[39mimport\u001b[39;00m convolution\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sympy'"
     ]
    }
   ],
   "source": [
    "from operator import getitem\n",
    "from pyexpat import model\n",
    "from traceback import print_tb\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from sympy import convolution\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import lines\n",
    "from matplotlib import patches\n",
    "from matplotlib.patheffects import withStroke\n",
    "#from sympy import pretty_print\n",
    "import stylecloud\n",
    "import stop_words\n",
    "from stop_words import get_stop_words\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "import networkx as nx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicio definición de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = \"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/Propuesta formato tabla.xlsx\"\n",
    "tabla=pd.read_excel(\"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/Propuesta formato tabla.xlsx\")\n",
    "df= pd.DataFrame(tabla)\n",
    "df_sin_duplicado = df.drop_duplicates(subset=\"paper_id\")\n",
    "df.head()\n",
    "df.columns\n",
    "\n",
    "#Tomo la columna Affective model de la hoja Statistical Learning model\n",
    "modelos = pd.read_excel(archivo, sheet_name=\"Statistical Learning model\", usecols=[\"Affective model\"])\n",
    "año = pd.read_excel(archivo, sheet_name=\"Metadata\", usecols=['year'])\n",
    "paper_id = pd.read_excel(archivo, sheet_name=\"Metadata\", usecols=['paper_id'])\n",
    "\n",
    "#Evolución del modelo\n",
    "modelo_evolve= pd.read_excel(\"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/Evolución del modelo.xlsx\")\n",
    "modelo_evolve.columns\n",
    "\n",
    "modelo = modelo_evolve.drop_duplicates(subset='paper_id')\n",
    "\n",
    "#Cantidad de modelos en total a través del tiempo\n",
    "tipo_modelo = modelo['Affective model'] #dimensional = 318 , categorial= 183, sin eliminación duplicado\n",
    "#Con eliminación de duplicados : dimensional = 64 categorial = 37\n",
    "\n",
    "#Tomo archivo para luego graficar la evolución del modelo\n",
    "grafico = pd.read_excel(\"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/frecuencia_modelo.xlsx\")\n",
    "grafico_modelo = pd.DataFrame(grafico)\n",
    "\n",
    "# BASE DE DATOS\n",
    "data_type = pd.read_excel(archivo, sheet_name=\"Data type\")\n",
    "data_type= data_type.drop_duplicates(subset='paper_id')\n",
    "colum_data = data_type.columns\n",
    "\n",
    "#AUTOREPORTE\n",
    "self_report = pd.read_excel(archivo, sheet_name=\"Selft report\")\n",
    "self_report.columns\n",
    "self_report = self_report.drop_duplicates(subset='paper_id')\n",
    "\n",
    "#Tecnicas de elicitación\n",
    "emotion_elicitation = pd.read_excel(archivo, sheet_name=\"Emotion elicitation techniques\")\n",
    "emotion_elicitation.columns\n",
    "emotion_elicitation = emotion_elicitation.drop_duplicates(subset='paper_id')\n",
    "\n",
    "# Duración de los estimulos\n",
    "duration= emotion_elicitation ['elicitation_duration'] \n",
    "duration_range = emotion_elicitation ['elicitation_duration_range']\n",
    "duration_mean = emotion_elicitation ['elicitation_duration_mean']\n",
    "duration_median= emotion_elicitation ['elicitation_duration_median']# no tiene nada\n",
    "\n",
    "#Tipo de algoritmo\n",
    "algoritmo = pd.read_excel(archivo, sheet_name=\"Statistical Learning model\")\n",
    "#creo archivo con los nombres\n",
    "#al= pd.ExcelWriter(\"algoritmo.xlsx\")\n",
    "#a.to_excel(al, \"modelo\")\n",
    "#al.save()\n",
    "#al.close()\n",
    "\n",
    "#Sección de interpretación \n",
    "model_interpretation = pd.read_excel(archivo, sheet_name=\"Statistical Learning model\")\n",
    "interpretation=model_interpretation[model_interpretation['model_interpretation']== \"-\"].index\n",
    "model_interpretation= model_interpretation.drop(interpretation)\n",
    "\n",
    "#Distribución por paises de mundo\n",
    "world = pd.read_excel(\"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/mapa.xlsx\")\n",
    "world= pd.DataFrame(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpieza y exploración de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELOS AFECTIVOS\n",
    "\n",
    "#filtramos cantidad de modelos solo en 2010\n",
    "selec1=modelo[modelo[\"year\"]== 2010]\n",
    "cant1= selec1.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2011\n",
    "selec2=modelo[modelo[\"year\"]== 2011]\n",
    "cant2= selec2.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2012\n",
    "selec3=modelo[modelo[\"year\"]== 2012]\n",
    "cant3= selec3.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2013\n",
    "selec4=modelo[modelo[\"year\"]== 2013]\n",
    "cant4= selec4.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2014\n",
    "selec5=modelo[modelo[\"year\"]== 2014]\n",
    "cant5= selec5.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2015\n",
    "selec6=modelo[modelo[\"year\"]== 2015]\n",
    "cant6= selec6.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2016\n",
    "selec7=modelo[modelo[\"year\"]== 2016]\n",
    "cant7= selec7.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2017\n",
    "selec8=modelo[modelo[\"year\"]== 2017]\n",
    "cant8= selec8.groupby(\"Affective model\").size()\n",
    "print(\"cantidad de modelos solo en 2017\"\":\", cant8)\n",
    "#filtramos cantidad de modelos solo en 2018\n",
    "selec9=modelo[modelo[\"year\"]== 2018]\n",
    "cant9= selec9.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2019\n",
    "selec10=modelo[modelo[\"year\"]== 2019]\n",
    "cant10= selec10.groupby(\"Affective model\").size()\n",
    "#filtramos cantidad de modelos solo en 2020\n",
    "selec11=modelo[modelo[\"year\"]== 2020]\n",
    "cant11= selec11.groupby(\"Affective model\").size()\n",
    "\n",
    "#BASE DE DATOS \n",
    "\n",
    "# DEAP\n",
    "data_type['DEAP ']= data_type['DEAP ']. replace(\"x\", \"DEAP\") \n",
    "data_type['DEAP ']= data_type['DEAP ']. replace(\"X\", \"DEAP\") \n",
    "deap= data_type.groupby('DEAP ').size()\n",
    "print(deap)\n",
    "\n",
    "#AMIGOS\n",
    "data_type['AMIGOS']= data_type['AMIGOS']. replace(\"x\", 'AMIGOS') \n",
    "data_type['AMIGOS']= data_type['AMIGOS']. replace(\"X\", \"AMIGOS\") \n",
    "amigos= data_type.groupby('AMIGOS').size()\n",
    "print(amigos)\n",
    "\n",
    "#MAHNOB\n",
    "data_type['MAHNOB']= data_type['MAHNOB']. replace(\"x\", 'MAHNOB') \n",
    "data_type['MAHNOB']= data_type['MAHNOB']. replace(\"X\", 'MAHNOB')\n",
    "mahnob= data_type.groupby('MAHNOB').size()\n",
    "print(mahnob)\n",
    "\n",
    "#CASE\n",
    "data_type['CASE']= data_type['CASE']. replace(\"x\", 'CASE') \n",
    "data_type['CASE']= data_type['CASE']. replace(\"X\", 'CASE')\n",
    "case= data_type.groupby('CASE').size()\n",
    "print(case)\n",
    "\n",
    "#Ascertain\n",
    "data_type['Ascertain']= data_type['Ascertain']. replace(\"x\", 'Ascertain') \n",
    "data_type['Ascertain']= data_type['Ascertain']. replace(\"X\", 'Ascertain')\n",
    "asc= data_type.groupby('Ascertain').size()\n",
    "print(asc)\n",
    "\n",
    "#Cog.load\n",
    "data_type['Cog.load']= data_type['Cog.load']. replace(\"x\", 'Cog.load')\n",
    "data_type['Cog.load']= data_type['Cog.load']. replace(\"X\", 'Cog.load')\n",
    "cog= data_type.groupby('Cog.load').size()\n",
    "\n",
    "#Multimodal Dyadic Behavior (MMDB)\n",
    "data_type['Multimodal Dyadic Behavior (MMDB)']= data_type['Multimodal Dyadic Behavior (MMDB)']. replace(\"x\", 'MMDB')\n",
    "data_type['Multimodal Dyadic Behavior (MMDB)']= data_type['Multimodal Dyadic Behavior (MMDB)']. replace(\"X\", 'MMDB')\n",
    "mmdb= data_type.groupby('Multimodal Dyadic Behavior (MMDB)').size()\n",
    "\n",
    "#RECOLA\n",
    "data_type['RECOLA']= data_type['RECOLA']. replace(\"x\", 'RECOLA')\n",
    "data_type['RECOLA']= data_type['RECOLA']. replace(\"X\", 'RECOLA')\n",
    "recola= data_type.groupby('RECOLA').size()\n",
    "\n",
    "#DECAF\n",
    "data_type['DECAF']= data_type['DECAF']. replace(\"x\", 'DECAF')\n",
    "data_type['DECAF']= data_type['DECAF']. replace(\"X\", 'DECAF')\n",
    "decaf = data_type.groupby('DECAF').size()\n",
    "\n",
    "#Driving Workload\n",
    "data_type['Driving Workload']= data_type['Driving Workload']. replace(\"x\", 'Driving Workload')\n",
    "data_type['Driving Workload']= data_type['Driving Workload']. replace(\"X\", 'Driving Workload')\n",
    "driving = data_type.groupby('Driving Workload').size()\n",
    "\n",
    "#(AV+EC) 2015\n",
    "data_type['(AV+EC) 2015']= data_type['(AV+EC) 2015']. replace(\"x\", 'AV+EC')\n",
    "data_type['(AV+EC) 2015']= data_type['(AV+EC) 2015']. replace(\"X\", 'AV+EC')\n",
    "av = data_type.groupby('(AV+EC) 2015').size()\n",
    "\n",
    "#Liris\n",
    "data_type['Liris']= data_type['Liris']. replace(\"x\", 'Liris')\n",
    "data_type['Liris']= data_type['Liris']. replace(\"X\", 'Liris')\n",
    "liris= data_type.groupby('Liris').size()\n",
    "\n",
    "#SenseEmotion\n",
    "sense= data_type.groupby('SenseEmotion').size()\n",
    "\n",
    "#\"PMEmo\"\n",
    "pmemo= data_type.groupby(\"PMEmo\").size()\n",
    "\n",
    "#'AFEW'\n",
    "afew= data_type.groupby('AFEW').size()\n",
    "\n",
    "#'Hazumi1911'\n",
    "hazumi= data_type.groupby('Hazumi1911').size()\n",
    "\n",
    "#'Bio Vid Emo\\nDB'\n",
    "bio = data_type.groupby('Bio Vid Emo\\nDB').size()\n",
    "\n",
    "#'RCDAT' \n",
    "rcdat= data_type.groupby('RCDAT').size()\n",
    "\n",
    "#'DREAMER',\n",
    "dreamer= data_type.groupby(\"DREAMER\").size()\n",
    "\n",
    "#'Non-EEG Biosignals Data Set for\\nAssessment and Visualization of Neurological Status'\n",
    "eeg= data_type.groupby('Non-EEG Biosignals Data Set for\\nAssessment and Visualization of Neurological Status').size()\n",
    "\n",
    "#'Stress Recognition in Automobile\\nDrivers Data Set', \n",
    "str= data_type.groupby('Stress Recognition in Automobile\\nDrivers Data Set').size()\n",
    "\n",
    "#'PsPM-HRA1'\n",
    "pspm= data_type.groupby('PsPM-HRA1').size()\n",
    "\n",
    "#LOS PAPER ID CON EXCEPCIÓN A LA REGLA DE ELIMINACIÓN DE DUPLICADOS SON: 38 (se agrega 1 a DEAP, AMIGOS, MAHNOB, COG.LOAD, DRIVING WORKLOAD) \n",
    "# Y 62 ( SE AGREGA UNO DE DEAP Y MAHNOB)\n",
    "#PAPER ID 59 (DE LOLO) Y 94 (DE EMMA) NO TIENEN MARCADA EL TIPO DE BASE DE DATOS; CHEQUEAR\n",
    "#PAPER ID 95 TIENE BASE DE DATOS Y PRIVADA, OJO AL ELIMINAR DUPLICADOS PARA EL GRAFICO DE BASE DE DATOS PRIVADAS\n",
    "#PAPER ID 7 NO TIENE NADA MARCADO (ES DE AGUS VER)\n",
    "\n",
    "#DIMENSIONES DEL AUTOREPORTE\n",
    "\n",
    "#inicio conteo de casos de los modelos dimensionales\n",
    "valence= self_report.groupby('dimension_valence').size()\n",
    "arousal= self_report.groupby('dimension_arousal').size()\n",
    "dominance= self_report.groupby('dimension_dominance').size()\n",
    "like = self_report.groupby('dimension_like_or_dislike').size()\n",
    "familiarity = self_report.groupby('dimension_familiarity').size()\n",
    "stress = self_report.groupby('dimension_stress').size()\n",
    "engagement= self_report.groupby('dimension_engagement').size()\n",
    "\n",
    "\n",
    "#Filtrado para categorias dentro de self-report (FLACO HACETE UNA FUNCIÓN)\n",
    "\n",
    "anger = self_report.groupby(\"categorial_anger\").size()#1\n",
    "stress = self_report.groupby(\"Stress\").size()#1\n",
    "disgust = self_report.groupby(\"Disgust\").size()#2\n",
    "fear = self_report.groupby(\"Fear\").size()#2\n",
    "sadness= self_report.groupby(\"Sadness\").size()#2\n",
    "surprise = self_report.groupby(\"Surprise\").size()#1\n",
    "hapiness = self_report.groupby(\"Happiness\").size()#1\n",
    "pleasant= self_report.groupby(\"Pleasant\").size()#1\n",
    "unpleasant = self_report.groupby(\"Unpleasant\").size()#0\n",
    "anxiety = self_report.groupby(\"Anxiety\").size()#2\n",
    "neutral = self_report.groupby(\"Neutral\").size()#3\n",
    "funny= self_report.groupby(\"Funny\").size()#1\n",
    "horror= self_report.groupby(\"Horror\").size()#nada\n",
    "weepy= self_report.groupby(\"Weepy\").size()#nada\n",
    "boredom= self_report.groupby(\"Boredom\").size()#2\n",
    "relaxation= self_report.groupby(\"Relaxation\").size()#1\n",
    "amusement= self_report.groupby(\"Amusement\").size() #nada\n",
    "confusion= self_report.groupby(\"Confusion\").size()#nada\n",
    "curiosity= self_report.groupby(\"Curiosity\").size()#nada\n",
    "delight = self_report.groupby(\"Delight\").size()#nada\n",
    "flow= self_report.groupby(\"flow/engagement\").size()#nada\n",
    "frustration= self_report.groupby(\"Frustration\").size()#nada\n",
    "tenderness= self_report.groupby(\"Tenderness\").size()#nada\n",
    "joy= self_report.groupby(\"Joy\").size()#1\n",
    "\n",
    "#print(anger,stress,disgust,fear,sadness,surprise,hapiness,pleasant,unpleasant,anxiety,neutral)\n",
    "#print(unpleasant,funny,horror,weepy,boredom,relaxation,amusement,curiosity,delight,flow,frustration,tenderness,joy)\n",
    "\n",
    "#Tiempo de elicitación \n",
    "\n",
    "emotion_elicitation.groupby('elicitation_duration').size()\n",
    "emotion_elicitation.groupby('elicitation_duration_range').size()\n",
    "emotion_elicitation.groupby('elicitation_duration_mean').size()\n",
    "\n",
    "\n",
    "#Excluyo columnas, podría haber condensado todo en una linea de codigo ?, no es necesario igual esta exclusión\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != \"Affective model\"]\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != \"apa_citation\"]\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != \"model_id\"]\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != \"paper_id\"]\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != 'class_model_output_number']\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != 'class_model_output_categories']\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != 'public_code_location']\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != 'is_public_code']\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != 'model_interpretation']\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != 'is_physiologicall_interpretation']\n",
    "algoritmo= algoritmo.loc[:,algoritmo.columns != 'n_model_input']\n",
    "a = algoritmo.columns; a = pd.DataFrame(a)\n",
    "\n",
    "#PIBE CREATE UNA FUNCIÓNNNN\n",
    "pnn = algoritmo.groupby('PNN probabilistic neural network').size(); lstm = algoritmo.groupby('LSTM.1').size()\n",
    "print(pnn,lstm)# NO HAY NADA\n",
    "recurrent= algoritmo.groupby('Recurrent NN.1').size(); convulution= algoritmo.groupby('Convolutional NN.1').size()\n",
    "print(recurrent,convulution)# 4 y 4\n",
    "full= algoritmo.groupby('Fully connected NN (or Multi layer perceptron).1').size(); regre=algoritmo.groupby('regre_Boosted regression trees').size()\n",
    "print(full,regre) # 3 y 2\n",
    "multi = algoritmo.groupby('regre_Multilayer regression').size(); dec=algoritmo.groupby('regre_decision tree').size()\n",
    "print(multi,dec)#2 y 6\n",
    "knn=algoritmo.groupby('regre_knn').size(); rr=algoritmo.groupby('regre_Random Regression').size()\n",
    "print(knn,rr)# 4 y 15\n",
    "lr= algoritmo.groupby('regre_Logistic regression').size(); rrr=algoritmo.groupby('regre_Ridge regression').size()\n",
    "print(lr,rrr)#2 y 2\n",
    "rpl=algoritmo.groupby('regre_Polynomial Regression').size(); rsvm=algoritmo.groupby('regre_Support Vector Regression (SVR)').size()\n",
    "print(rpl,rsvm)# 1 y 4\n",
    "regre_inr=algoritmo.groupby('regre_inear Regression').size(); jrip=algoritmo.groupby('class_JRip').size()\n",
    "print(regre_inr,jrip)# 6 y 3\n",
    "# dejo afuera: 'regre_model_output_number', 'regre_model_output_dimensions'\n",
    "rule= algoritmo.groupby('class_1R rule').size(); rbf= algoritmo.groupby('class_radial basis function\\n(RBF) ').size()\n",
    "print(rule,rbf)# 1 y 2\n",
    "sdbn= algoritmo.groupby('Spiking Deep Belief Network (SDBN)').size() ; pnn2 = algoritmo.groupby('Probabilistic Neural Network (PNN)').size()\n",
    "print(sdbn,pnn2)# 2 y 12\n",
    "qnn= algoritmo.groupby('Quantum Neural Network (QNN)').size() ; dt= algoritmo.groupby('AdaBoost DT').size()\n",
    "print(qnn,dt) # 2 y 11\n",
    "cnn= algoritmo.groupby('Cellular Neural Networks').size() ; lstm2= algoritmo.groupby('LSTM').size()\n",
    "print(cnn,lstm2) # 1 y 6 \n",
    "gru = algoritmo.groupby('GRU (Gated Recurrent Units)').size(); rnn2 = algoritmo.groupby('Recurrent NN').size()\n",
    "print(gru,rnn2)# 2 y 0\n",
    "cnn2= algoritmo.groupby('Convolutional NN').size(); fullnn=algoritmo.groupby('Fully connected NN (or Multi layer perceptron)').size()\n",
    "print(cnn2,fullnn)#28 y 20\n",
    "#dejo afuera 'Fully connected Layer?'\n",
    "grboost= algoritmo.groupby('Gradient boostingclass_').size(); hmm= algoritmo.groupby('class_HMM').size()\n",
    "print(grboost,hmm)# 21 y 3\n",
    "hdc= algoritmo.groupby('class_HDC-MER').size(); tpa= algoritmo.groupby('class_TPA').size()\n",
    "print(hdc,tpa) # 2 y 0\n",
    "var= algoritmo.groupby('class_Ridge-VAR').size(); lasso_var= algoritmo.groupby('class_LASSO-VAR').size()\n",
    "print(var,lasso_var)#0 y 0\n",
    "ridget=algoritmo.groupby('class_Ridge-T').size(); class_lasso=algoritmo.groupby('class_LASSO-T').size()\n",
    "print(ridget,class_lasso)# 0 y 0\n",
    "avg = algoritmo.groupby('class_AVG').size(); bayes= algoritmo.groupby('class_Naive Bayes').size()\n",
    "print(avg,bayes)#0 y 31\n",
    "tree= algoritmo.groupby('class_Tree based models').size(); lda2= algoritmo.groupby('class_Linear Discriminant Analysis (LDA)').size()\n",
    "print(tree,lda2)#57 y 18\n",
    "qda2= algoritmo.groupby('class_Quadratic discrimant clasifier (QDA)').size(); knn2= algoritmo.groupby('class_k-Nearest Neighbor (k-NN)').size()\n",
    "print(qda2,knn2)#11 y 77\n",
    "svm2= algoritmo.groupby('class_Support Vector Machine (SVM)').size(); lr2=algoritmo.groupby('class_Logistic Regression').size()\n",
    "print(svm2,lr2)#132 y 10\n",
    "\n",
    "bp = algoritmo.groupby(\"Backpropagation (BP)\").size()\n",
    "elm= algoritmo.groupby(\"Extreme Learning Machine (ELM)\").size()\n",
    "ann = algoritmo.groupby(\"ANN\").size()\n",
    "print(bp,elm,ann)\n",
    "\n",
    "algoritmo = pd.read_excel(\"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/algoritmo.xlsx\")\n",
    "algoritmo= algoritmo.sort_values(\"fr\",ascending=False)\n",
    "\n",
    "#creo archivo con los nombres\n",
    "#al= pd.ExcelWriter(\"algoritmo.xlsx\")\n",
    "#a.to_excel(al, \"modelo\")\n",
    "#al.save()\n",
    "#al.close()\n",
    "\n",
    "#Interpretación del modelo\n",
    "model_interpretation=model_interpretation.drop_duplicates(subset='paper_id')\n",
    "model_interpretation = model_interpretation[model_interpretation['model_interpretation'].notna()]\n",
    "model_interpretation= model_interpretation['model_interpretation']\n",
    "#model_interpretation.astype(str)# me aseguro que me lo tome todo como string\n",
    "model_interpretation= model_interpretation.to_numpy().transpose().tolist() # aca transformo todo a una lista de listas\n",
    "\n",
    "#creo un documento word\n",
    "#document = Document()\n",
    "#document.add_heading('Documento creado con Python', 0)\n",
    "#p = document.add_paragraph(model_interpretation)\n",
    "#document.save('Model interpretation.docx')\n",
    "\n",
    "#Mapa mundo\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2010]\n",
    "uno=mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2011]\n",
    "dos= mapa.groupby('first_author_country_affiliation').size()\n",
    "print(tres)\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2012]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2013]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2014]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2015]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2016]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2017]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "print(tres)\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2018]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2019]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "\n",
    "mapa=df_sin_duplicado[df_sin_duplicado[\"year\"]==2020]\n",
    "tres= mapa.groupby('first_author_country_affiliation').size()\n",
    "\n",
    "h =list (pycountry.countries)\n",
    "\n",
    "pycountry.countries.get(name=\"Italy\") # ITA\n",
    "pycountry.countries.get(name=\"lithuania\")#LTU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicio Graficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evolución del modelo\n",
    "\n",
    "g = sns.FacetGrid(grafico_modelo, col='Modelos', hue='Modelos', col_wrap=1)\n",
    "\n",
    "# Add the line over the area with the plot function\n",
    "g = g.map(plt.plot, 'Año', \"Frecuencia\")\n",
    " \n",
    "# Fill the area with fill_between\n",
    "g = g.map(plt.fill_between, 'Año', \"Frecuencia\", alpha=0.2).set_titles(\"{col_name} Año\")\n",
    " \n",
    "# Control the title of each facet\n",
    "g = g.set_titles(\"{col_name}\")\n",
    " \n",
    "# Add a title for the whole plot\n",
    "plt.subplots_adjust(top=0.90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Tipo de base de datos\n",
    "\n",
    "\n",
    "counts = [12,9,10,2,1,2,1,2,1,1,3,1,1,1,1,1] #agregada info de casos particulares\n",
    "names = [\"DEAP\", \"AMIGOS\", \"MANHOB\", \"ASCERTAIN\", \"MMDB\", \"RECOLA\", \"DECAF\", \"DRIVING WORKLOAD\", \"AV+EC\", \"LIRIS\", \"PMEMO\", \"AFEW\", \"HAZUMI1911\", \"BIO VID\", \"RCDAT\", \"Cog.load\" ]\n",
    "y = [i * 0.9 for i in range(len(names))]\n",
    "BLUE = \"#076fa2\"\n",
    "RED = \"#E3120B\"\n",
    "BLACK = \"#202020\"\n",
    "GREY = \"#a2a2a2\"\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.barh(y, counts, height=0.55, align=\"edge\", color=BLUE);\n",
    "\n",
    "#personalizacion\n",
    "\n",
    "ax.xaxis.set_ticks([i * 5 for i in range(0, 12)])\n",
    "ax.xaxis.set_ticklabels([i * 5 for i in range(0, 12)], size=16, fontfamily=\"Econ Sans Cnd\", fontweight=100)\n",
    "ax.xaxis.set_tick_params(labelbottom=False, labeltop=True, length=0)\n",
    "\n",
    "ax.set_xlim((0, 55.5))\n",
    "ax.set_ylim((0, len(names) * 0.9 - 0.2))\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(axis = \"x\", color=\"#A8BAC4\", lw=1.2)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_lw(1.5)\n",
    "ax.spines[\"left\"].set_capstyle(\"butt\")\n",
    "ax.yaxis.set_visible(False)\n",
    "\n",
    "#agregado de etiqueta\n",
    "PAD = 0.3\n",
    "for name, count, y_pos in zip(names, counts, y):\n",
    "    x = 0\n",
    "    color = \"white\"\n",
    "    path_effects = None\n",
    "    if count < 8:\n",
    "        x = count\n",
    "        color = BLUE    \n",
    "        path_effects=[withStroke(linewidth=6, foreground=\"white\")]\n",
    "    \n",
    "    ax.text(\n",
    "        x + PAD, y_pos + 0.5 / 2, name, \n",
    "        color=color, fontfamily=\"Econ Sans Cnd\", fontsize=18, va=\"center\",\n",
    "        path_effects=path_effects\n",
    "    ) \n",
    "fig\n",
    "\n",
    "#retoque final\n",
    "fig.subplots_adjust(left=0.005, right=1, top=0.8, bottom=0.1)\n",
    "fig.text(\n",
    "    0, 0.925, \"Type Database\", \n",
    "    fontsize=22, fontweight=\"bold\", fontfamily=\"Econ Sans Cnd\"\n",
    ")\n",
    "\n",
    "fig.add_artist(lines.Line2D([0, 1], [1, 1], lw=3, color=RED, solid_capstyle=\"butt\"))\n",
    "fig.add_artist(patches.Rectangle((0, 0.975), 0.05, 0.025, color=RED))\n",
    "fig.set_facecolor(\"white\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensión de los modelos emocionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensiones tomadas en modelos dimensionales\n",
    "\n",
    "names = ['Valence', 'Arousal', 'Dominance', 'Like or Dislike', \"Familiarity\"]\n",
    "size = [21,21,2,2,1]\n",
    "my_circle = plt.Circle( (0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, wedgeprops = { 'linewidth' : 7, 'edgecolor' : 'white' })\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Grafico de categorias \n",
    "# FIJARSE DE HACERLO EN DUPLAS Y VER TEMA DE GRAFICO DE GRAFO\n",
    "categorial = pd.DataFrame ({ \"Name\": [\"Anger\", \"Stress\",\"Disgust\",\"Fear\",\"Sadness\", \"Surprise\",\"Hapiness\", \"Pleasant\",\"Anxiety\", \"Neutral\", \"Funny\", \"Boredom\", \"Relaxation\", \"Joy\"], \"Value\":[1,1,2,2,2,1,1,1,2,3,1,2,1,1]})\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "plt.axis('off')\n",
    "upperLimit = 100\n",
    "lowerLimit = 30\n",
    "labelPadding = 4\n",
    "max = categorial['Value'].max()\n",
    "slope = (max - lowerLimit) / max\n",
    "heights = slope * categorial.Value + lowerLimit\n",
    "width = 2*np.pi / len(categorial.index)\n",
    "\n",
    "indexes = list(range(1, len(categorial.index)+1))\n",
    "angles = [element * width for element in indexes]\n",
    "##\n",
    "bars = ax.bar(\n",
    "    x=angles, \n",
    "    height=heights, \n",
    "    width=width, \n",
    "    bottom=lowerLimit,\n",
    "    linewidth=2, \n",
    "    edgecolor=\"white\",\n",
    "    color=\"#61a4b2\",\n",
    "    )\n",
    "for bar, angle, height, label in zip(bars,angles, heights, categorial[\"Name\"]):\n",
    "\n",
    "    \n",
    "    rotation = np.rad2deg(angle)\n",
    "\n",
    "    \n",
    "    alignment = \"\"\n",
    "    if angle >= np.pi/2 and angle < 3*np.pi/2:\n",
    "        alignment = \"right\"\n",
    "        rotation = rotation + 180\n",
    "    else: \n",
    "        alignment = \"left\"\n",
    "\n",
    "    \n",
    "    ax.text(\n",
    "        x=angle, \n",
    "        y=lowerLimit + bar.get_height() + labelPadding, \n",
    "        s=label, \n",
    "        ha=alignment, \n",
    "        va='center', \n",
    "        rotation=rotation, \n",
    "        rotation_mode=\"anchor\") \n",
    "plt.title(\"Categorys of affective questionarie\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=\"fr\", y=\"name\", data=algoritmo,\n",
    "            label=\"Algoritmos\", color=\"b\")\n",
    "\n",
    "\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(xlim=(0, 24), ylabel=\"\",\n",
    "       xlabel=\"Frequency\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.title(\"Types algorithm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafico frecuencias de palabras\n",
    "# queda por eliminar words: show, smill, proposed, dataset, difference \n",
    "palabras_irrelevantes = get_stop_words (\"english\")\n",
    "print(palabras_irrelevantes)\n",
    "stylecloud.gen_stylecloud(file_path=\"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/model_interpretation.txt\",\n",
    "icon_name=\"fab fa-apple\", palette= \"cartocolors.qualitative.Pastel_3\", background_color=\"black\", output_name=\"Model interpretation.png\", collocations=False, custom_stopwords= palabras_irrelevantes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mundo \n",
    "\n",
    "fig = px.choropleth(world,locations='id', color= \"fr\", scope=\"world\", projection='natural earth',animation_frame= 'year', hover_name = \"country\", color_continuous_scale = \"aggrnyl\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafico de Grafo con las categorias y dimensiones del self-report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graf= \"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/categories_selfreport.xlsx\"\n",
    "\n",
    "grafo = pd.read_excel(graf, sheet_name=\"self\")\n",
    "grafo.columns\n",
    "\n",
    "G = nx.from_pandas_edgelist(grafo,'Name', \"Type\",edge_attr=True)\n",
    "edgelist=nx.to_edgelist(G)\n",
    "colors=[]\n",
    "for node in G:\n",
    "    if node in grafo[\"Type\"].values:\n",
    "        colors.append(\"yellow\")\n",
    "    else:\n",
    "        colors.append(\"red\")\n",
    "#Recordar que la data de Frequency fue alterada, lo de ahora solo tiene valor de prueba, lo mismo aplica hasta donde aclare que no \n",
    "plt.figure(figsize=(20,20))\n",
    "nx.draw(G,with_labels= True,node_color = colors, node_size=[v * 100 for v in dict(G.degree()).values()], width=[v[2][\"Frequency\"]/5 for v in edgelist])\n",
    "ax = plt.gca()\n",
    "ax.collections[0].set_edgecolor(\"#696969\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Probando\n",
    "h =\"C:/Users/martu/OneDrive/Escritorio/Review_EDA_Emotion_Recognition/Lorenzo/categories_selfreport.xlsx\"\n",
    "\n",
    "grafico= pd.read_excel(h, sheet_name=\"matriz\")\n",
    "#grafico = grafico.iloc[:,18:]\n",
    "grafico.columns\n",
    "grafico= grafico[[\"anger\",\"stress\",\"disgust\",\"fear\",\"sadness\",\"surprise\",\"hapiness\",\"pleasant\",\"anxiety\",\"neutral\",\"boredom\",\"relaxation\",\"joy\"]]\n",
    "adj_matrix = grafico.T.dot(grafico)\n",
    "np.fill_diagonal(adj_matrix.values, 0)\n",
    "G = nx.DiGraph(adj_matrix)\n",
    "plt.figure(figsize=(15,15))\n",
    "g = nx.draw_circular(G,with_labels= True, node_size=1500, node_color = \"skyblue\", linewidths= 40,\n",
    "font_size=18,font_color=\"black\", font_weight= \"bold\", width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lo hacemos circular\n",
    "\n",
    "G = nx.from_pandas_edgelist(grafo,'Name', \"Type\",edge_attr=True)\n",
    "\n",
    "for v in G:\n",
    "    print(f\"{v:4} {G.degree(v):6}\")\n",
    "plt.figure(figsize=(15,15))\n",
    "nx.draw_circular(G, with_labels=True,width=[v[2][\"Frequency\"]/5 for v in edgelist])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRAFO CON DATA REAL A PARTIR DE AQUI\n",
    "df_self_report=pd.read_csv('..\\Emmanuel\\data\\cleaned\\Tabla Normalizada - Self report.csv')\n",
    "df_self_report=df_self_report.fillna('-')\n",
    "df_self_report = df_self_report[df_self_report['is_categorial'] == 'x']\n",
    "df_self_report = df_self_report\n",
    "df_matrix = df_self_report.iloc[:,18:]\n",
    "df_matrix = df_matrix[['categorial_anger', 'Stress', 'Disgust', 'Fear', 'Sadness', 'Surprise', 'Happiness','Pleasant', 'Anxiety', 'Neutral', 'Funny', 'Boredom', 'Relaxation', 'Joy']]\n",
    "#hay que cambiar el nombre de la columna categorial_anger \n",
    "df_matrix = df_matrix.replace('-', 0)\n",
    "df_matrix = df_matrix.replace('x', 1)\n",
    "\n",
    "adj_matrix = df_matrix.T.dot(df_matrix)\n",
    "\n",
    "np.fill_diagonal(adj_matrix.values, 0)\n",
    "G = nx.DiGraph(adj_matrix)\n",
    "\n",
    "G.number_of_edges()\n",
    "pos=nx.circular_layout(G)\n",
    "weights=[wt for u, v, wt in G.edges(data=\"weight\")]\n",
    "plt.figure(figsize=(15,15))\n",
    "nx.draw_networkx(G,pos,width=2)\n",
    "labels=nx.get_edge_attributes(G, \"weight\") # entiendo que esto te permite poner la cantidad de frecuencia de cada relación con una etiqueta \n",
    "#plt.title('Grafo de relaciones entre categorias emocionales')\n",
    "#plt.figure(figsize=(10,10))\n",
    "nx.draw_networkx_edge_labels(G,pos,edge_labels=labels,font_size=18, font_color=\"black\", font_weight=\"bold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distingo las relaciones por colores\n",
    "\n",
    "G = nx.DiGraph(adj_matrix)\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "labels=nx.get_edge_attributes(G, \"weight\")\n",
    "weights=[wt for u, v, wt in G.edges(data=\"weight\")]\n",
    "colours = \"red\"\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Grafo de relaciones entre categorias emocionales')\n",
    "values = range(35)\n",
    "jet = cm = plt.get_cmap('jet') \n",
    "cNorm  = colors.Normalize(vmin=0, vmax=values[-1])\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)\n",
    "colorList = []\n",
    "\n",
    "for i in range(35):\n",
    "    colorVal = scalarMap.to_rgba(values[i])\n",
    "    colorList.append(colorVal)\n",
    "\n",
    "\n",
    "g = nx.draw_circular(G, with_labels=True, node_size=1500, node_color=\"skyblue\",edge_color=colorList,edge_cmap=plt.cm.jet,linewidths=40,arrows=True,\n",
    "                    font_size=18, font_color=\"black\", font_weight=\"bold\", width=10)\n",
    "#plt.savefig('edges.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Las relaciones fuertes en rojo, las debiles en azul\n",
    "G = nx.DiGraph(adj_matrix)\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "labels=nx.get_edge_attributes(G, \"weight\")\n",
    "weights=[wt for u, v, wt in G.edges(data=\"weight\")]\n",
    "colours = \"red\"\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Grafo de relaciones entre categorias emocionales')\n",
    "values = range(35)\n",
    "jet = cm = plt.get_cmap('jet') \n",
    "cNorm  = colors.Normalize(vmin=0, vmax=values[-1])\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)\n",
    "colorList = []\n",
    "\n",
    "for i in range(35):\n",
    "    colorVal = scalarMap.to_rgba(values[i])\n",
    "    colorList.append(colorVal)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "g = nx.draw_circular(G, with_labels=True, node_size=1500, node_color=\"paleturquoise\",edge_color=weights,edge_cmap=plt.cm.jet,linewidths=40,arrows=True,\n",
    "                    font_size=18, font_color=\"black\", font_weight=\"bold\", width=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinción de la frecuencia por la intensidad de los bordes\n",
    "\n",
    "labels=nx.get_edge_attributes(G, \"weight\")\n",
    "weights=[wt for u, v, wt in G.edges(data=\"weight\")]\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Grafo de relaciones entre categorias emocionales')\n",
    "\n",
    "g = nx.draw_circular(G, with_labels=True, node_size=1500, node_color=\"skyblue\",edge_color=weights,edge_cmap=plt.cm.Blues,linewidths=40,\n",
    "                    font_size=18, font_color=\"black\", font_weight=\"bold\", width=10) # el arrow no me funcionó"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2e637689e3f8b5962031bb00bd419bdecd681b5fd76726fb03d874837e7027d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
