{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis extra\n",
    "Acá se analizan los datos por fuera de los requerimientos principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se procede con importar librerias y datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerias a usar\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "from pathlib import Path  \n",
    "\n",
    "#Creación de data frames a usar\n",
    "df_metadata = pd.read_csv(\"c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - Metadata.csv\")\n",
    "df_data_type = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - Data type.csv')\n",
    "df_participants = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - Participants.csv')\n",
    "df_self_report = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - Self report.csv')\n",
    "df_emotion_elicitation_techniques = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - Emotion elicitation techniques.csv')\n",
    "df_eda = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - EDA.csv')\n",
    "df_statistical_learning_models = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - Statistical Learning model.csv')\n",
    "df_performances = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - Performances.csv')\n",
    "df_alg_perf = pd.read_csv('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Tabla Normalizada - Alg_Perf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Frecuencia de papers por tipo de source (conference, journal, pre-print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata=df_metadata.fillna('-')\n",
    "df_metadata_sin_duplicates = df_metadata.drop_duplicates(subset='paper_id')\n",
    "\n",
    "df_sources = df_metadata_sin_duplicates.iloc[:,7:10]\n",
    "def get_value(row):\n",
    "     for c in df_sources.columns:\n",
    "         if row[c]== 'x':\n",
    "             return c\n",
    "\n",
    "df_sources = df_sources.apply(get_value, axis=1)\n",
    "df_sources = pd.DataFrame(df_sources)\n",
    "df_sources.columns = ['Source type']\n",
    "\n",
    "\n",
    "#ploteo\n",
    "quantity = df_sources['Source type'].value_counts()\n",
    "df_quantity = pd.DataFrame(quantity)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "sns.countplot(x='Source type', data=df_sources)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#pie plot\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "names = 'Journal', 'Conference', 'Pre-print'\n",
    "plt.pie(df_quantity['Source type'], labels = names, labeldistance = 1.15, wedgeprops = { 'linewidth' : 3, 'edgecolor' : 'white' })\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Gráfico de barras papers por país y continente\n",
    "Interpretación: primacía de trabajos provenientes de China y Asia. Seguidos estos por trabajos europeos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises = df_metadata_sin_duplicates[\"first_author_country_affiliation\"].unique()\n",
    "\n",
    "countries = df_metadata_sin_duplicates.pivot_table(columns=['first_author_country_affiliation'], aggfunc='size')\n",
    "df_countries = pd.DataFrame(countries)\n",
    "\n",
    "order = ['China','USA', 'Germany', 'India','Turkey','Italy', 'Malaysia','Spain','Iran', 'Switzerland','Romania','Pakistan', 'Taiwan','Greece', 'Japan',\n",
    "'Austria', 'Tunisia','Macedonian', 'Finland', 'Slovenia', 'Portugal', 'Korea',\n",
    "'UK', 'Indonesia','Canada', 'France', 'Lithuania', \n",
    "'Egypt','Colombia', 'Australia', 'Poland']\n",
    "\n",
    "df_countries.loc[order].plot(kind='bar', title='Cantidad de papers por país', xlabel='country', ylabel='paper quantity')\n",
    "plt.show()\n",
    "\n",
    "#Papers por continente - plot\n",
    "papers_continents = {'continents' : ['Asia', 'Europa','America','Africa','Australia'],\n",
    "'quantity' : [49, 39, 9, 3, 1]}\n",
    "df_continents = pd.DataFrame(papers_continents)\n",
    "print(df_continents)\n",
    "\n",
    "df_continents.set_index('continents').plot(kind='bar', title='Cantidad de papers por continente', xlabel='continent', ylabel='paper quantity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pycountry.countries.get(name=\"spain\")#ESP\n",
    "\n",
    "#df_countries.loc[order].plot(kind='bar', title='Cantidad de papers por país', xlabel='country', ylabel='paper quantity')\n",
    "#plt.show()\n",
    "\n",
    "#df_paises = pd.read_excel('c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\EMMA\\\\data\\\\cleaned\\\\Paises.xlsx')\n",
    "df_countries1 = df_countries.index\n",
    "\n",
    "def get_alpha_3(location):\n",
    "    try:\n",
    "        return pycountry.countries.get(name=location).alpha_3\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_countries.reset_index(inplace = True, drop = True)\n",
    "df_countries['country'] = df_countries1\n",
    "\n",
    "df_countries = df_countries.replace('Korea','Korea, Republic of')\n",
    "df_countries = df_countries.replace('Iran','Iran, Islamic Republic of')\n",
    "df_countries = df_countries.replace('UK','United Kingdom')\n",
    "df_countries = df_countries.replace('USA','United States')\n",
    "df_countries = df_countries.replace('Macedonian','North Macedonia')\n",
    "\n",
    "df_countries[\"code\"] = df_countries[\"country\"].apply(lambda x: get_alpha_3(x))\n",
    "\n",
    "fig = px.choropleth(df_countries,\n",
    "                    locations=\"code\",\n",
    "                    color=0,\n",
    "                    hover_name=\"country\",\n",
    "                    title = \"Cantidad de papers por pais\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#CSV archive to Tableau\n",
    "filepath = Path(\"c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Countries.csv\")  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_countries.to_csv(filepath)\n",
    "\n",
    "filepath = Path(\"c:\\\\Users\\\\LENOVO\\\\Downloads\\\\Review_EDA_Emotion_Recognition\\\\Emmanuel\\\\data\\\\cleaned\\\\Continents.csv\")  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_continents.to_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. Grafico base de datos, privadas y composicion de publicas\n",
    "\n",
    "Incluir en un grafico, dentro de las bases de datos publicas, cada base de dato publica (AMIGOS, MAHNOB, etc.)\n",
    "\n",
    "* cantidad total bases de datos: 101 (igual que la cantidad de papers, suponiendo que es solo un uso de base de datos por paper, que los papers 37 y 62 no siguen)\n",
    "    * db privada: 58\n",
    "    * db databases: 38\n",
    "        * DEAP 10\n",
    "        * MAHNOB 7\n",
    "        * AMIGOS 7\n",
    "        * PMEmo 3\n",
    "        * Ascertein 2\n",
    "        * RECOLA 2\n",
    "        * Otros 7\n",
    "    * db publica: 3\n",
    "    * db uppon request: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la divergencia de los papers 37 y 62\n",
    "\n",
    "* cantidad total bases de datos: 107 (101 + 6)\n",
    "    * db privada: 58\n",
    "    * db databases: 44\n",
    "        * DEAP 11\n",
    "        * MAHNOB 9\n",
    "        * AMIGOS 8\n",
    "        * PMEmo 3\n",
    "        * Ascertein 2\n",
    "        * RECOLA 2\n",
    "        * Otros 9\n",
    "    * db publica: 3\n",
    "    * db uppon request: 1\n",
    "\n",
    "Gráfico realizado en Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rellenar datos faltantes y dropeo de duplicados\n",
    "df_data_type=df_data_type.fillna('-')\n",
    "df_data_type_sin_duplicates = df_data_type.drop_duplicates(subset='paper_id')\n",
    "\n",
    "df_db_types = df_data_type_sin_duplicates.iloc[:,6:11]\n",
    "print(df_db_types)\n",
    "def get_value(row):\n",
    "     for c in df_db_types.columns:\n",
    "         if row[c]== 'x':\n",
    "             return c\n",
    "\n",
    "df_db_types = df_db_types.apply(get_value, axis=1)\n",
    "df_db_types = pd.DataFrame(df_db_types)\n",
    "df_db_types.columns = ['Database type']\n",
    "\n",
    "#ploteo\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "plt.rcParams[\"legend.fontsize\"] = 15\n",
    "plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette('colorblind')\n",
    "g = sns.countplot(x='Database type', data=df_db_types)\n",
    "g.set(title = 'Frecuencia de tipo de base de datos', xlabel = 'Tipo de base de datos', ylabel = 'Cantidad')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(df_db_types.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d. Análisis estadístico para determinar si modelos de detección de arousal performan mejor que los basados en valence (Algoritmos de regresión)\n",
    "* Procedimiento: subsetear para quedarnos con modelos dimensionales(columna affective model), subsetear con regressor, donde nos quedamos solo con las dimensiones que sean arousal/valence, quedarse con la medida de perforrmace que mas aparezca (count), hacer el test estadístico correspondiente (t, wettney, etc)\n",
    "* Resultado: no existe diferencia estadisticamente significativa entre grupos, por lo que los algoritmos de regresión basados en modelos dimensionales (basados en arousal o valence) no performan mejor uno sobre otros (Segun t de student y u de mann-whitney).\n",
    "* Tener en cuenta el tamaño de la muestra que cumple con todos los criterios mencionados en el Procedimiento: 16 modelos en total.\n",
    "* **Revision**, la obtencion con get_value no funciona, de todas formas RMSE es el algoritmo mas usado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion del data frames, y subseteo por: modelos dimensionales, tipo regressor, dimensiones arousal/valence\n",
    "df = df_alg_perf\n",
    "df = df[df['affective_model'] == 'dimensional']\n",
    "df = df[df['is_regressor'].isin(['x', 'X'])]\n",
    "df = df[df['regre_model_output_dimensions'].isin(['arousal', 'valence'])]\n",
    "\n",
    "#que medida de performance mas aparece?\n",
    "#dataframe con performances\n",
    "df_performance_medidas = df.iloc[:,58:]\n",
    "\n",
    "print(df_performance_medidas)\n",
    "\n",
    "#busqueda de la medida de performance mas recurrente, paso de los valores a 0 y 1 para facilitar su conteo\n",
    "df_performance_medidas = df_performance_medidas.fillna('No')\n",
    "df_performance_medidas = df_performance_medidas.replace('-', 'No')\n",
    "df_performance_medidas=df_performance_medidas.mask(df_performance_medidas != 'No','Yes')\n",
    "\n",
    "#obtener performance mas frecuente\n",
    "def get_value(row):\n",
    "    for c in df_performance_medidas.columns:\n",
    "        if row[c] == 'Yes':\n",
    "            return c\n",
    "\n",
    "df_performance_medidas = df_performance_medidas.apply(get_value, axis=1)\n",
    "df_performance_medidas = pd.DataFrame(df_performance_medidas)\n",
    "df_performance_medidas.columns = ['Performances']\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context = 'notebook')\n",
    "sns.countplot(x='Performances', data=df_performance_medidas)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#subseteo por la medida de performance que mas aparece (RMSE)\n",
    "df = df.fillna('-')\n",
    "df = df[df['Root-Mean-Square-Error-(RMSE)'] != '-']\n",
    "\n",
    "print(df)\n",
    "#la muestra es de 16 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_arousal = df[df['regre_model_output_dimensions'] == 'arousal']\n",
    "df_arousal = df_arousal['Root-Mean-Square-Error-(RMSE)']\n",
    "arousal = df_arousal.values.tolist()\n",
    "arousal = list(map(float, arousal))\n",
    "\n",
    "df_valence = df[df['regre_model_output_dimensions'] == 'valence']\n",
    "df_valence = df_valence['Root-Mean-Square-Error-(RMSE)']\n",
    "valence = df_valence.values.tolist()\n",
    "valence = list(map(float, valence))\n",
    "\n",
    "#Test parametrico - t de student\n",
    "print('Students t-test')\n",
    "from scipy.stats import ttest_ind\n",
    "stat, p = ttest_ind(arousal, valence, alternative = 'greater')\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')\n",
    "\n",
    "\n",
    "#Test no parametrico - U de Mann-Whitney\n",
    "print('Mann-Whitney U Test')\n",
    "from scipy.stats import mannwhitneyu\n",
    "stat, p = mannwhitneyu(arousal, valence, alternative = 'greater')\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (obsoleto) Cambios, teniendo en cuenta los resultados obtenidos en el analisis exploratorio y estadistico anterior\n",
    "\n",
    "* Se partia del analisis hecho primeramente, sobre que la medida mas usada era UAR, y los papers sacados aca no corresponden con los que podrian ser ahora que se sabe cual es la mas frecuente (accuracy). Para actualizarse esta parte debe tenerse en cuenta eso.\n",
    "* Nos quedamos solo con los modelos de clasificacion, y tenemos en cuenta los papers que posean ambos (High y Low para arousal y valencia)\n",
    "* Solamente se dropearon dos papers, que testeaban solo modelos o de valencia o de arousal. Con esto se prosiguió con el análisis estadístico y los resultados fueron los mismos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "papers_titles = title_dimensions.drop_duplicates(subset='apa_citation')\n",
    "\n",
    "print(title_dimensions)\n",
    "#Kołodziej, M., Tarnowski, P., Majkowski, A., &...   vuela\n",
    "#Greco, A., Marzi, C., Lanata, A., Scilingo, E.\n",
    "\n",
    "df_arousal = df_3[df_3['class_model_output_categories'] == 'HA, LA']\n",
    "df_arousal = df_arousal['accuracy']\n",
    "arousal = df_arousal.values.tolist()\n",
    "arousal = list(map(float, arousal))\n",
    "\n",
    "df_valence = df_3[df_3['class_model_output_categories'] == 'HV, LV']\n",
    "df_valence = df_valence['accuracy']\n",
    "valence = df_valence.values.tolist()\n",
    "valence = list(map(float, valence))\n",
    "\n",
    "#Test parametrico - t de student\n",
    "print('Students t-test')\n",
    "from scipy.stats import ttest_ind\n",
    "stat, p = ttest_ind(arousal, valence, alternative = 'greater')\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')\n",
    "\n",
    "\n",
    "#Test no parametrico - U de Mann-Whitney\n",
    "print('Mann-Whitney U Test')\n",
    "from scipy.stats import mannwhitneyu\n",
    "stat, p = mannwhitneyu(arousal, valence, alternative = 'greater')\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')\n",
    "Lo mismo pero con HAHV, HALV, LAHV, LALV\n",
    "#creacion data frame y subseteo por: modelos dimensionales, tipo clasiffier, categorias HA/LA y HV/LV\n",
    "df_hvlv_hala = df_alg_perf\n",
    "df_hvlv_hala = df_hvlv_hala[df_hvlv_hala['affective_model'] == 'dimensional']\n",
    "df_hvlv_hala = df_hvlv_hala[df_hvlv_hala['is_classifier'].isin(['x', 'X'])]\n",
    "df_hvlv_hala = df_hvlv_hala[df_hvlv_hala['class_model_output_categories'].isin(['HAHV, HALV, LAHV, LALV'])]\n",
    "\n",
    "#que medida de performance mas aparece en este grupo?\n",
    "#dataframe con performances\n",
    "title_dimensions_hvlv_hala = df_hvlv_hala[['apa_citation', 'class_model_output_categories']]\n",
    "\n",
    "df_hvlv_hala_performances = df_hvlv_hala.iloc[:,58:]\n",
    "df_hvlv_hala_performances = df_hvlv_hala_performances.fillna('-')\n",
    "\n",
    "#subseteo por la medida de performance que mas aparece (accuracy)\n",
    "df_hvlv_hala = df_hvlv_hala[df_hvlv_hala['accuracy'] != '-']\n",
    "\n",
    "print(df_hvlv_hala)\n",
    "df_ayv = df_hvlv_hala[df_hvlv_hala['class_model_output_categories'] == 'HAHV, HALV, LAHV, LALV']\n",
    "df_ayv = df_ayv['accuracy']\n",
    "hvlv_hala = df_ayv.values.tolist()\n",
    "hvlv_hala = list(map(float, hvlv_hala))\n",
    "\n",
    "print(hvlv_hala)\n",
    "\n",
    "#Test parametrico - t de student\n",
    "print('Students t-test')\n",
    "from scipy.stats import ttest_ind\n",
    "stat, p = ttest_ind(arousal, valence, alternative = 'greater')\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')\n",
    "\n",
    "\n",
    "#Test no parametrico - U de Mann-Whitney\n",
    "print('Mann-Whitney U Test')\n",
    "from scipy.stats import mannwhitneyu\n",
    "stat, p = mannwhitneyu(arousal, valence, alternative = 'greater')\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e31aef8222fb7c235d2ed8e74ce17e973738f89b37261e7466b7a63a6dfb1214"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
