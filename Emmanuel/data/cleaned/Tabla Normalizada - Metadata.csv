paper_id,model_id,apa_citation,authors,title,year,source_title,source_type_journal,source_type_conference,source_type_preprint,first_author_country_affiliation
1,1,"Zangróniz, R., Martínez-Rodrigo, A., Pastor, J. M., López, M. T., & Fernández-Caballero, A. (2017). Electrodermal Activity Sensor for Classification of Calm/Distress Condition. Sensors (Basel, Switzerland), 17(10), E2324. https://doi.org/10.3390/s17102324
","Roberto Zangróniz, Arturo Martínez-Rodrigo, José Manuel Pastor, María T. López and Antonio Fernández-Caballero",Electrodermal Activity Sensor for Classification of Calm/Distress Condition,2017,Sensors,x,-,,Spain
2,2,"Liu, M., Fan, D., Zhang, X., & Gong, X. (2017). Human Emotion Recognition Based on Galvanic Skin Response Signal Feature Selection and SVM. 157–160. Scopus. https://doi.org/10.1109/ICSCSE.2016.0051
","Mingyang Liu, Di Fan, Xiaohan Zhang, Xiaopeng Gong",Human Emotion Recognition Based on Galvanic Skin Response Signal Feature Selection and SVM,2016,2016 International Conference on Smart City and Systems Engineering,-,x,,China
3,3,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
","Ayata, D., Yaslan, Y., & Kamasak, M. E.
",Emotion Based Music Recommendation System Using Wearable Physiological Sensors,2018,IEEE Transactions on Consumer Electronics,x,-,,Turkey
3,4,"Ayata, D., Yaslan, Y., & Kamasak, M. E. (2018). Emotion Based Music Recommendation System Using Wearable Physiological Sensors. IEEE Transactions on Consumer Electronics, 64(2), 196–203. Scopus. https://doi.org/10.1109/TCE.2018.2844736
","Ayata, D., Yaslan, Y., & Kamasak, M. E.
",Emotion Based Music Recommendation System Using Wearable Physiological Sensors,2018,IEEE Transactions on Consumer Electronics,x,-,,Turkey
4,5,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061905","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,Austria
4,6,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061906","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,Austria
4,7,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061907","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,Austria
4,8,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061908","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,Austria
4,9,"Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K. (2018). A Globally Generalized Emotion Recognition System Involving Different Physiological Signals. Sensors (Basel, Switzerland), 18(6), E1905. https://doi.org/10.3390/s18061909","Ali, M., Machot, F. A., Mosa, A. H., Jdeed, M., Machot, E. A., & Kyamakya, K.",A Globally Generalized Emotion Recognition System Involving Different Physiological Signals,2018,Sensors,x,-,,Austria
5,10,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384","Wei, J., Chen, T., Liu, G., & Yang, J.",Higher-order Multivariable Polynomial Regression to Estimate Human Affective States.,2016,Scientific Reports,x,-,,China
5,11,"Wei, J., Chen, T., Liu, G., & Yang, J. (2016). Higher-order Multivariable Polynomial Regression to Estimate Human Affective States. Scientific Reports, 6, 23384. https://doi.org/10.1038/srep23384","Wei, J., Chen, T., Liu, G., & Yang, J.",Higher-order Multivariable Polynomial Regression to Estimate Human Affective States.,2016,Scientific Reports,x,-,,China
6,12,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,USA
6,13,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,USA
6,14,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,USA
6,15,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,USA
6,16,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,USA
6,17,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,USA
6,18,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,USA
6,19,"Feng, H., Golshan, H. M., & Mahoor, M. H. (2018). A wavelet-based approach to emotion classification using EDA signals. Expert Systems with Applications, 112, 77–86. Scopus. https://doi.org/10.1016/j.eswa.2018.06.014","Feng, H., Golshan, H. M., & Mahoor, M. H.",A wavelet-based approach to emotion classification using EDA signals.,2018,Expert Systems with Applications,x,-,,USA
7,20,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,21,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,22,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,23,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,24,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,25,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,26,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,27,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,28,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,29,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,30,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,31,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,32,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,33,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,34,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,35,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,36,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,37,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,38,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
7,39,"Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K. (2018). Introducing WeSAD, a multimodal dataset for wearable stress and affect detection. 400–408. Scopus. https://doi.org/10.1145/3242969.3242985","Schmidt, P., Reiss, A., Duerichen, R., & Van Laerhoven, K","Introducing WeSAD, a multimodal dataset for wearable stress and affect detection",2018,ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction,-,x,,Germany
8,40,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180","Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G.",A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices.,2018,"2018 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2018",-,x,,UK
8,41,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180","Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G.",A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices.,2018,"2018 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2018",-,x,,UK
8,42,"Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G. (2018). A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices. 306–311. Scopus. https://doi.org/10.1109/PERCOMW.2018.8480180","Dobbins, C., Fairclough, S., Lisboa, P., & Navarro, F. F. G.",A Lifelogging Platform Towards Detecting Negative Emotions in Everyday Life using Wearable Devices.,2018,"2018 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2018",-,x,,UK
9,43,"Amalan, S., Shyam, A., Anusha, A. S., Preejith, S. P., Tony, A., Jayaraj, J., & Mohanasankar, S. (2018). Electrodermal Activity based Classification of Induced Stress in a Controlled Setting. MeMeA 2018 - 2018 IEEE International Symposium on Medical Measurements and Applications, Proceedings. Scopus. https://doi.org/10.1109/MeMeA.2018.8438703","Amalan, S., Shyam, A., Anusha, A. S., Preejith, S. P., Tony, A., Jayaraj, J., & Mohanasankar, S.",Electrodermal Activity based Classification of Induced Stress in a Controlled Setting.,2018,"MeMeA 2018 - 2018 IEEE International Symposium on Medical Measurements and Applications, Proceedings",-,x,,India
10,44,"Machot, F. A., Ali, M., Ranasinghe, S., Mosa, A. H., & Kyandoghere, K. (2018). Improving subject-independent human emotion recognition using electrodermal activity sensors for active and assisted living. 222–228. Scopus. https://doi.org/10.1145/3197768.3201523","Machot, F. A., Ali, M., Ranasinghe, S., Mosa, A. H., & Kyandoghere, K",Improving subject-independent human emotion recognition using electrodermal activity sensors for active and assisted living.,2018,ACM International Conference Proceeding Series,-,x,,Austria
11,45,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589","Girardi, D., Lanubile, F., & Novielli, N.",Emotion detection using noninvasive low cost sensors.,2018,"2017 7th International Conference on Affective Computing and Intelligent Interaction, ACII 2017",-,x,,Italy
11,46,"Girardi, D., Lanubile, F., & Novielli, N. (2018). Emotion detection using noninvasive low cost sensors. 2018-January, 125–130. Scopus. https://doi.org/10.1109/ACII.2017.8273589","Girardi, D., Lanubile, F., & Novielli, N.",Emotion detection using noninvasive low cost sensors.,2018,"2017 7th International Conference on Affective Computing and Intelligent Interaction, ACII 2017",-,x,,Italy
12,47,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076","Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z.","Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions.",2018,International Journal of Medical Engineering and Informatics,x,-,,Malaysia
12,48,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076","Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z.","Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions.",2018,International Journal of Medical Engineering and Informatics,x,-,,Malaysia
12,49,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076","Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z.","Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions.",2018,International Journal of Medical Engineering and Informatics,x,-,,Malaysia
12,50,"Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z. (2018). Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions. International Journal of Medical Engineering and Informatics, 10(1), 16–29. Scopus. https://doi.org/10.1504/IJMEI.2018.090076","Ooi, J. S. K., Ahmad, S. A., Ishak, A. J., Minhad, K. N., Md Ali, S. H., & Chong, Y. Z.","Grove: An auxiliary device for sympathetic assessment via EDA measurement of neutral, stress, and anger emotions during simulated driving conditions.",2018,International Journal of Medical Engineering and Informatics,x,-,,Malaysia
13,51,"Setyohadi, D. B., Kusrohmaniah, S., Gunawan, S. B., Pranowo, & Prabuwono, A. S. (2018). Galvanic skin response data classification for emotion detection. International Journal of Electrical and Computer Engineering, 8(5), 4004–4014. Scopus. https://doi.org/10.11591/ijece.v8i5.pp4004-4014","Setyohadi, D. B., Kusrohmaniah, S., Gunawan, S. B., Pranowo, & Prabuwono, A. S.",Galvanic skin response data classification for emotion detection.,2018,International Journal of Electrical and Computer Engineering,x,-,,Indonesia
14,52,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,53,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,54,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,55,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,56,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,57,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,58,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,59,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,60,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,61,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,62,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,63,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,64,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,65,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
14,66,"Soleymani, M., & Mortillaro, M. (2018). Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition. Frontiers in ICT, 5(JUL). Scopus. https://doi.org/10.3389/fict.2018.00017","Soleymani, M., & Mortillaro, M.",Behavioral and physiological responses to visual interest and appraisals: Multimodal analysis and automatic recognition.,2018,Frontiers in ICT,x,-,,Switzerland
15,67,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,68,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,69,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,70,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,71,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,72,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,73,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,74,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,75,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,76,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,77,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,78,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,79,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,80,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,81,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,82,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,83,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,84,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,85,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,86,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,87,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,88,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,89,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,90,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,91,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
15,92,"Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K. (2017). Low cost wearable sensor for human emotion recognition using skin conductance response. IEICE Transactions on Information and Systems, E100D(12), 3010–3017. Scopus. https://doi.org/10.1587/transinf.2017EDP7067","Md Ali, S. H., Ibne Reaz, M., Ahmad, S. A., Minhad, K. N., & Ooi, J. S. K",Low cost wearable sensor for human emotion recognition using skin conductance response.,2017,IEICE Transactions on Information and Systems,x,-,,Malaysia
16,93,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660","Wiem, M. B. H., & Lachiri, Z.",Emotion sensing from physiological signals using three defined areas in arousal-valence model.,2017,"2017 International Conference on Control, Automation and Diagnosis, ICCAD 2017",-,x,,Tunisia
16,94,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion sensing from physiological signals using three defined areas in arousal-valence model. 219–223. Scopus. https://doi.org/10.1109/CADIAG.2017.8075660","Wiem, M. B. H., & Lachiri, Z.",Emotion sensing from physiological signals using three defined areas in arousal-valence model.,2017,"2017 International Conference on Control, Automation and Diagnosis, ICCAD 2017",-,x,,Tunisia
17,95,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,96,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,97,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,98,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,99,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,100,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,101,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,102,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,103,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,104,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,105,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,106,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,107,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,108,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,109,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,110,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,111,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,112,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,113,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,114,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,115,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,116,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,117,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,118,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,119,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,120,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,121,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,122,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,123,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,124,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,125,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
17,126,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2017). Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses. Signal, Image and Video Processing, 11(7), 1347–1355. Scopus. https://doi.org/10.1007/s11760-017-1092-9","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses.,2017,"Signal, Image and Video Processing",x,-,,Iran
18,127,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533","Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B.",End-to-end learning for dimensional emotion recognition from physiological signals.,2017,Proceedings - IEEE International Conference on Multimedia and Expo,-,x,,Germany
18,128,"Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B. (2017). End-to-end learning for dimensional emotion recognition from physiological signals. 985–990. Scopus. https://doi.org/10.1109/ICME.2017.8019533","Keren, G., Kirschstein, T., Marchi, E., Ringeval, F., & Schuller, B.",End-to-end learning for dimensional emotion recognition from physiological signals.,2017,Proceedings - IEEE International Conference on Multimedia and Expo,-,x,,Germany
19,129,"Hernández-García, A., Fernández-Martínez, F., & Díaz-De-maría, F. (2017). Emotion and attention: Predicting electrodermal activity through video visual descriptors. 914–923. Scopus. https://doi.org/10.1145/3106426.3109418","Hernández-García, A., Fernández-Martínez, F., & Díaz-De-maría, F.",Emotion and attention: Predicting electrodermal activity through video visual descriptors.,2017,"Proceedings - 2017 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2017",-,x,,Germany
20,130,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",,"Wiem, M. B. H., & Lachiri, Z.",2017,"4th International Conference on Control Engineering and Information Technology, CEIT 2016",-,x,,Tunisia
20,131,"Wiem, M. B. H., & Lachiri, Z. (2017). Emotion assessing using valence-arousal evaluation based on peripheral physiological signals and support vector machine. 4th International Conference on Control Engineering and Information Technology, CEIT 2016. Scopus. https://doi.org/10.1109/CEIT.2016.7929117",,"Wiem, M. B. H., & Lachiri, Z.",2017,"4th International Conference on Control Engineering and Information Technology, CEIT 2016",-,x,,Tunisia
21,132,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,133,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,134,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,135,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,136,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,137,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,138,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,139,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,140,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,141,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,142,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
21,143,"Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K. (2017). From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals. 345–350. Scopus. https://doi.org/10.1109/PERCOMW.2017.7917586","Xu, Y., Hubener, I., Seipp, A.-K., Ohly, S., & David, K.",From the lab to the real-world: An investigation on the influence of human movement on Emotion Recognition using physiological signals.,2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",-,x,,Germany
22,144,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072","Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A.",Human emotion classifications for automotive driver using skin conductance response signal.,2017,"2016 International Conference on Advances in Electrical, Electronic and Systems Engineering, ICAEES 2016",-,x,,Malaysia
22,145,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072","Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A.",Human emotion classifications for automotive driver using skin conductance response signal.,2017,"2016 International Conference on Advances in Electrical, Electronic and Systems Engineering, ICAEES 2016",-,x,,Malaysia
22,146,"Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A. (2017). Human emotion classifications for automotive driver using skin conductance response signal. 371–375. Scopus. https://doi.org/10.1109/ICAEES.2016.7888072","Nisa’Minhad, K., Ali, S. H. M., Khai, J. O. S., & Ahmad, S. A.",Human emotion classifications for automotive driver using skin conductance response signal.,2017,"2016 International Conference on Advances in Electrical, Electronic and Systems Engineering, ICAEES 2016",-,x,,Malaysia
23,147,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130","Ayata, D., Yaslan, Y., & Kamasak, M.","Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches.",2017,"2016 Medical Technologies National Conference, TIPTEKNO 2016",-,x,,Turkey
23,148,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches. 2016 Medical Technologies National Conference, TIPTEKNO 2016. Scopus. https://doi.org/10.1109/TIPTEKNO.2016.7863130","Ayata, D., Yaslan, Y., & Kamasak, M.","Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches.",2017,"2016 Medical Technologies National Conference, TIPTEKNO 2016",-,x,,Turkey
24,149,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677","Greco, A., Valenza, G., Citi, L., & Scilingo, E. P.",Arousal and valence recognition of affective sounds based on electrodermal activity.,2017,IEEE Sensors Journal,x,-,,Italy
24,150,"Greco, A., Valenza, G., Citi, L., & Scilingo, E. P. (2017). Arousal and valence recognition of affective sounds based on electrodermal activity. IEEE Sensors Journal, 17(3), 716–725. Scopus. https://doi.org/10.1109/JSEN.2016.2623677","Greco, A., Valenza, G., Citi, L., & Scilingo, E. P.",Arousal and valence recognition of affective sounds based on electrodermal activity.,2017,IEEE Sensors Journal,x,-,,Italy
25,151,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.","Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I.",A design framework for human emotion recognition using electrocardiogram and skin conductance response signals.,2017,Journal of Engineering Science and Technology,x,-,,Malaysia
25,152,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.","Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I.",A design framework for human emotion recognition using electrocardiogram and skin conductance response signals.,2017,Journal of Engineering Science and Technology,x,-,,Malaysia
25,153,"Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I. (2017). A design framework for human emotion recognition using electrocardiogram and skin conductance response signals. Journal of Engineering Science and Technology, 12(11), 3102–3119. Scopus.","Minhad, K. N., Ali, S. H. M. D., & Reaz, M. B. I.",A design framework for human emotion recognition using electrocardiogram and skin conductance response signals.,2017,Journal of Engineering Science and Technology,x,-,,Malaysia
26,154,"Zhang, Q., Lai, X., & Liu, G. (2016). Emotion recognition of GSR based on an improved quantum neural network. 1, 488–492. Scopus. https://doi.org/10.1109/IHMSC.2016.66","Zhang, Q., Lai, X., & Liu, G.",Emotion recognition of GSR based on an improved quantum neural network.,2016,"Proceedings - 2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2016",-,x,,China
27,155,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",A novel signal-based fusion approach for accurate music emotion recognition.,2016,"Biomedical Engineering - Applications, Basis and Communications",x,-,,Iran
27,156,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",A novel signal-based fusion approach for accurate music emotion recognition.,2016,"Biomedical Engineering - Applications, Basis and Communications",x,-,,Iran
27,157,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). A novel signal-based fusion approach for accurate music emotion recognition. Biomedical Engineering - Applications, Basis and Communications, 28(6). Scopus. https://doi.org/10.4015/S101623721650040X","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",A novel signal-based fusion approach for accurate music emotion recognition.,2016,"Biomedical Engineering - Applications, Basis and Communications",x,-,,Iran
28,158,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
28,159,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
28,160,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
28,161,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
28,162,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
28,163,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
28,164,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
28,165,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
28,166,"Das, P., Khasnobish, A., & Tibarewala, D. N. (2016). Emotion recognition employing ECG and GSR signals as markers of ANS. 37–42. Scopus. https://doi.org/10.1109/CASP.2016.7746134","Das, P., Khasnobish, A., & Tibarewala, D. N.",Emotion recognition employing ECG and GSR signals as markers of ANS.,2016,"Conference on Advances in Signal Processing, CASP 2016",-,x,,India
29,167,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,Canada
29,168,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,Canada
29,169,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,Canada
29,170,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,Canada
29,171,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,Canada
29,172,"Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N. (2016). A quality adaptive multimodal affect recognition system for user-centric multimedia indexing. 317–320. Scopus. https://doi.org/10.1145/2911996.2912059","Gupta, R., Abadi, M. K., Cabré, J. A. C., Morreale, F., Falk, T. H., & Sebe, N.",A quality adaptive multimodal affect recognition system for user-centric multimedia indexing.,2016,ICMR 2016 - Proceedings of the 2016 ACM International Conference on Multimedia Retrieval,-,x,,Canada
30,173,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H.",2016,IECBES 2016 - IEEE-EMBS Conference on Biomedical Engineering and Sciences,-,x,,Malaysia
30,174,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H.",2016,IECBES 2016 - IEEE-EMBS Conference on Biomedical Engineering and Sciences,-,x,,Malaysia
30,175,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H. (2016). Driver emotion recognition framework based on electrodermal activity measurements during simulated driving conditions. 365–369. Scopus. https://doi.org/10.1109/IECBES.2016.7843475",,"Ooi, J. S. K., Ahmad, S. A., Chong, Y. Z., Ali, S. H. M., Ai, G., & Wagatsuma, H.",2016,IECBES 2016 - IEEE-EMBS Conference on Biomedical Engineering and Sciences,-,x,,Malaysia
31,176,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform.,2016,Iranian Journal of Medical Physics,x,-,,Iran
31,177,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform.,2016,Iranian Journal of Medical Physics,x,-,,Iran
31,178,"Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S. (2016). Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform. Iranian Journal of Medical Physics, 13(3), 163–173. Scopus. https://doi.org/10.22038/ijmp.2016.7960","Goshvarpour, A., Abbasi, A., Goshvarpour, A., & Daneshvar, S.",Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform.,2016,Iranian Journal of Medical Physics,x,-,,Iran
32,179,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,-,x,,USA
32,180,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,-,x,,USA
32,181,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,-,x,,USA
32,182,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,-,x,,USA
32,183,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,-,x,,USA
32,184,"Siddharth,  null, Jung, T.-P., & Sejnowski, T. J. (2018). Multi-modal Approach for Affective Computing. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2018, 291–294. https://doi.org/10.1109/EMBC.2018.8512320","Siddharth, null, Jung, T.-P., & Sejnowski, T. J.",Multi-modal Approach for Affective Computing.,2018,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,-,x,,USA
33,185,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,Macedonian
33,186,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,Macedonian
33,187,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,Macedonian
33,188,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,Macedonian
33,189,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,Macedonian
33,190,"Goshvarpour, A., Abbasi, A., & Goshvarpour, A. (2017). An accurate emotion recognition system using ECG and GSR signals and matching pursuit method. Biomedical Journal, 40(6), 355–368. https://doi.org/10.1016/j.bj.2017.11.001","Goshvarpour, A., Abbasi, A., & Goshvarpour, A.",An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.,2017,Biomedical Journal,x,-,,Macedonian
34,191,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516647","Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I.",Experimental analysis of emotion classification techniques.,2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2018",-,x,,Romania
34,192,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516648","Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I.",Experimental analysis of emotion classification techniques.,2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2019",-,x,,Romania
34,193,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516649","Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I.",Experimental analysis of emotion classification techniques.,2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2020",-,x,,Romania
34,194,"Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I. (2018). Experimental analysis of emotion classification techniques. 63–70. Scopus. https://doi.org/10.1109/ICCP.2018.8516649","Dumitriu, T., Cimpanu, C., Ungureanu, F., & Manta, V.-I.",Experimental analysis of emotion classification techniques.,2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2020",-,x,,Romania
35,195,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.","Ferdinando, H., & Alasaarela, E.",Emotion recognition using cvxEDA-based features.,2018,"Journal of Telecommunication, Electronic and Computer Engineering",x,-,,Finland
35,196,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.","Ferdinando, H., & Alasaarela, E.",Emotion recognition using cvxEDA-based features.,2018,"Journal of Telecommunication, Electronic and Computer Engineering",x,-,,Finland
35,197,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.","Ferdinando, H., & Alasaarela, E.",Emotion recognition using cvxEDA-based features.,2018,"Journal of Telecommunication, Electronic and Computer Engineering",x,-,,Finland
35,198,"Ferdinando, H., & Alasaarela, E. (2018). Emotion recognition using cvxEDA-based features. Journal of Telecommunication, Electronic and Computer Engineering, 10(2–3), 19–23. Scopus.","Ferdinando, H., & Alasaarela, E.",Emotion recognition using cvxEDA-based features.,2018,"Journal of Telecommunication, Electronic and Computer Engineering",x,-,,Finland
36,199,"Zhang, S., Liu, G., & Lai, X. (2015). Classification of evoked emotions using an artificial neural network based on single, short-term physiological signals. Journal of Advanced Computational Intelligence and Intelligent Informatics, 19(1), 118-126.","Zhang, S., Liu, G., & Lai, X. ","Classification of evoked emotions using an artificial neural network based on single, short-term physiological signals.",2015,Journal of Advanced Computational Intelligence and Intelligent Informatics,x,-,,China
37,200,"Gjoreski, M., Lustrek, M., & Gams, M. (2018). Multi-task ensemble learning for affect recognition. 553–558. Scopus. https://doi.org/10.1145/3267305.3267308
","Gjoreski, M., Lustrek, M., & Gams, M.",Multi-task ensemble learning for affect recognition.,2018,UbiComp/ISWC 2018 - Adjunct Proceedings of the 2018 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2018 ACM International Symposium on Wearable Computers,-,x,,Slovenia
38,201,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,202,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,203,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,204,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,205,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,206,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,207,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,208,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,209,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,210,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,211,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,212,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,213,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,214,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,215,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,216,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,217,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,218,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,219,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,220,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,221,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,222,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,223,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,224,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,225,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,226,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,227,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,228,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,229,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,230,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,231,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,232,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,233,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,234,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,235,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,236,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,237,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,238,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,239,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,240,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,241,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
38,242,"Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B. (2018). An inter-domain study for arousal recognition from physiological signals. Informatica (Slovenia), 42(1), 61–68. Scopus.","Gjoreski, M., Luštrek, M., Gams, M., & Mitrevski, B.",An inter-domain study for arousal recognition from physiological signals.,2018,Informatica (Slovenia),x,-,,Slovenia
39,243,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,Turkey
39,244,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,Turkey
39,245,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,Turkey
39,246,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,Turkey
39,247,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,Turkey
39,248,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,Turkey
39,249,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,Turkey
39,250,"Ayata, D., Yaslan, Y., & Kamasak, M. (2017). Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods. Istanbul University - Journal of Electrical and Electronics Engineering, 17, 3129–3136. Scopus.","Ayata, D., Yaslan, Y., & Kamasak, M.",Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods.,2017,Istanbul University - Journal of Electrical and Electronics Engineering,x,-,,Turkey
40,251,"Martínez-Rodrigo, A., Zangróniz, R., Pastor, J. M., & Sokolova, M. V. (2017). Arousal level classification of the aging adult from electro-dermal activity: From hardware development to software architecture. Pervasive and Mobile Computing, 34, 46–59. Scopus. https://doi.org/10.1016/j.pmcj.2016.04.006","Martínez-Rodrigo, A., Zangróniz, R., Pastor, J. M., & Sokolova, M. V.",Arousal level classification of the aging adult from electro-dermal activity: From hardware development to software architecture.,2017,Pervasive and Mobile Computing,x,-,,Spain
41,252,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636","Milchevski, A., Rozza, A., & Taskovski, D.",Multimodal affective analysis combining regularized linear regression and boosted regression trees.,2015,"AVEC 2015 - Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015",-,x,,Macedonian
41,253,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636","Milchevski, A., Rozza, A., & Taskovski, D.",Multimodal affective analysis combining regularized linear regression and boosted regression trees.,2015,"AVEC 2015 - Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015",-,x,,Macedonian
41,254,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636","Milchevski, A., Rozza, A., & Taskovski, D.",Multimodal affective analysis combining regularized linear regression and boosted regression trees.,2015,"AVEC 2015 - Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015",-,x,,Macedonian
41,255,"Milchevski, A., Rozza, A., & Taskovski, D. (2015). Multimodal affective analysis combining regularized linear regression and boosted regression trees. 33–39. Scopus. https://doi.org/10.1145/2808196.2811636","Milchevski, A., Rozza, A., & Taskovski, D.",Multimodal affective analysis combining regularized linear regression and boosted regression trees.,2015,"AVEC 2015 - Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015",-,x,,Macedonian
42,256,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007","Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B.",Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data.,2015,Pattern Recognition Letters,x,-,,Germany
42,257,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007","Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B.",Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data.,2015,Pattern Recognition Letters,x,-,,Germany
42,258,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007","Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B.",Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data.,2015,Pattern Recognition Letters,x,-,,Germany
42,259,"Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters, 66, 22–30. Scopus. https://doi.org/10.1016/j.patrec.2014.11.007","Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J.-P., Ebrahimi, T., Lalanne, D., & Schuller, B.",Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data.,2015,Pattern Recognition Letters,x,-,,Germany
43,260,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011","Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T.","Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting.",2017,Frontiers in ICT,x,-,,Switzerland
43,261,"Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T. (2017). Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting. Frontiers in ICT, 4(JUN). Scopus. https://doi.org/10.3389/fict.2017.00011","Kostoulas, T., Chanel, G., Muszynski, M., Lombardo, P., & Pun, T.","Films, affective computing and aesthetic experience: Identifying emotional and aesthetic highlights from multimodal signals in a social setting.",2017,Frontiers in ICT,x,-,,Switzerland
44,262,"Barral, O., Kosunen, I., & Jacucci, G. (2017). No need to laugh out loud: Predicting humor appraisal of comic strips based on physiological signals in a realistic environment. ACM Transactions on Computer-Human Interaction, 24(6). Scopus. https://doi.org/10.1145/3157730","Barral, O., Kosunen, I., & Jacucci, G.",No need to laugh out loud: Predicting humor appraisal of comic strips based on physiological signals in a realistic environment.,2017,ACM Transactions on Computer-Human Interaction,x,-,,Finland
45,263,"Zhang, Q., Lai, X., & Liu, G. (2016). Emotion recognition of GSR based on an improved quantum neural network. 1, 488-492. Scopus. https://doi.org/10.1109/IHMSC.2016.66
","Zhang, Q., Lai, X., & Liu, G.", Emotion recognition of GSR based on an improved quantum neural network.,2016,"Proceedings - 2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2016",-,x,,China
46,264,"Lanatà, A., Valenza, G., & Scilingo, E. P. (2012). A novel EDA glove based on textile-integrated electrodes for affective computing. Medical & Biological Engineering & Computing, 50(11), 1163–1172. doi:10.1007/s11517-012-0921-9","Lanatá,A, Valenza,G y Scilingo.E.P", A novel EDA glove based on textile-integrated electrodes for affective computing,2012,International Federation for Medical and Biological Engineering,x,-,,Italy
47,265,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,x,-,,USA
47,266,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,x,-,,USA
47,267,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,x,-,,USA
47,268,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,x,-,,USA
47,269,"Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M. (2013). Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal. Annals of Biomedical Engineering, 42(1), 162–176. doi:10.1007/s10439-013-0880-9 ","Ren, P., Barreto, A., Huang, J., Gao, Y., Ortega, F. R., & Adjouadi, M",Off-line and On-line Stress Detection Through Processing of the Pupil Diameter Signal,2013,Annals of Biomedical Engineering,x,-,,USA
48,270,"GOUIZI, K., BEREKSI REGUIG, F., & MAAOUI, C. (2011). Emotion recognition from physiological signals. Journal of Medical Engineering & Technology, 35(6-7), 300–307. doi:10.3109/03091902.2011.601784","GOUIZI, K., BEREKSI REGUIG, F., & MAAOUI, C. ",Emotion recognition from physiological signals,2011,Journal of Medical Engineering & Technology,x,-,,France
49,271,"Bornoiu, I.-V., Strungaru, R., & Grigore, O. (2015). Intelligent System for Emotion Recognition Based on Electrodermal Activity Processing. 6th European Conference of the International Federation for Medical and Biological Engineering, 70–73. doi:10.1007/978-3-319-11128-5_18 ","Bornoiu, I.-V., Strungaru, R., & Grigore, O",Intelligent System for Emotion Recognition Based on Electrodermal Activity Processing,2015,Medical and Biological Engineering,-,x,,Romania
50,272,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,x,,Greece
50,273,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,x,,Greece
50,274,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,x,,Greece
50,275,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,x,,Greece
50,276,"Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N. (2015). Subjective Assessment of Stress in HCI. Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015. doi:10.1145/2808435.2808450","Liapis, A., Katsanos, C., Sotiropoulos, D., Xenos, M., & Karousos, N.",Subjective Assessment of Stress in HCI,2015,Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter - CHItaly 2015.,-,x,,Greece
51,277,"Drungilas, D., Bielskis, A. A., & Denisov, V. (2010). An intelligent control system based on non-invasive man machine interaction. In Innovations in Computing Sciences and Software Engineering (pp. 63-68). Springer, Dordrecht.","Drungilas, D., Bielskis, A. A., & Denisov, V. ",An intelligent control system based on non-invasive man machine interaction,2010,Innovations in Computing Sciences and Software Engineering,-,x,,Lithuania
52,278,"Wu, G., Liu, G., & Hao, M. (2010). The Analysis of Emotion Recognition from GSR Based on PSO. 2010 International Symposium on Intelligence Information Processing and Trusted Computing. doi:10.1109/iptc.2010.60","Wu, G., Liu, G., & Hao, M.",The Analysis of Emotion Recognition from GSR Based on PSO,2010,2010 International Symposium on Intelligence Information Processing and Trusted Computing,-,x,,China
53,279,"Giakoumis, D., Tzovaras, D., Moustakas, K., & Hassapis, G. (2011). Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features. IEEE Transactions on Affective Computing, 2(3), 119–133. doi:10.1109/t-affc.2011.4 ","Giakoumis, D., Tzovaras, D., Moustakas, K., & Hassapis, G.",Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features,2011,IEEE Transactions on Affective Computing,x,-,,Greece
54,280,"Safta, I., Grigore, O., & Căruntu, C.(2011). Emotion Detection Using Psycho-Physiological Signal Processing. Computer, 3, 4.","Safta, I., Grigore, O., & Căruntu, C",Automatic Recognition of Boredom in Video Games Using Novel Biosignal Moment-Based Features,2011,2011 7TH INTERNATIONAL SYMPOSIUM ON ADVANCED TOPICS IN ELECTRICAL ENGINEERING (ATEE),-,x,,Romania
55,281,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,x,,USA
55,282,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,x,,USA
55,283,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,x,,USA
55,284,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,x,,USA
55,285,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2012). Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users. Biomedical sciences instrumentation, 48, 345-350.","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Comparison of the use of pupil diameter and galvanic skin response signals for affective assessment of computer users,2012,Biomedical sciences instrumentation,-,x,,USA
56,286,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ","Cheng, J., & Liu, G.",Computing nonlinear features of skin conductance to build the affective detection model,2013,"International Conference on Communications, Circuits and Systems (ICCCAS)",-,x,,China
56,287,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ","Cheng, J., & Liu, G.",Computing nonlinear features of skin conductance to build the affective detection model,2013,"International Conference on Communications, Circuits and Systems (ICCCAS)",-,x,,China
56,288,"Cheng, J., & Liu, G. (2013). Computing nonlinear features of skin conductance to build the affective detection model. 2013 International Conference on Communications, Circuits and Systems (ICCCAS). doi:10.1109/icccas.2013.6765349 ","Cheng, J., & Liu, G.",Computing nonlinear features of skin conductance to build the affective detection model,2013,"International Conference on Communications, Circuits and Systems (ICCCAS)",-,x,,China
57,289,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,x,-,,USA
57,290,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,x,-,,USA
57,291,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,x,-,,USA
57,292,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,x,-,,USA
57,293,"Ren, P., Barreto, A., Gao, Y., & Adjouadi, M. (2013). Affective Assessment by Digital Processing of the Pupil Diameter. IEEE Transactions on Affective Computing, 4(1), 2–14. doi:10.1109/t-affc.2012.25","Ren, P., Barreto, A., Gao, Y., & Adjouadi, M.",Affective Assessment by Digital Processing of the Pupil Diameter,2013,IEEE Transactions on Affective Computing,x,-,,USA
58,294,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.","Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G.",Pervasive and unobtrusive emotion sensing for human mental health,2013,7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops,-,x,,USA
58,295,"Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G. (2013, May). Pervasive and unobtrusive emotion sensing for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops (pp. 436-439). IEEE.","Guo, R., Li, S., He, L., Gao, W., Qi, H., & Owens, G.",Pervasive and unobtrusive emotion sensing for human mental health,2013,7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops,-,x,,USA
59,296,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,x,,Portugal
59,297,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,x,,Portugal
59,298,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,x,,Portugal
59,299,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,x,,Portugal
59,300,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,x,,Portugal
59,301,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,x,,Portugal
59,302,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,x,,Portugal
59,303,"Henriques, R., Paiva, A., & Antunes, C. (2013). Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity. 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. doi:10.1109/acii.2013.14 ","Henriques, R., Paiva, A., & Antunes, C",Accessing Emotion Patterns from Affective Interactions Using Electrodermal Activity,2013,Humaine Association Conference on Affective Computing and Intelligent Interaction,-,x,,Portugal
60,304,"Li, S., Guo, R., He, L., Gao, W., Qi, H., & Owens, G. (2014). MoodMagician. Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems - SenSys ’14. doi:10.1145/2668332.2668371","Li, S., Guo, R., He, L., Gao, W., Qi, H., & Owens, G.",MoodMagician,2014,Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems,-,x,,USA
61,305,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,x,,,China
61,306,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,x,,,China
61,307,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,x,,,China
61,308,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,x,,,China
61,309,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,x,,,China
61,310,"Yu, D., & Sun, S. (2020). A systematic exploration of deep neural networks for EDA-based emotion recognition. Information, 11(4), 212.","Yu, D.; Sun, S.","A systematic exploration of deep neural networks for EDA-based emotion recognition
",2020,Information,x,,,China
62,311,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,312,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,313,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,314,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,315,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,316,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,317,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,318,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,319,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,320,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,321,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,322,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,323,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,324,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,325,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,326,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,327,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,328,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,329,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
62,330,"Al Machot, F., Elmachot, A., Ali, M., Al Machot, E., & Kyamakya, K. (2019). A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors. Sensors, 19(7), 1659.","F, Al Machot; A, Elmachot; M, Ali; E, Al Machot; K, Kyamakya","A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors.
",2019,Sensors,x,,,Germany
63,331,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.","J, Seo; TH, Laine; KA, Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data.
",2019,Sensors,x,,,Korea
63,332,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.","J, Seo; TH, Laine; KA, Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data.
",2019,Sensors,x,,,Korea
63,333,"Seo, J., Laine, T. H., & Sohn, K. A. (2019). An exploration of machine learning methods for robust boredom classification using EEG and GSR data. Sensors, 19(20), 4561.","J, Seo; TH, Laine; KA, Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data.
",2019,Sensors,x,,,Korea
64,334,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE Access,x,,,Spain
64,335,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE Access,x,,,Spain
64,336,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE Access,x,,,Spain
64,337,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE Access,x,,,Spain
64,338,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE Access,x,,,Spain
64,339,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE Access,x,,,Spain
64,340,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE Access,x,,,Spain
64,341,"Martinez, R., Salazar-Ramirez, A., Arruti, A., Irigoyen, E., Martin, J. I., & Muguerza, J. (2019). A self-paced relaxation response detection system based on galvanic skin response analysis. IEEE Access, 7, 43730-43741.","Martinez, R.; Salazar-Ramirez, A.; Arruti, A.; Irigoyen, E.; Martin, J.I.; Muguerza, J.","A Self-Paced Relaxation Response Detection System Based on Galvanic Skin Response Analysis
",2019,IEEE Access,x,,,Spain
65,342,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.","Sharma, V.; Prakash, N.R.; Kalra, P.","Audio-video emotional response mapping based upon Electrodermal Activity
",2019,Biomedical Signal Processing and Control,x,,,India
65,343,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.","Sharma, V.; Prakash, N.R.; Kalra, P.","Audio-video emotional response mapping based upon Electrodermal Activity
",2019,Biomedical Signal Processing and Control,x,,,India
65,344,"Sharma, V., Prakash, N. R., & Kalra, P. (2019). Audio-video emotional response mapping based upon electrodermal activity. Biomedical Signal Processing and Control, 47, 324-333.","Sharma, V.; Prakash, N.R.; Kalra, P.","Audio-video emotional response mapping based upon Electrodermal Activity
",2019,Biomedical Signal Processing and Control,x,,,India
66,345,"Dar, M. N., Akram, M. U., Khawaja, S. G., & Pujari, A. N. (2020). Cnn and lstm-based emotion charting using physiological signals. Sensors, 20(16), 4551.","Dar, M.N.; Akram, M.U.; Khawaja, S.G.; Pujari, A.N.","Cnn and lstm-based emotion charting using physiological signals
",2020,Sensors,x,,,Pakistan
67,346,"Greco, A., Marzi, C., Lanata, A., Scilingo, E. P., & Vanello, N. (2019, July). Combining electrodermal activity and speech analysis towards a more accurate emotion recognition system. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 229-232). IEEE.","A, Greco; C, Marzi; A, Lanata; EP, Scilingo; N, Vanello","Combining Electrodermal Activity and Speech Analysis towards a more Accurate Emotion Recognition System.
",2019,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,,x,,Italy
68,347,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,348,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,349,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,350,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,351,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,352,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,353,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,354,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,355,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,356,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,357,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
68,358,"Ganapathy, N., Veeranki, Y. R., & Swaminathan, R. (2020). Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features. Expert Systems with Applications, 159, 113571.","Ganapathy, N.; Veeranki, Y.R.; Swaminathan, R.","Convolutional neural network based emotion classification using electrodermal activity signals and time-frequency features
",2020,Expert Systems With Applications,,,x,India
69,359,"Lee, S., Lee, T., Yang, T., Yoon, C., & Kim, S. P. (2020). Detection of drivers’ anxiety invoked by driving situations using multimodal biosignals. Processes, 8(2), 155.","Lee, S.; Lee, T.; Yang, T.; Yoon, C.; Kim, S.P.","Detection of drivers' anxiety invoked by driving situations using multimodal biosignals
",2020,Processes,x,,,Korea
70,360,"García-Faura, Á., Hernández-García, A., Fernández-Martínez, F., Díaz-de-María, F., & San-Segundo, R. (2019, January). Emotion and attention: Audiovisual models for group-level skin response recognition in short movies. In Web Intelligence (Vol. 17, No. 1, pp. 29-40). IOS Press.","García-Faura, A.; Hernández-García, A.; Fernández-Martínez, F.; Díaz-De-María, F.; San-Segundo, R.","Emotion and attention: Audiovisual models for group-level skin response recognition in short movies
",2019,Web Intelligence,x,,,Spain
71,361,"Wei, W., Jia, Q., Feng, Y., & Chen, G. (2018). Emotion recognition based on weighted fusion strategy of multichannel physiological signals. Computational intelligence and neuroscience, 2018.","W, Wei; Q, Jia; Y, Feng; G, Chen","Emotion Recognition Based on Weighted Fusion Strategy of Multichannel Physiological Signals.
",2018,Computational Intelligence and Neuroscience,x,,,China
72,362,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,x,,Egypt
72,363,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,x,,Egypt
72,364,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,x,,Egypt
72,365,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,x,,Egypt
72,366,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,x,,Egypt
72,367,"El-Amir, M. M., Al-Atabany, W., & Eldosoky, M. A. (2019, April). Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions. In 2019 36th National Radio Science Conference (NRSC) (pp. 200-208). IEEE.","El-Amir, M.M.; Al-Atabany, W.; Eldosoky, M.A.; R., Sadek; A.A., Goudah; S., ElDiasty","Emotion Recognition via Detrended Fluctuation Analysis and Fractal Dimensions
",2019,"National Radio Science conference
",,x,,Egypt
73,368,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.","Tung, K.; Liu, P.-K.; Chuang, Y.-C.; Wang, S.-H.; Wu, A.-Y.","Entropy-assisted multi-modal emotion recognition framework based on physiological signals
",2019,Conference on biomedical engineering and science,,x,,Taiwan
73,369,"Tung, K., Liu, P. K., Chuang, Y. C., Wang, S. H., & Wu, A. Y. A. (2018, December). Entropy-assisted multi-modal emotion recognition framework based on physiological signals. In 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) (pp. 22-26). IEEE.","Tung, K.; Liu, P.-K.; Chuang, Y.-C.; Wang, S.-H.; Wu, A.-Y.","Entropy-assisted multi-modal emotion recognition framework based on physiological signals
",2019,Conference on biomedical engineering and science,,x,,Taiwan
74,370,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,x,,,Iran
74,371,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,x,,,Iran
74,372,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,x,,,Iran
74,373,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,x,,,Iran
74,374,"Golgouneh, A., & Tarvirdizadeh, B. (2020). Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms. Neural Computing and Applications, 32(11), 7515-7537.","Golgouneh, A.; Tarvirdizadeh, B.","Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms
",2020,Neural Computing and Applications,x,,,Iran
75,375,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.","Sun, X.; Hong, T.; Li, C.; Ren, F.","Hybrid spatiotemporal models for sentiment classification via galvanic skin response
",2019,Neurocomputing,x,,,China
75,376,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.","Sun, X.; Hong, T.; Li, C.; Ren, F.","Hybrid spatiotemporal models for sentiment classification via galvanic skin response
",2019,Neurocomputing,x,,,China
75,377,"Sun, X., Hong, T., Li, C., & Ren, F. (2019). Hybrid spatiotemporal models for sentiment classification via galvanic skin response. Neurocomputing, 358, 385-400.","Sun, X.; Hong, T.; Li, C.; Ren, F.","Hybrid spatiotemporal models for sentiment classification via galvanic skin response
",2019,Neurocomputing,x,,,China
76,378,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,x,,Switzerland
76,379,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,x,,Switzerland
76,380,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,x,,Switzerland
76,381,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,x,,Switzerland
76,382,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,x,,Switzerland
76,383,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,x,,Switzerland
76,384,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,x,,Switzerland
76,385,"Chang, E. J., Rahimi, A., Benini, L., & Wu, A. Y. A. (2019, March). Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) (pp. 137-141). IEEE.","Chang, E.-J.; Rahimi, A.; Benini, L.; Wu, A.-Y.A.","Hyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals
",2019,"Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst., AICAS",,x,,Switzerland
77,386,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,387,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,388,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,389,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,390,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,391,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,392,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,393,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,394,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,395,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,396,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,397,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,398,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,399,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,400,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,401,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,402,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,403,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,404,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,405,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,406,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,407,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,408,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,409,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,410,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,411,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,412,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,413,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,414,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,415,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,416,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,417,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
77,418,"Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., & Cui, Z. (2019). MPED: A multi-modal physiological emotion database for discrete emotion recognition. IEEE Access, 7, 12177-12191.","Song, T.; Zheng, W.; Lu, C.; Zong, Y.; Zhang, X.; Cui, Z.","MPED: A multi-modal physiological emotion database for discrete emotion recognition
",2019,IEEE Access,x,,,China
78,419,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.","Thammasan, N.; Hagad, J.L.; Fukui, K.-I.; Numao, M.","Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals
",2018,"Int. Conf. Affect. Comput. Intel. Interact. Workshops Demos, ACIIW",,x,,Japan
78,420,"Thammasan, N., Hagad, J. L., Fukui, K. I., & Numao, M. (2017, October). Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 44-49). IEEE.","Thammasan, N.; Hagad, J.L.; Fukui, K.-I.; Numao, M.","Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals
",2018,"Int. Conf. Affect. Comput. Intel. Interact. Workshops Demos, ACIIW",,x,,Japan
79,421,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.","Pinto, G.; Carvalho, J.M.; Barros, F.; Soares, S.C.; Pinho, A.J.; Brás, S.","Multimodal emotion evaluation: A physiological model for cost-effective emotion classification
",2020,Sensors,x,,,Portugal
79,422,"Pinto, G., Carvalho, J. M., Barros, F., Soares, S. C., Pinho, A. J., & Brás, S. (2020). Multimodal emotion evaluation: A physiological model for cost-effective emotion classification. Sensors, 20(12), 3510.","Pinto, G.; Carvalho, J.M.; Barros, F.; Soares, S.C.; Pinho, A.J.; Brás, S.","Multimodal emotion evaluation: A physiological model for cost-effective emotion classification
",2020,Sensors,x,,,Portugal
80,423,"Raheel, A., Majid, M., Alnowami, M., & Anwar, S. M. (2020). Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia. Sensors, 20(14), 4037.","Raheel, A.; Majid, M.; Alnowami, M.; Anwar, S.M.","Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia
",2020,Sensors,x,,,Pakistan
81,424,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.","Liu, Y.; Jiang, C.","Recognition of Shooter's Emotions under Stress Based on Affective Computing
",2019,IEEE Access,x,,,China
81,425,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.","Liu, Y.; Jiang, C.","Recognition of Shooter's Emotions under Stress Based on Affective Computing
",2019,IEEE Access,x,,,China
81,426,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.","Liu, Y.; Jiang, C.","Recognition of Shooter's Emotions under Stress Based on Affective Computing
",2019,IEEE Access,x,,,China
81,427,"Liu, Y., & Jiang, C. (2019). Recognition of shooter’s emotions under stress based on affective computing. IEEE Access, 7, 62338-62343.","Liu, Y.; Jiang, C.","Recognition of Shooter's Emotions under Stress Based on Affective Computing
",2019,IEEE Access,x,,,China
82,428,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).","Zhang, K.; Zhang, H.; Li, S.; Yang, C.; Sun, L.","The PMEmo dataset for music emotion recognition
",2018,ICMR - Proc. ACM Int. Conf. Multimed. Retr.,,x,,China
82,429,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).","Zhang, K.; Zhang, H.; Li, S.; Yang, C.; Sun, L.","The PMEmo dataset for music emotion recognition
",2018,ICMR - Proc. ACM Int. Conf. Multimed. Retr.,,x,,China
82,430,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).","Zhang, K.; Zhang, H.; Li, S.; Yang, C.; Sun, L.","The PMEmo dataset for music emotion recognition
",2018,ICMR - Proc. ACM Int. Conf. Multimed. Retr.,,x,,China
82,431,"Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018, June). The pmemo dataset for music emotion recognition. In Proceedings of the 2018 acm on international conference on multimedia retrieval (pp. 135-142).","Zhang, K.; Zhang, H.; Li, S.; Yang, C.; Sun, L.","The PMEmo dataset for music emotion recognition
",2018,ICMR - Proc. ACM Int. Conf. Multimed. Retr.,,x,,China
83,432,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,x,,China
83,433,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,x,,China
83,434,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,x,,China
83,435,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,x,,China
83,436,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,x,,China
83,437,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,x,,China
83,438,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,x,,China
83,439,"Niu, Y., Wang, D., Wang, Z., Sun, F., Yue, K., & Zheng, N. (2020). User experience evaluation in virtual reality based on subjective feelings and physiological signals. Electronic Imaging, 2020(13), 60413-1.","Niu, Y.; Wang, D.; Wang, Z.; Sun, F.; Yue, K.; Zheng, N.; M., Dolinsky; I.E., McDowall","User Experience Evaluation in Virtual Reality based on Subjective Feelings and Physiological Signals
",2020,IS T Intl. Symposium Electronic Imaging Science Technol.,,x,,China
84,440,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.","Santamaria-Granados, L.; Munoz-Organero, M.; Ramirez-Gonzalez, G.; Abdulhay, E.; Arunkumar, N.","Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)
",2019,IEEE Access,x,,,Colombia
84,441,"Santamaria-Granados, L., Munoz-Organero, M., Ramirez-Gonzalez, G., Abdulhay, E., & Arunkumar, N. J. I. A. (2018). Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS). IEEE Access, 7, 57-67.","Santamaria-Granados, L.; Munoz-Organero, M.; Ramirez-Gonzalez, G.; Abdulhay, E.; Arunkumar, N.","Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)
",2019,IEEE Access,x,,,Colombia
85,442,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,443,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,444,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,445,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,446,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,447,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,448,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,449,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,450,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,451,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,452,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
85,453,"Zhang, L. K., Sun, S. Q., Xing, B. X., Luo, R. M., & Zhang, K. J. (2019). Using psychophysiological measures to recognize personal music emotional experience. Frontiers of Information Technology & Electronic Engineering, 20(7), 964-974.","Zhang, L.-K.; Sun, S.-Q.; Xing, B.-X.; Luo, R.-M.; Zhang, K.-J.","Using psychophysiological measures to recognize personal music emotional experience
",2019,Frontiers of Information Technology and Electronic Engineering,x,,,China
86,454,"Liapis, A., Katsanos, C., Karousos, N., Xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).","Liapis, A.; Katsanos, C.; Karousos, N.; Xenos, M.; Orphanoudakis, T.","UDSP+: Stress detection based on user-reported emotional ratings and wearable skin conductance sensor
",2019,UbiComp/ISWC - Adjun. Proc. ACM Int. Jt. Conf. Pervasive Ubiquitous Comput. Proc. ACM Int. Symp. Wearable Comput.,,x,,Greece
86,455,"Liapis, A., Katsanos, C., Karousos, N., Xenos, M., & Orphanoudakis, T. (2019, September). UDSP+ stress detection based on user-reported emotional ratings and wearable skin conductance sensor. In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (pp. 125-128).","Liapis, A.; Katsanos, C.; Karousos, N.; Xenos, M.; Orphanoudakis, T.","UDSP+: Stress detection based on user-reported emotional ratings and wearable skin conductance sensor
",2019,UbiComp/ISWC - Adjun. Proc. ACM Int. Jt. Conf. Pervasive Ubiquitous Comput. Proc. ACM Int. Symp. Wearable Comput.,,x,,Greece
87,456,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,x,,China
87,457,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,x,,China
87,458,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,x,,China
87,459,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,x,,China
87,460,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,x,,China
87,461,"Xie, J., Xu, X., & Shu, L. (2018, May). WT feature based emotion recognition from multi-channel physiological signals with decision fusion. In 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) (pp. 1-6). IEEE.","Xie, J.; Xu, X.; Shu, L.","WT Feature Based Emotion Recognition from Multi-channel Physiological Signals with Decision Fusion
",2018,"Asian Conf. Affective Comput. Intell. Interaction, ACII Asia",,x,,China
88,462,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press."," N, Ganapathy; R, Swaminathan;","Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network
",2020,Studies in health technology and informatics,x,,,India
88,463,"Ganapathy, N., & Swaminathan, R. (2020). Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network. In Digital Personalized Health and Medicine (pp. 1269-1270). IOS Press."," N, Ganapathy; R, Swaminathan;","Emotion Analysis Using Electrodermal Signals and Spiking Deep Belief Network
",2020,Studies in health technology and informatics,x,,,India
89,464,"Yasemin, M., Sarıkaya, M. A., & Ince, G. (2019, July). Emotional state estimation using sensor fusion of EEG and EDA. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 5609-5612). IEEE.","Mine Yasemin, Mehmet Ali Sarikaya, Gokhan Ince","Emotional State Estimation using Sensor Fusion of EEG and EDA.",2019,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,,x,,Turkey
90,465,"Ghiasi, S., Greco, A., Barbieri, R., Scilingo, E. P., & Valenza, G. (2020). Assessing autonomic function from electrodermal activity and heart rate variability during cold-pressor test and emotional challenge. Scientific reports, 10(1), 1-13.","Ghiasi, S.; Greco, A.; Barbieri, R.; Scilingo, E.P.; Valenza, G.","Assessing Autonomic Function from Electrodermal Activity and Heart Rate Variability During Cold-Pressor Test and Emotional Challenge
",2020,Scientific Reports,x,,,Italy
91,466,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).","Gümüslü, E.; Erol Barkana, D.; Köse, H.","Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems",2020,International Conference on Multimodal Interaction,,x,,Turkey
91,467,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).","Gümüslü, E.; Erol Barkana, D.; Köse, H.","Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems",2020,International Conference on Multimodal Interaction,,x,,Turkey
91,468,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).","Gümüslü, E.; Erol Barkana, D.; Köse, H.","Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems",2020,International Conference on Multimodal Interaction,,x,,Turkey
91,469,"Gümüslü, E., Erol Barkana, D., & Köse, H. (2020, October). Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems. In Companion Publication of the 2020 International Conference on Multimodal Interaction (pp. 379-387).","Gümüslü, E.; Erol Barkana, D.; Köse, H.","Emotion recognition using EEG and physiological data for robot-assisted rehabilitation systems",2020,International Conference on Multimodal Interaction,,x,,Turkey
92,470,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).","Katada, S.; Okada, S.; Hirano, Y.; Komatani, K.","Is She Truly Enjoying the Conversation?: Analysis of Physiological Signals toward Adaptive Dialogue Systems",2020,International Conference on Multimodal Interaction,,x,,Japan
92,471,"Katada, S., Okada, S., Hirano, Y., & Komatani, K. (2020, October). Is She Truly Enjoying the Conversation? Analysis of Physiological Signals toward Adaptive Dialogue Systems. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 315-323).","Katada, S.; Okada, S.; Hirano, Y.; Komatani, K.","Is She Truly Enjoying the Conversation?: Analysis of Physiological Signals toward Adaptive Dialogue Systems",2020,International Conference on Multimodal Interaction,,x,,Japan
93,472,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,x,,Taiwan
93,473,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,x,,Taiwan
93,474,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,x,,Taiwan
93,475,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,x,,Taiwan
93,476,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,x,,Taiwan
93,477,"Susanto, I. Y., Pan, T. Y., Chen, C. W., Hu, M. C., & Cheng, W. H. (2020, June). Emotion recognition from galvanic skin response signal based on deep hybrid neural networks. In Proceedings of the 2020 International Conference on Multimedia Retrieval (pp. 341-345).","Susanto, I.Y.; Pan, T.-Y.; Chen, C.-W.; Hu, M.-C.; Cheng, W.-H.","Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",2020,International Conference on Multimedia Retrieval,,x,,Taiwan
94,478,"Rahman, J. S., Hossain, M. Z., & Gedeon, T. (2019, December). Measuring Observers' EDA Responses to Emotional Videos. In Proceedings of the 31st Australian Conference on Human-Computer-Interaction (pp. 457-461).","Rahman, J.S.; Zakir Hossain, M.; Gedeon, T.","Measuring observers' eda responses to emotional videos
",2019,Australian Conference on Human-Computer-Interaction,,x,,Australia
95,479,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.","Rahim, A.; Sagheer, A.; Nadeem, K.; Dar, M.N.; Rahim, A.; Akram, U.","Emotion Charting Using Real-time Monitoring of Physiological Signals
",2020,International Conference on Robotics and Automation in Industry,,x,,Pakistan
95,480,"Rahim, A., Sagheer, A., Nadeem, K., Dar, M. N., Rahim, A., & Akram, U. (2019, October). Emotion Charting Using Real-time Monitoring of Physiological Signals. In 2019 International Conference on Robotics and Automation in Industry (ICRAI) (pp. 1-5). IEEE.","Rahim, A.; Sagheer, A.; Nadeem, K.; Dar, M.N.; Rahim, A.; Akram, U.","Emotion Charting Using Real-time Monitoring of Physiological Signals
",2020,International Conference on Robotics and Automation in Industry,,x,,Pakistan
96,481,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.","Yin, G.; Sun, S.; Zhang, H.; Yu, D.; Li, C.; Zhang, K.; Zou, N.","User Independent Emotion Recognition with Residual Signal-Image Network",2020,IEEE International Conference on Image Processing,,x,,China
96,482,"Yin, G., Sun, S., Zhang, H., Yu, D., Li, C., Zhang, K., & Zou, N. (2019, September). User Independent Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing (ICIP) (pp. 3277-3281). IEEE.","Yin, G.; Sun, S.; Zhang, H.; Yu, D.; Li, C.; Zhang, K.; Zou, N.","User Independent Emotion Recognition with Residual Signal-Image Network",2020,IEEE International Conference on Image Processing,,x,,China
97,483,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,484,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,485,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,486,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,487,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,488,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,489,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,490,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,491,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
97,492,"Yang, H. C., & Lee, C. C. (2019, September). Annotation matters: A comprehensive study on recognizing intended, self-reported, and observed emotion labels using physiology. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 1-7). IEEE.","Yang, H.-C.; Lee, C.-C.","Annotation Matters: A Comprehensive Study on Recognizing Intended, Self-reported, and Observed Emotion Labels using Physiology
",2020,International Conference on Affective Computing and Intelligent Interaction,,x,,Taiwan
98,493,"Kołodziej, M., Tarnowski, P., Majkowski, A., & Rak, R. J. (2019). Electrodermal activity measurements for detection of emotional arousal. Bulletin of the Polish Academy of Sciences. Technical Sciences, 67(4).","Kołodziej, M.; Tarnowski, P.; Majkowski, A.; Rak, R.J.","Electrodermal activity measurements for detection of emotional arousal
",2020,Bulletin of the Polish Academy of Sciences: Technical Sciences,x,,,Poland
99,494,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.","Ganapathy, N.; Swaminathan, R.","Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network",2020,Studies in health technology and informatics,x,,,India
99,495,"Ganapathy, N., & Swaminathan, R. (2019). Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network. Studies in health technology and informatics, 258, 140-140.","Ganapathy, N.; Swaminathan, R.","Emotion Recognition Using Electrodermal Activity Signals and Multiscale Deep Convolution Neural Network",2020,Studies in health technology and informatics,x,,,India
100,496,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.","Subramanian, R.; Wache, J.; Abadi, M.K.; Vieriu, R.L.; Winkler, S.; Sebe, N.","Ascertain: Emotion and personality recognition using commercial sensors
",2018,IEEE Transactions on Affective Computing,x,,,India
100,497,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.","Subramanian, R.; Wache, J.; Abadi, M.K.; Vieriu, R.L.; Winkler, S.; Sebe, N.","Ascertain: Emotion and personality recognition using commercial sensors
",2018,IEEE Transactions on Affective Computing,x,,,India
100,498,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.","Subramanian, R.; Wache, J.; Abadi, M.K.; Vieriu, R.L.; Winkler, S.; Sebe, N.","Ascertain: Emotion and personality recognition using commercial sensors
",2018,IEEE Transactions on Affective Computing,x,,,India
100,499,"Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160.","Subramanian, R.; Wache, J.; Abadi, M.K.; Vieriu, R.L.; Winkler, S.; Sebe, N.","Ascertain: Emotion and personality recognition using commercial sensors
",2018,IEEE Transactions on Affective Computing,x,,,India
101,500,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).","Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. ",In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures,2019,CSEDU,,x,,Germany
101,501,"Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. (2019). In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures. In CSEDU (2) (pp. 111-121).","Yun, H., Fortenbacher, A., Helbig, R., & Pinkwart, N. ",In Search of Learning Indicators: A Study on Sensor Data and IAPS Emotional Pictures,2019,CSEDU,,x,,Germany